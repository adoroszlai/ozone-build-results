Attaching to ozonesecure-ha_datanode2_1, ozonesecure-ha_scm3.org_1, ozonesecure-ha_scm2.org_1, ozonesecure-ha_om1_1, ozonesecure-ha_datanode1_1, ozonesecure-ha_s3g_1, ozonesecure-ha_scm1.org_1, ozonesecure-ha_kdc_1, ozonesecure-ha_kms_1, ozonesecure-ha_om2_1, ozonesecure-ha_om3_1, ozonesecure-ha_recon_1, ozonesecure-ha_datanode3_1
datanode2_1  | Sleeping for 5 seconds
datanode2_1  | Waiting for the service scm3.org:9894
datanode2_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode2_1  | 2021-07-12 13:47:17,377 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode2_1  | /************************************************************
datanode2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode2_1  | STARTUP_MSG:   host = f6c2a59662b6/172.25.0.103
datanode2_1  | STARTUP_MSG:   args = []
datanode2_1  | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
datanode2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.0-SNAPSHOT.jar
datanode2_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
datanode2_1  | STARTUP_MSG:   java = 11.0.10
datanode2_1  | ************************************************************/
datanode2_1  | 2021-07-12 13:47:17,422 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode2_1  | 2021-07-12 13:47:19,485 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode2_1  | 2021-07-12 13:47:20,141 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode2_1  | 2021-07-12 13:47:21,323 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode2_1  | 2021-07-12 13:47:21,323 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode2_1  | 2021-07-12 13:47:22,385 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:f6c2a59662b6 ip:172.25.0.103
datanode2_1  | 2021-07-12 13:47:26,497 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode2_1  | 2021-07-12 13:47:27,479 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode2_1  | 2021-07-12 13:47:27,479 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode2_1  | 2021-07-12 13:47:29,427 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode2_1  | 2021-07-12 13:47:29,434 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode2_1  | 2021-07-12 13:47:29,434 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode2_1  | 2021-07-12 13:47:29,436 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode2_1  | 2021-07-12 13:47:33,051 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode2_1  | 2021-07-12 13:47:33,124 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.103,host:f6c2a59662b6
datanode2_1  | 2021-07-12 13:47:33,148 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode2_1  | 2021-07-12 13:47:33,164 [main] ERROR client.DNCertificateClient: Invalid domain f6c2a59662b6
datanode2_1  | 2021-07-12 13:47:33,165 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@f6c2a59662b6
datanode2_1  | 2021-07-12 13:47:38,921 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode2_1  | 2021-07-12 13:47:39,031 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode2_1  | 2021-07-12 13:47:39,073 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-3115382315205.crt.
datanode2_1  | 2021-07-12 13:47:39,082 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/3208765789747.crt.
datanode2_1  | 2021-07-12 13:47:39,083 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode2_1  | 2021-07-12 13:47:39,204 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode2_1  | 2021-07-12 13:47:40,206 [main] INFO reflections.Reflections: Reflections took 746 ms to scan 2 urls, producing 84 keys and 166 values 
datanode2_1  | 2021-07-12 13:47:41,978 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode2_1  | 2021-07-12 13:47:42,042 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode2_1  | 2021-07-12 13:47:42,104 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode2_1  | 2021-07-12 13:47:42,135 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode2_1  | 2021-07-12 13:47:42,386 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode2_1  | 2021-07-12 13:47:42,584 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2021-07-12 13:47:42,612 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode2_1  | 2021-07-12 13:47:42,634 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode2_1  | 2021-07-12 13:47:42,635 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode2_1  | 2021-07-12 13:47:42,639 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode2_1  | 2021-07-12 13:47:42,811 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
datanode2_1  | 2021-07-12 13:47:42,850 [Thread-8] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode2_1  | 2021-07-12 13:47:42,861 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode2_1  | 2021-07-12 13:47:42,865 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode2_1  | 2021-07-12 13:47:49,975 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode2_1  | 2021-07-12 13:47:50,990 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode2_1  | 2021-07-12 13:47:51,632 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode2_1  | 2021-07-12 13:47:51,635 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode2_1  | 2021-07-12 13:47:51,653 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode2_1  | 2021-07-12 13:47:51,674 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode2_1  | 2021-07-12 13:47:51,709 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:47:51,715 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode2_1  | 2021-07-12 13:47:51,719 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2021-07-12 13:48:01,523 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode2_1  | 2021-07-12 13:48:01,533 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:01,540 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2021-07-12 13:48:01,616 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2021-07-12 13:48:04,041 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode2_1  | 2021-07-12 13:48:04,076 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode2_1  | 2021-07-12 13:48:04,076 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode2_1  | 2021-07-12 13:48:04,287 [main] INFO util.log: Logging initialized @56537ms to org.eclipse.jetty.util.log.Slf4jLog
datanode2_1  | 2021-07-12 13:48:04,999 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode2_1  | 2021-07-12 13:48:05,056 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode2_1  | 2021-07-12 13:48:05,085 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode2_1  | 2021-07-12 13:48:05,085 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode2_1  | 2021-07-12 13:48:05,085 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode2_1  | 2021-07-12 13:48:05,093 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode2_1  | 2021-07-12 13:48:05,458 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode2_1  | 2021-07-12 13:48:05,470 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode2_1  | 2021-07-12 13:48:05,737 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode2_1  | 2021-07-12 13:48:05,737 [main] INFO server.session: No SessionScavenger set, using defaults
datanode2_1  | 2021-07-12 13:48:05,738 [main] INFO server.session: node0 Scavenging every 600000ms
datanode2_1  | 2021-07-12 13:48:05,950 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:05,955 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@27f6e2c3{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode2_1  | 2021-07-12 13:48:05,955 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@48064a5a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode2_1  | 2021-07-12 13:48:06,727 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:06,835 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4ef7145f{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_0-SNAPSHOT_jar-_-any-5347387218017372351/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode2_1  | 2021-07-12 13:48:06,920 [main] INFO server.AbstractConnector: Started ServerConnector@7444714e{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode2_1  | 2021-07-12 13:48:06,920 [main] INFO server.Server: Started @59170ms
datanode2_1  | 2021-07-12 13:48:06,932 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode2_1  | 2021-07-12 13:48:06,932 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode2_1  | 2021-07-12 13:48:06,953 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode2_1  | 2021-07-12 13:48:07,144 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@e86acdd] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode2_1  | 2021-07-12 13:48:07,738 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode2_1  | 2021-07-12 13:48:09,933 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode2_1  | 2021-07-12 13:48:09,939 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode2_1  | 2021-07-12 13:48:10,353 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:10,478 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.RaftServer: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start RPC server
datanode2_1  | 2021-07-12 13:48:10,499 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e7ed1272-c70a-4bce-ab84-6d03b85621dc: GrpcService started, listening on 9856
datanode2_1  | 2021-07-12 13:48:10,505 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e7ed1272-c70a-4bce-ab84-6d03b85621dc: GrpcService started, listening on 9857
datanode2_1  | 2021-07-12 13:48:10,515 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO server.GrpcService: e7ed1272-c70a-4bce-ab84-6d03b85621dc: GrpcService started, listening on 9858
datanode2_1  | 2021-07-12 13:48:10,524 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e7ed1272-c70a-4bce-ab84-6d03b85621dc is started using port 9858 for RATIS
datanode2_1  | 2021-07-12 13:48:10,525 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e7ed1272-c70a-4bce-ab84-6d03b85621dc is started using port 9857 for RATIS_ADMIN
datanode2_1  | 2021-07-12 13:48:10,525 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis e7ed1272-c70a-4bce-ab84-6d03b85621dc is started using port 9856 for RATIS_SERVER
datanode2_1  | 2021-07-12 13:48:10,590 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$327/0x00000008405b2040@b8eb39d] INFO util.JvmPauseMonitor: JvmPauseMonitor-e7ed1272-c70a-4bce-ab84-6d03b85621dc: Started
datanode2_1  | 2021-07-12 13:48:10,641 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2021-07-12 13:48:10,644 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode2_1  | 2021-07-12 13:48:11,253 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode2_1  | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:629)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:269)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:454)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: java.util.concurrent.TimeoutException
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode2_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode2_1  | 	... 1 more
datanode2_1  | 2021-07-12 13:48:13,466 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:14,369 [Command processor thread] INFO server.RaftServer: e7ed1272-c70a-4bce-ab84-6d03b85621dc: addNew group-89C5F88BC674:[e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] returns group-89C5F88BC674:java.util.concurrent.CompletableFuture@3ea98552[Not completed]
datanode2_1  | 2021-07-12 13:48:14,449 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc: new RaftServerImpl for group-89C5F88BC674:[e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1] with ContainerStateMachine:uninitialized
datanode2_1  | 2021-07-12 13:48:14,453 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2021-07-12 13:48:14,454 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2021-07-12 13:48:14,455 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2021-07-12 13:48:14,466 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:14,486 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2021-07-12 13:48:14,488 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2021-07-12 13:48:14,489 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:14,508 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: ConfigurationManager, init=-1: [e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2021-07-12 13:48:14,516 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2021-07-12 13:48:14,529 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2021-07-12 13:48:14,538 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7f7f3d83-0cbd-41c6-b795-89c5f88bc674 does not exist. Creating ...
datanode2_1  | 2021-07-12 13:48:14,595 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7f7f3d83-0cbd-41c6-b795-89c5f88bc674/in_use.lock acquired by nodename 7@f6c2a59662b6
datanode2_1  | 2021-07-12 13:48:14,637 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7f7f3d83-0cbd-41c6-b795-89c5f88bc674 has been successfully formatted.
datanode2_1  | 2021-07-12 13:48:14,704 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-89C5F88BC674: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2021-07-12 13:48:14,730 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:14,755 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2021-07-12 13:48:14,892 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2021-07-12 13:48:14,893 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:14,945 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674
datanode2_1  | 2021-07-12 13:48:15,079 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:15,154 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2021-07-12 13:48:15,154 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2021-07-12 13:48:15,212 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7f7f3d83-0cbd-41c6-b795-89c5f88bc674
datanode2_1  | 2021-07-12 13:48:15,228 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode2_1  | 2021-07-12 13:48:15,241 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:15,242 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:15,242 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
kms_1        | Sleeping for 5 seconds
kms_1        | WARNING: /opt/hadoop/temp does not exist. Creating.
datanode2_1  | 2021-07-12 13:48:15,242 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2021-07-12 13:48:15,244 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2021-07-12 13:48:15,244 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2021-07-12 13:48:15,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2021-07-12 13:48:15,472 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:15,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:15,566 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:15,566 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:15,588 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:15,596 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2021-07-12 13:48:15,600 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2021-07-12 13:48:15,602 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2021-07-12 13:48:15,606 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | Sleeping for 5 seconds
datanode1_1  | Waiting for the service scm3.org:9894
datanode1_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode1_1  | 2021-07-12 13:47:17,626 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode1_1  | /************************************************************
datanode1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode1_1  | STARTUP_MSG:   host = 27bc097164fa/172.25.0.102
datanode1_1  | STARTUP_MSG:   args = []
datanode1_1  | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): Loaded
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](Error): preauth spake failed to initialize: No SPAKE preauth groups configured
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): setting up network...
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): setsockopt(8,IPV6_V6ONLY,1) worked
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): setsockopt(10,IPV6_V6ONLY,1) worked
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): set up 4 sockets
kdc_1        | Jul 12 13:45:36 kdc krb5kdc[7](info): commencing operation
kdc_1        | krb5kdc: starting...
kdc_1        | Jul 12 13:45:38 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097538, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:45:46 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097546, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:09 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1626097569, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:16 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.116: ISSUE: authtime 1626097576, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:21 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626097569, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097546, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:34 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097594, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:36 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.117: ISSUE: authtime 1626097596, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:42 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097594, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:43 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626097596, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:46 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097606, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:48 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1626097608, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:49 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1626097608, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097606, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:46:59 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097619, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:46:59 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.118: ISSUE: authtime 1626097619, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:06 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.118: ISSUE: authtime 1626097619, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.0-SNAPSHOT.jar
datanode1_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
datanode1_1  | STARTUP_MSG:   java = 11.0.10
datanode1_1  | ************************************************************/
datanode1_1  | 2021-07-12 13:47:17,737 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode1_1  | 2021-07-12 13:47:19,839 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode1_1  | 2021-07-12 13:47:20,584 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode1_1  | 2021-07-12 13:47:21,508 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode1_1  | 2021-07-12 13:47:21,508 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode1_1  | 2021-07-12 13:47:22,761 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:27bc097164fa ip:172.25.0.102
datanode1_1  | 2021-07-12 13:47:26,529 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode1_1  | 2021-07-12 13:47:27,752 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode1_1  | 2021-07-12 13:47:27,756 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode1_1  | 2021-07-12 13:47:29,894 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode1_1  | 2021-07-12 13:47:29,899 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode1_1  | 2021-07-12 13:47:29,900 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode1_1  | 2021-07-12 13:47:29,905 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode1_1  | 2021-07-12 13:47:35,298 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode1_1  | 2021-07-12 13:47:35,373 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.102,host:27bc097164fa
datanode1_1  | 2021-07-12 13:47:35,373 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode1_1  | 2021-07-12 13:47:35,400 [main] ERROR client.DNCertificateClient: Invalid domain 27bc097164fa
datanode1_1  | 2021-07-12 13:47:35,401 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@27bc097164fa
datanode1_1  | 2021-07-12 13:47:40,506 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode1_1  | 2021-07-12 13:47:40,593 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/3210368079521.crt.
datanode1_1  | 2021-07-12 13:47:40,625 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode1_1  | 2021-07-12 13:47:40,642 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-3115382315205.crt.
datanode1_1  | 2021-07-12 13:47:40,642 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode1_1  | 2021-07-12 13:47:40,761 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode1_1  | 2021-07-12 13:47:41,639 [main] INFO reflections.Reflections: Reflections took 679 ms to scan 2 urls, producing 84 keys and 166 values 
datanode1_1  | 2021-07-12 13:47:43,514 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode1_1  | 2021-07-12 13:47:43,613 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode1_1  | 2021-07-12 13:47:43,636 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode1_1  | 2021-07-12 13:47:43,649 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode1_1  | 2021-07-12 13:47:43,968 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode1_1  | 2021-07-12 13:47:44,192 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2021-07-12 13:47:44,215 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode1_1  | 2021-07-12 13:47:44,237 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode1_1  | 2021-07-12 13:47:44,238 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode1_1  | 2021-07-12 13:47:44,240 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode1_1  | 2021-07-12 13:47:44,461 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
datanode1_1  | 2021-07-12 13:47:44,485 [Thread-8] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode1_1  | 2021-07-12 13:47:44,509 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode1_1  | 2021-07-12 13:47:44,510 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode1_1  | 2021-07-12 13:47:51,073 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode1_1  | 2021-07-12 13:47:51,727 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode1_1  | 2021-07-12 13:47:52,571 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode1_1  | 2021-07-12 13:47:52,582 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode1_1  | 2021-07-12 13:47:52,596 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode1_1  | 2021-07-12 13:47:52,671 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode1_1  | 2021-07-12 13:47:52,677 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2021-07-12 13:47:52,709 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode1_1  | 2021-07-12 13:47:52,735 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode1_1  | 2021-07-12 13:48:03,147 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode1_1  | 2021-07-12 13:48:03,159 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:03,160 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2021-07-12 13:48:03,242 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2021-07-12 13:48:05,600 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
kdc_1        | Jul 12 13:47:06 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097619, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:14 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097634, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:25 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.104: ISSUE: authtime 1626097645, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:27 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.103: ISSUE: authtime 1626097647, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:27 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.102: ISSUE: authtime 1626097647, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:29 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:29 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:29 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:47:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1626097649, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.104: ISSUE: authtime 1626097645, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:37 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.103: ISSUE: authtime 1626097647, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:38 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.102: ISSUE: authtime 1626097647, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, dn/dn@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:48 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097634, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:47:54 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097674, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:08 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.111: ISSUE: authtime 1626097688, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:10 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.112: ISSUE: authtime 1626097690, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.111: ISSUE: authtime 1626097688, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:14 kdc krb5kdc[7](info): AS_REQ (2 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17)}) 172.25.0.113: ISSUE: authtime 1626097694, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:15 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.112: ISSUE: authtime 1626097690, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:18 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.113: ISSUE: authtime 1626097694, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:27 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097674, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:32 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097712, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:39 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097712, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:44 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097724, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:49 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097724, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:48:50 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097730, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:48:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097730, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, scm/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:48:59 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:49:10 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:25 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:31 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:37 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:45 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:52 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:49:58 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:03 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:15,618 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2021-07-12 13:48:15,764 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674
datanode2_1  | 2021-07-12 13:48:15,801 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674
datanode2_1  | 2021-07-12 13:48:15,839 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: start as a follower, conf=-1: [e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2021-07-12 13:48:15,844 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2021-07-12 13:48:15,849 [pool-23-thread-1] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState
datanode2_1  | 2021-07-12 13:48:15,875 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-89C5F88BC674,id=e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:15,876 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674
datanode2_1  | 2021-07-12 13:48:15,988 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7f7f3d83-0cbd-41c6-b795-89c5f88bc674
datanode2_1  | 2021-07-12 13:48:15,991 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7f7f3d83-0cbd-41c6-b795-89c5f88bc674.
datanode2_1  | 2021-07-12 13:48:15,996 [Command processor thread] INFO server.RaftServer: e7ed1272-c70a-4bce-ab84-6d03b85621dc: addNew group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-ABE0FEB7C4D8:java.util.concurrent.CompletableFuture@6bae61f2[Not completed]
datanode2_1  | 2021-07-12 13:48:15,998 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc: new RaftServerImpl for group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode2_1  | 2021-07-12 13:48:16,004 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2021-07-12 13:48:16,004 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2021-07-12 13:48:16,005 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2021-07-12 13:48:16,005 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:16,005 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2021-07-12 13:48:16,005 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2021-07-12 13:48:16,006 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:16,014 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2021-07-12 13:48:16,014 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2021-07-12 13:48:16,016 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2021-07-12 13:48:16,017 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 does not exist. Creating ...
datanode2_1  | 2021-07-12 13:48:16,021 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/in_use.lock acquired by nodename 7@f6c2a59662b6
datanode2_1  | 2021-07-12 13:48:16,029 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 has been successfully formatted.
datanode2_1  | 2021-07-12 13:48:16,033 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-ABE0FEB7C4D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode2_1  | 2021-07-12 13:48:16,037 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:16,037 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2021-07-12 13:48:16,054 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2021-07-12 13:48:16,055 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:16,055 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:16,055 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:16,073 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2021-07-12 13:48:16,076 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2021-07-12 13:48:16,076 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode2_1  | 2021-07-12 13:48:16,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode2_1  | 2021-07-12 13:48:16,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:16,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:16,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2021-07-12 13:48:16,077 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2021-07-12 13:48:16,078 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | Sleeping for 5 seconds
datanode3_1  | Waiting for the service scm3.org:9894
datanode3_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode3_1  | 2021-07-12 13:47:16,773 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode3_1  | /************************************************************
datanode3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode3_1  | STARTUP_MSG:   host = abb35404295f/172.25.0.104
datanode3_1  | STARTUP_MSG:   args = []
datanode3_1  | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
kdc_1        | Jul 12 13:50:18 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:23 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:28 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:32 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:40 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:45 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:49 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:50:59 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:03 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:08 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:12 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:17 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097739, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:17 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097877, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:51:22 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097877, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:26 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097877, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:27 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097887, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:51:31 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097887, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:36 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097887, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:40 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097887, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:51 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097887, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:51:54 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097914, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:51:58 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097914, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:07 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097914, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:11 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097931, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:52:15 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097931, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:20 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097931, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:21 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097941, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:52:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097941, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:29 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097941, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:30 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097950, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:52:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097950, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:35 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097955, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:52:38 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097955, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:39 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode1_1  | 2021-07-12 13:48:05,601 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode1_1  | 2021-07-12 13:48:05,601 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode1_1  | 2021-07-12 13:48:05,807 [main] INFO util.log: Logging initialized @57932ms to org.eclipse.jetty.util.log.Slf4jLog
datanode1_1  | 2021-07-12 13:48:06,609 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode1_1  | 2021-07-12 13:48:06,649 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode1_1  | 2021-07-12 13:48:06,663 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode1_1  | 2021-07-12 13:48:06,664 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode1_1  | 2021-07-12 13:48:06,672 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode1_1  | 2021-07-12 13:48:06,713 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode1_1  | 2021-07-12 13:48:07,108 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode1_1  | 2021-07-12 13:48:07,110 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode1_1  | 2021-07-12 13:48:07,341 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode1_1  | 2021-07-12 13:48:07,352 [main] INFO server.session: No SessionScavenger set, using defaults
datanode1_1  | 2021-07-12 13:48:07,357 [main] INFO server.session: node0 Scavenging every 600000ms
datanode1_1  | 2021-07-12 13:48:07,471 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2021-07-12 13:48:07,496 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c89fdd8{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode1_1  | 2021-07-12 13:48:07,501 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d018c58{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode1_1  | 2021-07-12 13:48:08,079 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode1_1  | 2021-07-12 13:48:08,184 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1611ce1c{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_0-SNAPSHOT_jar-_-any-9997493823027815005/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode1_1  | 2021-07-12 13:48:08,230 [main] INFO server.AbstractConnector: Started ServerConnector@6b3708f0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode1_1  | 2021-07-12 13:48:08,230 [main] INFO server.Server: Started @60355ms
datanode1_1  | 2021-07-12 13:48:08,252 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode1_1  | 2021-07-12 13:48:08,252 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode1_1  | 2021-07-12 13:48:08,264 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode1_1  | 2021-07-12 13:48:08,400 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1bb3cb91] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode1_1  | 2021-07-12 13:48:08,909 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode1_1  | 2021-07-12 13:48:11,238 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode1_1  | 2021-07-12 13:48:11,241 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode1_1  | 2021-07-12 13:48:11,709 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5b90731c-5249-453a-9f10-ddd353e45997
datanode1_1  | 2021-07-12 13:48:11,860 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 5b90731c-5249-453a-9f10-ddd353e45997: start RPC server
datanode1_1  | 2021-07-12 13:48:11,866 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 5b90731c-5249-453a-9f10-ddd353e45997: GrpcService started, listening on 9856
datanode1_1  | 2021-07-12 13:48:11,881 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 5b90731c-5249-453a-9f10-ddd353e45997: GrpcService started, listening on 9857
datanode1_1  | 2021-07-12 13:48:11,897 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 5b90731c-5249-453a-9f10-ddd353e45997: GrpcService started, listening on 9858
datanode1_1  | 2021-07-12 13:48:11,916 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5b90731c-5249-453a-9f10-ddd353e45997 is started using port 9858 for RATIS
datanode1_1  | 2021-07-12 13:48:11,921 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5b90731c-5249-453a-9f10-ddd353e45997 is started using port 9857 for RATIS_ADMIN
datanode1_1  | 2021-07-12 13:48:11,921 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5b90731c-5249-453a-9f10-ddd353e45997 is started using port 9856 for RATIS_SERVER
datanode1_1  | 2021-07-12 13:48:11,921 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$327/0x00000008405b2040@359ad059] INFO util.JvmPauseMonitor: JvmPauseMonitor-5b90731c-5249-453a-9f10-ddd353e45997: Started
datanode1_1  | 2021-07-12 13:48:11,978 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2021-07-12 13:48:11,978 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode1_1  | 2021-07-12 13:48:14,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:15,661 [Command processor thread] INFO server.RaftServer: 5b90731c-5249-453a-9f10-ddd353e45997: addNew group-7844FDF8175C:[5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1] returns group-7844FDF8175C:java.util.concurrent.CompletableFuture@dc1ec90[Not completed]
datanode1_1  | 2021-07-12 13:48:15,810 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997: new RaftServerImpl for group-7844FDF8175C:[5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1] with ContainerStateMachine:uninitialized
datanode1_1  | 2021-07-12 13:48:15,812 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2021-07-12 13:48:15,816 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2021-07-12 13:48:15,816 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2021-07-12 13:48:15,818 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:15,819 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2021-07-12 13:48:15,830 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2021-07-12 13:48:15,831 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2021-07-12 13:48:15,860 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: ConfigurationManager, init=-1: [5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2021-07-12 13:48:15,863 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2021-07-12 13:48:15,884 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2021-07-12 13:48:15,894 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7ef1c9bf-43c8-43b8-b00a-7844fdf8175c does not exist. Creating ...
datanode1_1  | 2021-07-12 13:48:15,925 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7ef1c9bf-43c8-43b8-b00a-7844fdf8175c/in_use.lock acquired by nodename 8@27bc097164fa
datanode1_1  | 2021-07-12 13:48:15,984 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7ef1c9bf-43c8-43b8-b00a-7844fdf8175c has been successfully formatted.
datanode1_1  | 2021-07-12 13:48:16,022 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-7844FDF8175C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
kdc_1        | Jul 12 13:52:43 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:48 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:52 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:52:57 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:01 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:06 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097959, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:07 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626097987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:11 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:15 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:20 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626097987, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:25 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098005, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:25 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098005, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:29 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098005, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:30 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098010, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:30 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098010, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:33 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098010, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:16,078 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2021-07-12 13:48:16,078 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2021-07-12 13:48:16,081 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:16,083 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:16,087 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:16,087 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:16,118 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:16,118 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2021-07-12 13:48:16,118 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2021-07-12 13:48:16,118 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2021-07-12 13:48:16,119 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2021-07-12 13:48:16,119 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2021-07-12 13:48:16,119 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:16,121 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:16,122 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:16,132 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2021-07-12 13:48:16,132 [pool-23-thread-1] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState
datanode2_1  | 2021-07-12 13:48:16,162 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ABE0FEB7C4D8,id=e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:16,162 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:16,163 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode2_1  | 2021-07-12 13:48:16,430 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-8F4D0287087C->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode2_1  | 2021-07-12 13:48:16,538 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:19,367 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode2_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode2_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode2_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode2_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode2_1  | 	... 18 more
datanode3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.0-SNAPSHOT.jar
datanode3_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
datanode3_1  | STARTUP_MSG:   java = 11.0.10
datanode3_1  | ************************************************************/
datanode3_1  | 2021-07-12 13:47:16,897 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode3_1  | 2021-07-12 13:47:18,588 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode3_1  | 2021-07-12 13:47:19,508 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode3_1  | 2021-07-12 13:47:20,572 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode3_1  | 2021-07-12 13:47:20,572 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode3_1  | 2021-07-12 13:47:21,549 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:abb35404295f ip:172.25.0.104
datanode3_1  | 2021-07-12 13:47:24,947 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/dn@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode3_1  | 2021-07-12 13:47:26,024 [main] INFO security.UserGroupInformation: Login successful for user dn/dn@EXAMPLE.COM using keytab file dn.keytab. Keytab auto renewal enabled : false
datanode3_1  | 2021-07-12 13:47:26,024 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode3_1  | 2021-07-12 13:47:28,015 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode3_1  | 2021-07-12 13:47:28,024 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode3_1  | 2021-07-12 13:47:28,024 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode3_1  | 2021-07-12 13:47:28,029 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode3_1  | 2021-07-12 13:47:31,214 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode3_1  | 2021-07-12 13:47:31,287 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.104,host:abb35404295f
datanode3_1  | 2021-07-12 13:47:31,312 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode3_1  | 2021-07-12 13:47:31,360 [main] ERROR client.DNCertificateClient: Invalid domain abb35404295f
datanode3_1  | 2021-07-12 13:47:31,361 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@abb35404295f
datanode3_1  | 2021-07-12 13:47:36,529 [main] INFO client.DNCertificateClient: Loading certificate from location:/data/metadata/dn/certs.
datanode3_1  | 2021-07-12 13:47:36,665 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/ROOTCA-1.crt.
datanode3_1  | 2021-07-12 13:47:36,762 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/CA-3115382315205.crt.
datanode3_1  | 2021-07-12 13:47:36,781 [main] INFO client.DNCertificateClient: Added certificate from file:/data/metadata/dn/certs/3206477232763.crt.
datanode3_1  | 2021-07-12 13:47:36,784 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode3_1  | 2021-07-12 13:47:36,875 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode3_1  | 2021-07-12 13:47:37,826 [main] INFO reflections.Reflections: Reflections took 685 ms to scan 2 urls, producing 84 keys and 166 values 
datanode3_1  | 2021-07-12 13:47:39,701 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode3_1  | 2021-07-12 13:47:39,767 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89311358976
datanode3_1  | 2021-07-12 13:47:39,808 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode3_1  | 2021-07-12 13:47:39,825 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode3_1  | 2021-07-12 13:47:40,031 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode3_1  | 2021-07-12 13:47:40,302 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2021-07-12 13:47:40,314 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode3_1  | 2021-07-12 13:47:40,328 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode3_1  | 2021-07-12 13:47:40,332 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode3_1  | 2021-07-12 13:47:40,351 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode3_1  | 2021-07-12 13:47:40,538 [main] INFO ozoneimpl.ContainerReader: Running in upgrade mode:true
datanode3_1  | 2021-07-12 13:47:40,550 [Thread-8] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode3_1  | 2021-07-12 13:47:40,572 [Thread-8] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode3_1  | 2021-07-12 13:47:40,572 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode3_1  | 2021-07-12 13:47:48,056 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode3_1  | 2021-07-12 13:47:48,405 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode3_1  | 2021-07-12 13:47:49,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode3_1  | 2021-07-12 13:47:49,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode3_1  | 2021-07-12 13:47:49,618 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode3_1  | 2021-07-12 13:47:49,619 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode3_1  | 2021-07-12 13:47:49,661 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2021-07-12 13:47:49,669 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode3_1  | 2021-07-12 13:47:49,683 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2021-07-12 13:47:57,503 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
kdc_1        | Jul 12 13:53:34 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098014, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:34 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098014, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:38 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098014, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:42 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098014, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:43 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098023, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:53:47 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098023, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:52 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098023, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:53:56 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098023, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:01 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098023, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:01 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098041, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:54:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098041, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:09 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098041, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:19 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098041, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:23 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098063, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:54:27 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098063, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:32 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098063, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:36 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098063, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:54:57 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098097, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:19,403 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DEC549B2517D->5b90731c-5249-453a-9f10-ddd353e45997
datanode2_1  | 2021-07-12 13:48:19,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:20,476 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode2_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode2_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode2_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode2_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode2_1  | 	... 18 more
datanode2_1  | 2021-07-12 13:48:20,477 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8.
datanode2_1  | 2021-07-12 13:48:20,485 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc: new RaftServerImpl for group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode2_1  | 2021-07-12 13:48:20,494 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode2_1  | 2021-07-12 13:48:20,494 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:20,495 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode2_1  | 2021-07-12 13:48:20,496 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode2_1  | 2021-07-12 13:48:20,496 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode2_1  | 2021-07-12 13:48:20,499 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 does not exist. Creating ...
datanode2_1  | 2021-07-12 13:48:20,501 [Command processor thread] INFO server.RaftServer: e7ed1272-c70a-4bce-ab84-6d03b85621dc: addNew group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-562B256BCC94:java.util.concurrent.CompletableFuture@5372bdf6[Not completed]
datanode2_1  | 2021-07-12 13:48:20,509 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/in_use.lock acquired by nodename 7@f6c2a59662b6
datanode2_1  | 2021-07-12 13:48:20,516 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 has been successfully formatted.
datanode2_1  | 2021-07-12 13:48:20,517 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-562B256BCC94: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
kdc_1        | Jul 12 13:55:00 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098097, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:55:06 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098106, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:55:09 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098106, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:55:21 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098121, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:55:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098121, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:55:45 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098145, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:55:48 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098145, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:55:54 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098154, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:55:57 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098154, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:56:02 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098162, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:56:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098162, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:56:10 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098170, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:56:13 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098170, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:56:17 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098177, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:56:20 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098177, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:57:44 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098264, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:57:47 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098264, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode3_1  | 2021-07-12 13:47:57,517 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:47:57,517 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2021-07-12 13:47:57,589 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2021-07-12 13:48:00,140 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode3_1  | 2021-07-12 13:48:00,141 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
datanode3_1  | 2021-07-12 13:48:00,141 [main] INFO http.BaseHttpServer: HttpAuthType: hdds.datanode.http.auth.type = kerberos
datanode3_1  | 2021-07-12 13:48:00,482 [main] INFO util.log: Logging initialized @52791ms to org.eclipse.jetty.util.log.Slf4jLog
datanode3_1  | 2021-07-12 13:48:01,333 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode3_1  | 2021-07-12 13:48:01,350 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode3_1  | 2021-07-12 13:48:01,357 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode3_1  | 2021-07-12 13:48:01,381 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode3_1  | 2021-07-12 13:48:01,381 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode3_1  | 2021-07-12 13:48:01,393 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.datanode.http.auth.kerberos.principal keytabKey: hdds.datanode.http.auth.kerberos.keytab
datanode3_1  | 2021-07-12 13:48:01,662 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode3_1  | 2021-07-12 13:48:01,670 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode3_1  | 2021-07-12 13:48:01,874 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode3_1  | 2021-07-12 13:48:01,874 [main] INFO server.session: No SessionScavenger set, using defaults
datanode3_1  | 2021-07-12 13:48:01,876 [main] INFO server.session: node0 Scavenging every 600000ms
datanode3_1  | 2021-07-12 13:48:02,002 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2021-07-12 13:48:02,025 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@18bab95b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode3_1  | 2021-07-12 13:48:02,026 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2f1b8770{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode3_1  | 2021-07-12 13:48:02,585 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/db@EXAMPLE.COM
datanode3_1  | 2021-07-12 13:48:02,689 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@221961f2{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_0-SNAPSHOT_jar-_-any-15125711631236671359/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode3_1  | 2021-07-12 13:48:02,752 [main] INFO server.AbstractConnector: Started ServerConnector@eb23b9c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode3_1  | 2021-07-12 13:48:02,753 [main] INFO server.Server: Started @55062ms
datanode3_1  | 2021-07-12 13:48:02,760 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2021-07-12 13:48:02,760 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode3_1  | 2021-07-12 13:48:02,772 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode3_1  | 2021-07-12 13:48:02,943 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@321f567a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode3_1  | 2021-07-12 13:48:03,303 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.25.0.115:9891
datanode3_1  | 2021-07-12 13:48:07,279 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode3_1  | 2021-07-12 13:48:07,281 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode3_1  | 2021-07-12 13:48:07,693 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:07,945 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.RaftServer: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start RPC server
datanode3_1  | 2021-07-12 13:48:07,963 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: GrpcService started, listening on 9856
datanode3_1  | 2021-07-12 13:48:07,964 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: GrpcService started, listening on 9857
datanode3_1  | 2021-07-12 13:48:07,977 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO server.GrpcService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: GrpcService started, listening on 9858
datanode3_1  | 2021-07-12 13:48:08,001 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 is started using port 9858 for RATIS
datanode3_1  | 2021-07-12 13:48:08,012 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 is started using port 9857 for RATIS_ADMIN
datanode3_1  | 2021-07-12 13:48:08,012 [EndpointStateMachine task thread for scm3.org/172.25.0.118:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 is started using port 9856 for RATIS_SERVER
datanode3_1  | 2021-07-12 13:48:08,039 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$327/0x00000008405b2040@504c170a] INFO util.JvmPauseMonitor: JvmPauseMonitor-8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Started
datanode3_1  | 2021-07-12 13:48:08,072 [EndpointStateMachine task thread for scm1.org/172.25.0.116:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2021-07-12 13:48:08,072 [EndpointStateMachine task thread for scm2.org/172.25.0.117:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
datanode3_1  | 2021-07-12 13:48:09,242 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:10,734 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$327/0x00000008405b2040@504c170a] WARN util.JvmPauseMonitor: JvmPauseMonitor-8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Detected pause in JVM or host machine (eg GC): pause of approximately 144118557ns.
datanode3_1  | GC pool 'ParNew' had collection(s): count=1 time=158ms
datanode3_1  | 2021-07-12 13:48:12,313 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:15,385 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
kdc_1        | Jul 12 13:58:02 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098282, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:58:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098282, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:58:21 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:58:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098301, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:58:36 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098316, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:58:39 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098316, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:58:54 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098334, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:58:57 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098334, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:59:00 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098340, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:59:00 kdc krb5kdc[7](info): TGS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098340, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, HTTP/scm@EXAMPLE.COM for HTTP/s3g@EXAMPLE.COM
kdc_1        | Jul 12 13:59:10 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 13:59:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 13:59:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:31 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:35 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:39 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:43 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:20,518 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode2_1  | 2021-07-12 13:48:20,518 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode2_1  | 2021-07-12 13:48:20,518 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode2_1  | 2021-07-12 13:48:20,536 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:20,536 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:20,536 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:20,537 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode2_1  | 2021-07-12 13:48:20,537 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode2_1  | 2021-07-12 13:48:20,538 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94
datanode2_1  | 2021-07-12 13:48:20,540 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode2_1  | 2021-07-12 13:48:20,540 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:20,541 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:20,541 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode2_1  | 2021-07-12 13:48:20,541 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode2_1  | 2021-07-12 13:48:20,541 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2021-07-12 13:48:20,541 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode2_1  | 2021-07-12 13:48:20,546 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode2_1  | 2021-07-12 13:48:20,554 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode2_1  | 2021-07-12 13:48:20,560 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:20,570 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:20,570 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:48:20,574 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode2_1  | 2021-07-12 13:48:20,574 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode2_1  | 2021-07-12 13:48:20,589 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode2_1  | 2021-07-12 13:48:20,589 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode2_1  | 2021-07-12 13:48:20,589 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode2_1  | 2021-07-12 13:48:20,589 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode2_1  | 2021-07-12 13:48:20,590 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:20,590 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:20,593 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:20,593 [pool-23-thread-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode2_1  | 2021-07-12 13:48:20,609 [pool-23-thread-1] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState
datanode2_1  | 2021-07-12 13:48:20,624 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562B256BCC94,id=e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:20,624 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:20,658 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=660497cf-d26f-4475-b297-562b256bcc94
datanode2_1  | 2021-07-12 13:48:20,673 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FA7E4AE1500F->5b90731c-5249-453a-9f10-ddd353e45997
datanode2_1  | 2021-07-12 13:48:20,959 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState] INFO impl.FollowerState: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5110127779ns, electionTimeout:5102ms
datanode2_1  | 2021-07-12 13:48:20,960 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState
datanode2_1  | 2021-07-12 13:48:20,965 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2021-07-12 13:48:21,002 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2021-07-12 13:48:21,003 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1
datanode2_1  | 2021-07-12 13:48:21,081 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1], old=null
datanode2_1  | 2021-07-12 13:48:21,093 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2021-07-12 13:48:18,464 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:21,529 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:24,601 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:25,340 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:25,738 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed requestVote e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:548)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:172)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:394)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:25,764 [grpc-default-executor-1] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed requestVote e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:548)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:172)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:394)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:26,166 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed requestVote 5b90731c-5249-453a-9f10-ddd353e45997->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:548)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:172)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:394)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | Sleeping for 5 seconds
om3_1        | Waiting for the service scm3.org:9894
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2021-07-12 13:47:18,626 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = [--init]
om3_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
datanode2_1  | 2021-07-12 13:48:21,099 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1
datanode2_1  | 2021-07-12 13:48:21,110 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2021-07-12 13:48:21,112 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-89C5F88BC674 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:21,114 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 1 for becomeLeader, leader elected after 6398ms
datanode2_1  | 2021-07-12 13:48:21,179 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2021-07-12 13:48:21,211 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674
datanode2_1  | 2021-07-12 13:48:21,250 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:21,258 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode2_1  | 2021-07-12 13:48:21,290 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.FollowerState: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5157856988ns, electionTimeout:5078ms
datanode2_1  | 2021-07-12 13:48:21,291 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState
datanode2_1  | 2021-07-12 13:48:21,292 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2021-07-12 13:48:21,292 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2021-07-12 13:48:21,292 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2
datanode2_1  | 2021-07-12 13:48:21,380 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2021-07-12 13:48:21,410 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2021-07-12 13:48:21,415 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2021-07-12 13:48:21,475 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:21,543 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderStateImpl
datanode2_1  | 2021-07-12 13:48:21,815 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2021-07-12 13:48:22,320 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-LeaderElection1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674: set configuration 0: [e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:1], old=null
datanode2_1  | 2021-07-12 13:48:22,704 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:22,990 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode2_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode2_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode2_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode2_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
kdc_1        | Jul 12 13:59:47 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:51 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 13:59:55 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:00 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:04 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:18 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098350, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:22 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098422, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:00:25 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098422, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:25 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098425, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:00:29 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098425, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:30 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098430, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:00:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098430, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:38 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098430, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:42 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098430, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:46 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098430, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:50 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:00:53 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:00:57 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:01 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:09 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:18 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:22 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:26 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098450, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:29 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:01:32 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:36 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:41 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:45 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:50 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:01:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:00 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:09 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:15 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.116: ISSUE: authtime 1626098489, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:25 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:02:29 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1        | Jul 12 14:02:39 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:46 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:50 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:54 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:02:58 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:02 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:06 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:10 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:14 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:18 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098545, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:21 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098601, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:03:25 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098601, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:25 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098605, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:03:29 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098605, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:29 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098609, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode2_1  | 	... 18 more
datanode2_1  | 2021-07-12 13:48:22,998 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-F60CB2BEF07A->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode2_1  | 2021-07-12 13:48:23,294 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-89C5F88BC674-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7f7f3d83-0cbd-41c6-b795-89c5f88bc674/current/log_inprogress_0
datanode2_1  | 2021-07-12 13:48:24,566 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$327/0x00000008405b2040@b8eb39d] WARN util.JvmPauseMonitor: JvmPauseMonitor-e7ed1272-c70a-4bce-ab84-6d03b85621dc: Detected pause in JVM or host machine (eg GC): pause of approximately 119390237ns.
datanode2_1  | GC pool 'ParNew' had collection(s): count=1 time=132ms
datanode2_1  | 2021-07-12 13:48:25,007 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode2_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode2_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode2_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode2_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode2_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode2_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode2_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode2_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode2_1  | 	... 18 more
datanode2_1  | 2021-07-12 13:48:25,020 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=660497cf-d26f-4475-b297-562b256bcc94.
datanode2_1  | 2021-07-12 13:48:25,653 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState] INFO impl.FollowerState: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5048564929ns, electionTimeout:5028ms
datanode2_1  | 2021-07-12 13:48:25,653 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState
datanode2_1  | 2021-07-12 13:48:25,653 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode2_1  | 2021-07-12 13:48:25,653 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2021-07-12 13:48:25,653 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3
datanode2_1  | 2021-07-12 13:48:25,690 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:25,764 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:26,059 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode2_1  | 2021-07-12 13:48:26,070 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2: ELECTION REJECTED received 1 response(s) and 1 exception(s):
datanode2_1  | 2021-07-12 13:48:26,071 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection:   Response 0: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:FAIL-t1
datanode2_1  | 2021-07-12 13:48:26,072 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode2_1  | 2021-07-12 13:48:26,072 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2 ELECTION round 0: result REJECTED
datanode2_1  | 2021-07-12 13:48:26,073 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode2_1  | 2021-07-12 13:48:26,073 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2
datanode2_1  | 2021-07-12 13:48:26,073 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState
datanode2_1  | 2021-07-12 13:48:26,071 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode2_1  | 2021-07-12 13:48:26,135 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3: ELECTION PASSED received 1 response(s) and 1 exception(s):
datanode2_1  | 2021-07-12 13:48:26,135 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection:   Response 0: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t1
datanode2_1  | 2021-07-12 13:48:26,135 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode2_1  | 2021-07-12 13:48:26,136 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3 ELECTION round 0: result PASSED
datanode2_1  | 2021-07-12 13:48:26,136 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3
datanode2_1  | 2021-07-12 13:48:26,136 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode2_1  | 2021-07-12 13:48:26,136 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:26,137 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 1 for becomeLeader, leader elected after 5618ms
datanode2_1  | 2021-07-12 13:48:26,137 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2021-07-12 13:48:26,137 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:26,138 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:26,139 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode2_1  | 2021-07-12 13:48:26,140 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode2_1  | 2021-07-12 13:48:26,140 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2021-07-12 13:48:26,140 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2021-07-12 13:48:26,141 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2021-07-12 13:48:26,173 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2021-07-12 13:48:26,174 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:26,176 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2021-07-12 13:48:26,209 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:26,236 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om3_1        | STARTUP_MSG:   java = 11.0.10
om3_1        | ************************************************************/
om3_1        | 2021-07-12 13:47:18,726 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2021-07-12 13:47:28,342 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2021-07-12 13:47:28,817 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2021-07-12 13:47:28,826 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2021-07-12 13:47:28,827 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2021-07-12 13:47:30,270 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2021-07-12 13:47:30,270 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2021-07-12 13:47:30,480 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2021-07-12 13:47:35,397 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om3_1        | 2021-07-12 13:47:38,877 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om3_1        | 2021-07-12 13:47:38,878 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om3_1        | 2021-07-12 13:47:38,879 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om3_1        | 2021-07-12 13:47:45,624 [main] INFO om.OzoneManager: Init response: GETCERT
om3_1        | 2021-07-12 13:47:46,182 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.113,host:om3
om3_1        | 2021-07-12 13:47:46,182 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om3_1        | 2021-07-12 13:47:46,203 [main] ERROR client.OMCertificateClient: Invalid domain om3
om3_1        | 2021-07-12 13:47:46,247 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2021-07-12 13:47:46,251 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2021-07-12 13:47:46,252 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2021-07-12 13:47:46,258 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2021-07-12 13:47:46,267 [main] INFO om.OzoneManager: Creating csr for OM->dns:om3,ip:172.25.0.113,scmId:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:om3
om3_1        | 2021-07-12 13:47:47,474 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om3_1        | value: 9862
om3_1        | ]
om3_1        | 2021-07-12 13:47:49,294 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om3_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd;layoutVersion=0
om3_1        | 2021-07-12 13:47:49,498 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1        | /************************************************************
om3_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om3/172.25.0.113
om3_1        | ************************************************************/
om3_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1        | 2021-07-12 13:48:00,459 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1        | /************************************************************
om3_1        | STARTUP_MSG: Starting OzoneManager
om3_1        | STARTUP_MSG:   host = om3/172.25.0.113
om3_1        | STARTUP_MSG:   args = []
om3_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om3_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om3_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om3_1        | STARTUP_MSG:   java = 11.0.10
om3_1        | ************************************************************/
om3_1        | 2021-07-12 13:48:00,529 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1        | 2021-07-12 13:48:10,371 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om3_1        | 2021-07-12 13:48:11,173 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1        | 2021-07-12 13:48:11,173 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om3: om3
om3_1        | 2021-07-12 13:48:11,174 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om3: om3
om3_1        | 2021-07-12 13:48:11,254 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2021-07-12 13:48:11,518 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om3_1        | 2021-07-12 13:48:13,120 [main] INFO reflections.Reflections: Reflections took 1243 ms to scan 1 urls, producing 94 keys and 256 values [using 2 cores]
om3_1        | 2021-07-12 13:48:14,726 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om3_1        | 2021-07-12 13:48:14,727 [main] INFO om.OzoneManager: Ozone Manager login successful.
om3_1        | 2021-07-12 13:48:14,727 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2021-07-12 13:48:22,758 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om3_1        | 2021-07-12 13:48:23,464 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om3_1        | 2021-07-12 13:48:23,486 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-3115382315205.crt.
om3_1        | 2021-07-12 13:48:23,508 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/3219757494510.crt.
om3_1        | 2021-07-12 13:48:23,722 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1        | 2021-07-12 13:48:24,753 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1        | 2021-07-12 13:48:24,777 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1        | 2021-07-12 13:48:26,486 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om3_1        | 2021-07-12 13:48:26,486 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om3_1        | 2021-07-12 13:48:27,001 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
om3_1        | 2021-07-12 13:48:27,050 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2021-07-12 13:48:27,060 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1        | 2021-07-12 13:48:27,093 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1        | 2021-07-12 13:48:27,864 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1        | 2021-07-12 13:48:28,098 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1        | 2021-07-12 13:48:28,216 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om3:9872, om1:9872, om2:9872
om3_1        | 2021-07-12 13:48:28,249 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1        | 2021-07-12 13:48:29,142 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1        | 2021-07-12 13:48:29,417 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om3_1        | 2021-07-12 13:48:29,418 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2021-07-12 13:48:29,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om3_1        | 2021-07-12 13:48:29,423 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2021-07-12 13:48:29,423 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1        | 2021-07-12 13:48:29,424 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1        | 2021-07-12 13:48:29,427 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2021-07-12 13:48:29,432 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1        | 2021-07-12 13:48:29,435 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1        | 2021-07-12 13:48:33,572 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1        | 2021-07-12 13:48:33,584 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2021-07-12 13:48:33,585 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2021-07-12 13:48:33,630 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2021-07-12 13:48:33,655 [main] INFO server.RaftServer: om3: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@16c6d5a2[Not completed]
om3_1        | 2021-07-12 13:48:33,656 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1        | 2021-07-12 13:48:33,767 [pool-24-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om3_1        | 2021-07-12 13:48:33,775 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1        | 2021-07-12 13:48:33,783 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1        | 2021-07-12 13:48:33,783 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1        | 2021-07-12 13:48:33,784 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1        | 2021-07-12 13:48:33,784 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 2021-07-12 13:48:33,788 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | Sleeping for 5 seconds
om1_1        | Waiting for the service scm3.org:9894
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2021-07-12 13:47:18,366 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = [--init]
om1_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om1_1        | STARTUP_MSG:   java = 11.0.10
om1_1        | ************************************************************/
om1_1        | 2021-07-12 13:47:18,442 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2021-07-12 13:47:28,480 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2021-07-12 13:47:28,784 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2021-07-12 13:47:28,786 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2021-07-12 13:47:28,792 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2021-07-12 13:47:30,728 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2021-07-12 13:47:30,756 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2021-07-12 13:47:30,816 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2021-07-12 13:47:35,353 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om1_1        | 2021-07-12 13:47:39,154 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om1_1        | 2021-07-12 13:47:39,154 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om1_1        | 2021-07-12 13:47:39,156 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om1_1        | 2021-07-12 13:47:41,939 [main] INFO om.OzoneManager: Init response: GETCERT
om1_1        | 2021-07-12 13:47:42,352 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.111,host:om1
om1_1        | 2021-07-12 13:47:42,352 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om1_1        | 2021-07-12 13:47:42,377 [main] ERROR client.OMCertificateClient: Invalid domain om1
om1_1        | 2021-07-12 13:47:42,377 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2021-07-12 13:47:42,384 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2021-07-12 13:47:42,384 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2021-07-12 13:47:42,384 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2021-07-12 13:47:42,404 [main] INFO om.OzoneManager: Creating csr for OM->dns:om1,ip:172.25.0.111,scmId:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:om1
om1_1        | 2021-07-12 13:47:43,473 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om1_1        | value: 9862
om1_1        | ]
om1_1        | 2021-07-12 13:47:45,925 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om1_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd;layoutVersion=0
om1_1        | 2021-07-12 13:47:46,095 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1        | /************************************************************
om1_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om1/172.25.0.111
om1_1        | ************************************************************/
om1_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1        | 2021-07-12 13:47:56,651 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1        | /************************************************************
om1_1        | STARTUP_MSG: Starting OzoneManager
om1_1        | STARTUP_MSG:   host = om1/172.25.0.111
om1_1        | STARTUP_MSG:   args = []
om1_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om1_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om1_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om1_1        | STARTUP_MSG:   java = 11.0.10
om1_1        | ************************************************************/
om1_1        | 2021-07-12 13:47:56,754 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1        | 2021-07-12 13:48:05,256 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om1_1        | 2021-07-12 13:48:05,740 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1        | 2021-07-12 13:48:05,746 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om1: om1
om1_1        | 2021-07-12 13:48:05,746 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om1: om1
om1_1        | 2021-07-12 13:48:05,772 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2021-07-12 13:48:05,994 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
kdc_1        | Jul 12 14:03:33 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098609, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:37 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098609, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:41 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098609, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:44 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098609, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser2/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:47 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:03:51 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:03:55 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:00 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:04 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:08 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:12 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:16 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:20 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:24 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098627, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:27 kdc krb5kdc[7](info): AS_REQ (8 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23), camellia128-cts-cmac(25), camellia256-cts-cmac(26)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1        | Jul 12 14:04:30 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:34 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:38 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
datanode2_1  | 2021-07-12 13:48:26,239 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2021-07-12 13:48:26,240 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: receive requestVote(ELECTION, 5b90731c-5249-453a-9f10-ddd353e45997, group-ABE0FEB7C4D8, 1, (t:0, i:0))
datanode2_1  | 2021-07-12 13:48:26,244 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:26,245 [grpc-default-executor-0] INFO impl.VoteContext: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FOLLOWER: reject ELECTION from 5b90731c-5249-453a-9f10-ddd353e45997: already has voted for e7ed1272-c70a-4bce-ab84-6d03b85621dc at current term 1
datanode2_1  | 2021-07-12 13:48:26,251 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8 replies to ELECTION vote request: 5b90731c-5249-453a-9f10-ddd353e45997<-e7ed1272-c70a-4bce-ab84-6d03b85621dc#0:FAIL-t1. Peer's state: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8:t1, leader=null, voted=e7ed1272-c70a-4bce-ab84-6d03b85621dc, raftlog=e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:26,252 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:26,304 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2021-07-12 13:48:26,309 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:26,315 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2021-07-12 13:48:26,316 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode2_1  | 2021-07-12 13:48:26,317 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2021-07-12 13:48:26,317 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:26,323 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl
datanode2_1  | 2021-07-12 13:48:26,333 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2021-07-12 13:48:26,338 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0
datanode2_1  | 2021-07-12 13:48:26,365 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderElection3] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:28,829 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:31,126 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.FollowerState: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5052921986ns, electionTimeout:5024ms
datanode2_1  | 2021-07-12 13:48:31,126 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState
datanode2_1  | 2021-07-12 13:48:31,127 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode2_1  | 2021-07-12 13:48:31,127 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2021-07-12 13:48:31,127 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4
datanode2_1  | 2021-07-12 13:48:31,144 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:31,196 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode2_1  | 2021-07-12 13:48:31,237 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4: ELECTION PASSED received 1 response(s) and 1 exception(s):
datanode2_1  | 2021-07-12 13:48:31,238 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection:   Response 0: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t2
datanode2_1  | 2021-07-12 13:48:31,239 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode2_1  | 2021-07-12 13:48:31,239 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.LeaderElection: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4 ELECTION round 0: result PASSED
datanode2_1  | 2021-07-12 13:48:31,239 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4
datanode1_1  | 2021-07-12 13:48:16,061 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
kdc_1        | Jul 12 14:04:42 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:47 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:51 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:04:57 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:05:01 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:05:05 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1        | Jul 12 14:05:12 kdc krb5kdc[7](info): TGS_REQ (6 etypes {aes256-cts-hmac-sha1-96(18), aes128-cts-hmac-sha1-96(17), aes256-cts-hmac-sha384-192(20), aes128-cts-hmac-sha256-128(19), DEPRECATED:des3-cbc-sha1(16), DEPRECATED:arcfour-hmac(23)}) 172.25.0.117: ISSUE: authtime 1626098667, etypes {rep=aes256-cts-hmac-sha1-96(18), tkt=aes256-cts-hmac-sha1-96(18), ses=aes256-cts-hmac-sha1-96(18)}, testuser/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
om1_1        | 2021-07-12 13:48:07,938 [main] INFO reflections.Reflections: Reflections took 1619 ms to scan 1 urls, producing 94 keys and 256 values [using 2 cores]
om1_1        | 2021-07-12 13:48:09,330 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om1_1        | 2021-07-12 13:48:09,330 [main] INFO om.OzoneManager: Ozone Manager login successful.
om1_1        | 2021-07-12 13:48:09,331 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2021-07-12 13:48:18,354 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om1_1        | 2021-07-12 13:48:19,208 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om1_1        | 2021-07-12 13:48:19,229 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-3115382315205.crt.
om1_1        | 2021-07-12 13:48:19,250 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/3216053955267.crt.
om1_1        | 2021-07-12 13:48:19,566 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1        | 2021-07-12 13:48:20,634 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1        | 2021-07-12 13:48:20,644 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1        | 2021-07-12 13:48:22,163 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om1_1        | 2021-07-12 13:48:22,165 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om1_1        | 2021-07-12 13:48:22,733 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
om1_1        | 2021-07-12 13:48:22,795 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2021-07-12 13:48:22,797 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1        | 2021-07-12 13:48:22,802 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1        | 2021-07-12 13:48:23,772 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1        | 2021-07-12 13:48:23,934 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1        | 2021-07-12 13:48:24,101 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om1:9872, om2:9872, om3:9872
om1_1        | 2021-07-12 13:48:24,180 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1        | 2021-07-12 13:48:25,673 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1        | 2021-07-12 13:48:26,203 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om1_1        | 2021-07-12 13:48:26,210 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2021-07-12 13:48:26,213 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om1_1        | 2021-07-12 13:48:26,217 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2021-07-12 13:48:26,217 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1        | 2021-07-12 13:48:26,219 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1        | 2021-07-12 13:48:26,238 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2021-07-12 13:48:26,247 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1        | 2021-07-12 13:48:26,248 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2021-07-12 13:48:31,098 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1        | 2021-07-12 13:48:31,101 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2021-07-12 13:48:31,108 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2021-07-12 13:48:31,424 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2021-07-12 13:48:31,614 [main] INFO server.RaftServer: om1: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@54d62c35[Not completed]
om1_1        | 2021-07-12 13:48:31,620 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1        | 2021-07-12 13:48:31,850 [pool-24-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om1_1        | 2021-07-12 13:48:31,890 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1        | 2021-07-12 13:48:31,913 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1        | 2021-07-12 13:48:31,913 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1        | 2021-07-12 13:48:31,913 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1        | 2021-07-12 13:48:31,928 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2021-07-12 13:48:31,929 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1        | 2021-07-12 13:48:31,952 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2021-07-12 13:48:31,993 [pool-24-thread-1] INFO server.RaftServer$Division: om1@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om1_1        | 2021-07-12 13:48:31,993 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1        | 2021-07-12 13:48:32,028 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2021-07-12 13:48:32,031 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om1_1        | 2021-07-12 13:48:32,111 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1        | 2021-07-12 13:48:32,143 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om1
om1_1        | 2021-07-12 13:48:32,208 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1        | 2021-07-12 13:48:32,286 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om1_1        | 2021-07-12 13:48:32,290 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1        | 2021-07-12 13:48:32,318 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1        | 2021-07-12 13:48:32,375 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1        | 2021-07-12 13:48:32,376 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2021-07-12 13:48:32,434 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:32,608 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2021-07-12 13:48:32,630 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1        | 2021-07-12 13:48:32,631 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1        | 2021-07-12 13:48:32,657 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om1_1        | 2021-07-12 13:48:32,657 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1        | 2021-07-12 13:48:32,659 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1        | 2021-07-12 13:48:32,664 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1        | 2021-07-12 13:48:32,668 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1        | 2021-07-12 13:48:32,669 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1        | 2021-07-12 13:48:32,671 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode2_1  | 2021-07-12 13:48:31,249 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode2_1  | 2021-07-12 13:48:31,249 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ABE0FEB7C4D8 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode2_1  | 2021-07-12 13:48:31,250 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 2 for becomeLeader, leader elected after 15212ms
datanode2_1  | 2021-07-12 13:48:31,250 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode2_1  | 2021-07-12 13:48:31,258 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:31,258 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode2_1  | 2021-07-12 13:48:31,260 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode2_1  | 2021-07-12 13:48:31,261 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode2_1  | 2021-07-12 13:48:31,261 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode2_1  | 2021-07-12 13:48:31,261 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode2_1  | 2021-07-12 13:48:31,261 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2021-07-12 13:48:31,272 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:31,273 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2021-07-12 13:48:31,273 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode2_1  | 2021-07-12 13:48:31,273 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2021-07-12 13:48:31,273 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:31,274 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8
datanode2_1  | 2021-07-12 13:48:31,274 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode2_1  | 2021-07-12 13:48:31,287 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode2_1  | 2021-07-12 13:48:31,288 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode2_1  | 2021-07-12 13:48:31,288 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode2_1  | 2021-07-12 13:48:31,288 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode2_1  | 2021-07-12 13:48:31,289 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode2_1  | 2021-07-12 13:48:31,300 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderStateImpl
datanode2_1  | 2021-07-12 13:48:31,304 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode2_1  | 2021-07-12 13:48:31,309 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/current/log_inprogress_0
datanode2_1  | 2021-07-12 13:48:31,445 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8-LeaderElection4] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:31,652 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl send StartLeaderElectionRequest to follower:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 on term:1 because follower's priority:1 is higher than leader's:0 and follower's lastEntry index:0 catch up with leader's:0
datanode2_1  | 2021-07-12 13:48:31,901 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:34,973 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:36,861 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl send StartLeaderElectionRequest to follower:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 on term:1 because follower's priority:1 is higher than leader's:0 and follower's lastEntry index:0 catch up with leader's:0
datanode2_1  | 2021-07-12 13:48:38,041 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:48:33,791 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1        | 2021-07-12 13:48:33,809 [pool-24-thread-1] INFO server.RaftServer$Division: om3@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om3_1        | 2021-07-12 13:48:33,816 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1        | 2021-07-12 13:48:33,862 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1        | 2021-07-12 13:48:33,867 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1        | 2021-07-12 13:48:33,873 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om3_1        | 2021-07-12 13:48:33,946 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1        | 2021-07-12 13:48:33,967 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om3
om3_1        | 2021-07-12 13:48:34,102 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om3_1        | 2021-07-12 13:48:34,107 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1        | 2021-07-12 13:48:34,155 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1        | 2021-07-12 13:48:34,200 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1        | 2021-07-12 13:48:34,212 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1        | 2021-07-12 13:48:34,239 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om3@group-562213E44849
om3_1        | 2021-07-12 13:48:34,302 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2021-07-12 13:48:34,369 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1        | 2021-07-12 13:48:34,372 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1        | 2021-07-12 13:48:34,545 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om3@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om3_1        | 2021-07-12 13:48:34,549 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1        | 2021-07-12 13:48:34,559 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1        | 2021-07-12 13:48:34,561 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1        | 2021-07-12 13:48:34,565 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1        | 2021-07-12 13:48:34,566 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2021-07-12 13:48:16,092 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2021-07-12 13:48:16,219 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2021-07-12 13:48:16,228 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2021-07-12 13:48:16,270 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C
datanode1_1  | 2021-07-12 13:48:16,365 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:16,434 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2021-07-12 13:48:16,436 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2021-07-12 13:48:16,478 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7ef1c9bf-43c8-43b8-b00a-7844fdf8175c
datanode1_1  | 2021-07-12 13:48:16,493 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2021-07-12 13:48:16,493 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2021-07-12 13:48:16,494 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:16,496 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2021-07-12 13:48:16,501 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2021-07-12 13:48:16,509 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2021-07-12 13:48:16,511 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2021-07-12 13:48:16,511 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2021-07-12 13:48:16,582 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:16,584 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:16,614 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:16,627 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:16,653 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:16,658 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2021-07-12 13:48:16,665 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2021-07-12 13:48:16,666 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2021-07-12 13:48:16,669 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2021-07-12 13:48:16,675 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2021-07-12 13:48:16,860 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C
datanode1_1  | 2021-07-12 13:48:16,890 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C
datanode1_1  | 2021-07-12 13:48:16,968 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: start as a follower, conf=-1: [5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null
datanode1_1  | 2021-07-12 13:48:16,969 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2021-07-12 13:48:16,980 [pool-23-thread-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState
datanode1_1  | 2021-07-12 13:48:17,025 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7844FDF8175C,id=5b90731c-5249-453a-9f10-ddd353e45997
datanode1_1  | 2021-07-12 13:48:17,026 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C
datanode1_1  | 2021-07-12 13:48:17,173 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7ef1c9bf-43c8-43b8-b00a-7844fdf8175c
datanode1_1  | 2021-07-12 13:48:17,175 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=7ef1c9bf-43c8-43b8-b00a-7844fdf8175c.
datanode1_1  | 2021-07-12 13:48:17,178 [Command processor thread] INFO server.RaftServer: 5b90731c-5249-453a-9f10-ddd353e45997: addNew group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-ABE0FEB7C4D8:java.util.concurrent.CompletableFuture@d3c9771[Not completed]
datanode1_1  | 2021-07-12 13:48:17,209 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997: new RaftServerImpl for group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode1_1  | 2021-07-12 13:48:17,215 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2021-07-12 13:48:17,215 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2021-07-12 13:48:17,215 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2021-07-12 13:48:17,216 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:17,216 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2021-07-12 13:48:17,216 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2021-07-12 13:48:17,223 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2021-07-12 13:48:17,233 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2021-07-12 13:48:17,237 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2021-07-12 13:48:17,237 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2021-07-12 13:48:17,237 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 does not exist. Creating ...
datanode1_1  | 2021-07-12 13:48:17,260 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/in_use.lock acquired by nodename 8@27bc097164fa
datanode1_1  | 2021-07-12 13:48:17,268 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 has been successfully formatted.
datanode1_1  | 2021-07-12 13:48:17,269 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-ABE0FEB7C4D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2021-07-12 13:48:17,269 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:17,269 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2021-07-12 13:48:17,270 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2021-07-12 13:48:17,270 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2021-07-12 13:48:17,270 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8
datanode1_1  | 2021-07-12 13:48:17,270 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:17,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2021-07-12 13:48:17,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2021-07-12 13:48:17,271 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode1_1  | 2021-07-12 13:48:17,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2021-07-12 13:48:17,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2021-07-12 13:48:17,289 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:17,292 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2021-07-12 13:48:17,292 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2021-07-12 13:48:17,292 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2021-07-12 13:48:17,294 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2021-07-12 13:48:17,294 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2021-07-12 13:48:17,321 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:17,323 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:17,327 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:17,328 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:17,354 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:17,354 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2021-07-12 13:48:17,354 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2021-07-12 13:48:17,355 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2021-07-12 13:48:17,355 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2021-07-12 13:48:17,356 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2021-07-12 13:48:27,673 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:30,745 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:31,151 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed requestVote e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:548)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:172)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:394)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:31,673 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed startLeaderElection e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.startLeaderElection(RaftServerProxy.java:553)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.startLeaderElection(GrpcServerProtocolService.java:185)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:398)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 2021-07-12 13:48:32,673 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1        | 2021-07-12 13:48:32,677 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1        | 2021-07-12 13:48:32,727 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1        | 2021-07-12 13:48:32,728 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2021-07-12 13:48:32,758 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1        | 2021-07-12 13:48:32,767 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1        | 2021-07-12 13:48:32,783 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1        | 2021-07-12 13:48:32,783 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1        | 2021-07-12 13:48:32,789 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1        | 2021-07-12 13:48:32,813 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1        | 2021-07-12 13:48:32,819 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1        | 2021-07-12 13:48:32,824 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1        | 2021-07-12 13:48:33,010 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:33,033 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:33,252 [Listener at om1/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om1_1        | 2021-07-12 13:48:33,546 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1        | 2021-07-12 13:48:33,593 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1        | 2021-07-12 13:48:33,593 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1        | 2021-07-12 13:48:33,769 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.25.0.111:9862
om1_1        | 2021-07-12 13:48:33,776 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1        | 2021-07-12 13:48:33,792 [Listener at om1/9862] INFO server.RaftServer$Division: om1@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2021-07-12 13:48:33,797 [Listener at om1/9862] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1        | 2021-07-12 13:48:33,800 [Listener at om1/9862] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2021-07-12 13:48:33,803 [Listener at om1/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om1
om1_1        | 2021-07-12 13:48:33,820 [Listener at om1/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:33,845 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1        | 2021-07-12 13:48:33,996 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1        | 2021-07-12 13:48:34,010 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$392/0x000000084059b440@79927a7c] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1        | 2021-07-12 13:48:34,015 [Listener at om1/9862] INFO om.OzoneManager: Starting OM block token secret manager
om1_1        | 2021-07-12 13:48:34,018 [Listener at om1/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2021-07-12 13:48:34,020 [Listener at om1/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om1_1        | 2021-07-12 13:48:34,020 [Listener at om1/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om1_1        | 2021-07-12 13:48:34,040 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1        | 2021-07-12 13:48:34,051 [Thread[Thread-16,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om1_1        | 2021-07-12 13:48:34,227 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1        | 2021-07-12 13:48:34,228 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om1_1        | 2021-07-12 13:48:34,228 [Listener at om1/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om1_1        | 2021-07-12 13:48:34,367 [Listener at om1/9862] INFO util.log: Logging initialized @47009ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1        | 2021-07-12 13:48:34,947 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1        | 2021-07-12 13:48:34,975 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1        | 2021-07-12 13:48:34,984 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1        | 2021-07-12 13:48:34,986 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1        | 2021-07-12 13:48:34,988 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1        | 2021-07-12 13:48:35,013 [Listener at om1/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om1_1        | 2021-07-12 13:48:35,225 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1        | 2021-07-12 13:48:35,229 [Listener at om1/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm1.org_1   | Sleeping for 5 seconds
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2021-07-12 13:45:50,507 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = [--init]
scm1.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.10
scm1.org_1   | ************************************************************/
scm1.org_1   | 2021-07-12 13:45:50,661 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2021-07-12 13:45:51,269 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2021-07-12 13:45:51,583 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2021-07-12 13:45:51,583 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2021-07-12 13:45:52,231 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2021-07-12 13:45:52,257 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2021-07-12 13:45:52,374 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm1.org_1   | 2021-07-12 13:45:56,751 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm1.org_1   | 2021-07-12 13:45:56,769 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm1.org_1   | 2021-07-12 13:45:56,780 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm1.org_1   | 2021-07-12 13:46:00,647 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm1.org_1   | 2021-07-12 13:46:03,337 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2021-07-12 13:46:03,338 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2021-07-12 13:46:03,858 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.116,host:scm1.org
scm1.org_1   | 2021-07-12 13:46:03,859 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm1.org_1   | 2021-07-12 13:46:03,872 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm1.org,scmId:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:scm-sub@scm1.org
scm1.org_1   | 2021-07-12 13:46:04,269 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm1.org_1   | 2021-07-12 13:46:04,837 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2021-07-12 13:46:05,153 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm1.org_1   | 2021-07-12 13:46:05,154 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:05,154 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm1.org_1   | 2021-07-12 13:46:05,154 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:05,155 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:05,165 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2021-07-12 13:46:05,166 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:05,168 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2021-07-12 13:46:05,169 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2021-07-12 13:46:05,994 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2021-07-12 13:46:06,001 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2021-07-12 13:46:06,002 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2021-07-12 13:46:06,014 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2021-07-12 13:46:06,039 [main] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: addNew group-EE404DDD1BCD:[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|priority:0] returns group-EE404DDD1BCD:java.util.concurrent.CompletableFuture@1f03fba0[Not completed]
scm1.org_1   | 2021-07-12 13:46:06,165 [pool-2-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: new RaftServerImpl for group-EE404DDD1BCD:[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|priority:0] with SCMStateMachine:uninitialized
scm1.org_1   | 2021-07-12 13:46:06,170 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2021-07-12 13:46:06,171 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2021-07-12 13:46:06,171 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2021-07-12 13:46:06,172 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode2_1  | 2021-07-12 13:48:41,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:42,062 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl send StartLeaderElectionRequest to follower:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 on term:1 because follower's priority:1 is higher than leader's:0 and follower's lastEntry index:0 catch up with leader's:0
datanode2_1  | 2021-07-12 13:48:44,167 [grpc-default-executor-0] INFO leader.FollowerInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: nextIndex: updateUnconditionally 1 -> 0
datanode2_1  | 2021-07-12 13:48:44,189 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:44,196 [grpc-default-executor-0] INFO leader.FollowerInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: nextIndex: updateUnconditionally 1 -> 0
datanode2_1  | 2021-07-12 13:48:47,257 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:47,263 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl send StartLeaderElectionRequest to follower:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 on term:1 because follower's priority:1 is higher than leader's:0 and follower's lastEntry index:0 catch up with leader's:0
datanode2_1  | 2021-07-12 13:48:47,287 [Thread-59] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl received success reply of StartLeaderElectionRequest from follower:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode2_1  | 2021-07-12 13:48:48,261 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: receive requestVote(ELECTION, 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, group-562B256BCC94, 2, (t:1, i:0))
datanode2_1  | 2021-07-12 13:48:48,262 [grpc-default-executor-0] INFO impl.VoteContext: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LEADER: accept ELECTION from 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: our priority 0 <= candidate's priority 1
datanode2_1  | 2021-07-12 13:48:48,262 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: change Leader from e7ed1272-c70a-4bce-ab84-6d03b85621dc to null at term 2 for updateCurrentTerm
datanode2_1  | 2021-07-12 13:48:48,262 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: changes role from    LEADER to FOLLOWER at term 2 for candidate:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode2_1  | 2021-07-12 13:48:48,262 [grpc-default-executor-0] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: shutdown e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-LeaderStateImpl
datanode2_1  | 2021-07-12 13:48:48,263 [grpc-default-executor-0] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis_grpc.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:48,265 [grpc-default-executor-0] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis_grpc.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:48,265 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode2_1  | 2021-07-12 13:48:48,265 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->5b90731c-5249-453a-9f10-ddd353e45997-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->5b90731c-5249-453a-9f10-ddd353e45997-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode2_1  | 2021-07-12 13:48:48,266 [grpc-default-executor-0] INFO impl.PendingRequests: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-PendingRequests: sendNotLeaderResponses
datanode2_1  | 2021-07-12 13:48:48,274 [grpc-default-executor-0] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:48,275 [grpc-default-executor-0] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94
datanode2_1  | 2021-07-12 13:48:48,279 [grpc-default-executor-1] INFO server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->5b90731c-5249-453a-9f10-ddd353e45997-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode2_1  | 2021-07-12 13:48:48,279 [grpc-default-executor-0] INFO impl.RoleInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc: start e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-FollowerState
datanode2_1  | 2021-07-12 13:48:48,286 [grpc-default-executor-2] INFO server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode2_1  | 2021-07-12 13:48:48,287 [grpc-default-executor-2] INFO leader.FollowerInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: nextIndex: updateUnconditionally 1 -> 0
datanode2_1  | 2021-07-12 13:48:48,298 [grpc-default-executor-0] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94 replies to ELECTION vote request: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7<-e7ed1272-c70a-4bce-ab84-6d03b85621dc#0:OK-t2. Peer's state: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94:t2, leader=null, voted=8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, raftlog=e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:48,299 [grpc-default-executor-1] INFO leader.FollowerInfo: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94->5b90731c-5249-453a-9f10-ddd353e45997: nextIndex: updateUnconditionally 1 -> 0
datanode2_1  | 2021-07-12 13:48:48,381 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode2_1  | 2021-07-12 13:48:48,381 [grpc-default-executor-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: change Leader from null to 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 at term 2 for appendEntries, leader elected after 119ms
datanode2_1  | 2021-07-12 13:48:48,435 [grpc-default-executor-1] INFO server.RaftServer$Division: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94: set configuration 1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode2_1  | 2021-07-12 13:48:48,436 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode2_1  | 2021-07-12 13:48:48,439 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0 to /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_0-0
datanode2_1  | 2021-07-12 13:48:48,443 [e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_1
datanode2_1  | 2021-07-12 13:48:50,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:53,401 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:56,473 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:48:58,546 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode2_1  | java.net.NoRouteToHostException: No Route to Host from  f6c2a59662b6/172.25.0.103 to recon:9891 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
datanode2_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode2_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode2_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode2_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode2_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode2_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:855)
datanode2_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode2_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode2_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode2_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode2_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode2_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode2_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode2_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode2_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode2_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | Caused by: java.net.NoRouteToHostException: No route to host
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:33,817 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:36,870 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed startLeaderElection e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.startLeaderElection(RaftServerProxy.java:553)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.startLeaderElection(GrpcServerProtocolService.java:185)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:398)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:36,889 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:39,961 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:42,066 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Failed startLeaderElection e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#0
datanode3_1  | org.apache.ratis.protocol.exceptions.GroupMismatchException: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-562B256BCC94 not found.
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:147)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:339)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:348)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:343)
datanode3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.startLeaderElection(RaftServerProxy.java:553)
datanode3_1  | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.startLeaderElection(GrpcServerProtocolService.java:185)
datanode3_1  | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:398)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | 2021-07-12 13:48:42,144 [Command processor thread] INFO server.RaftServer: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: addNew group-C4D54606D42F:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1] returns group-C4D54606D42F:java.util.concurrent.CompletableFuture@13706b0d[Not completed]
datanode3_1  | 2021-07-12 13:48:42,190 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: new RaftServerImpl for group-C4D54606D42F:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1] with ContainerStateMachine:uninitialized
datanode3_1  | 2021-07-12 13:48:42,193 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2021-07-12 13:48:42,198 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2021-07-12 13:48:42,198 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2021-07-12 13:48:42,199 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:42,199 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1        | 2021-07-12 13:48:35,438 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1        | 2021-07-12 13:48:35,438 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1        | 2021-07-12 13:48:35,450 [Listener at om1/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1        | 2021-07-12 13:48:35,487 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2021-07-12 13:48:35,490 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79228440{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | Sleeping for 5 seconds
s3g_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1        | 2021-07-12 13:45:48,563 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1        | 2021-07-12 13:45:48,592 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
s3g_1        | 2021-07-12 13:45:48,593 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.s3g.http.auth.type = kerberos
s3g_1        | 2021-07-12 13:45:48,808 [main] INFO util.log: Logging initialized @6409ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1        | 2021-07-12 13:45:49,364 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1        | 2021-07-12 13:45:49,398 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1        | 2021-07-12 13:45:49,413 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1        | 2021-07-12 13:45:49,416 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1        | 2021-07-12 13:45:49,417 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1        | 2021-07-12 13:45:49,429 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.s3g.http.auth.kerberos.principal keytabKey: ozone.s3g.http.auth.kerberos.keytab
s3g_1        | 2021-07-12 13:45:49,957 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1        | /************************************************************
s3g_1        | STARTUP_MSG: Starting Gateway
s3g_1        | STARTUP_MSG:   host = s3g/172.25.0.114
s3g_1        | STARTUP_MSG:   args = []
s3g_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om3_1        | 2021-07-12 13:48:34,569 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1        | 2021-07-12 13:48:34,581 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1        | 2021-07-12 13:48:34,582 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1        | 2021-07-12 13:48:34,661 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1        | 2021-07-12 13:48:34,666 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1        | 2021-07-12 13:48:34,696 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1        | 2021-07-12 13:48:34,705 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1        | 2021-07-12 13:48:34,725 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1        | 2021-07-12 13:48:34,726 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1        | 2021-07-12 13:48:34,730 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1        | 2021-07-12 13:48:34,750 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1        | 2021-07-12 13:48:34,760 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1        | 2021-07-12 13:48:34,764 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1        | 2021-07-12 13:48:34,908 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om3@group-562213E44849
om3_1        | 2021-07-12 13:48:34,948 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om3@group-562213E44849
om3_1        | 2021-07-12 13:48:35,130 [Listener at om3/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om3_1        | 2021-07-12 13:48:35,379 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1        | 2021-07-12 13:48:35,425 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1        | 2021-07-12 13:48:35,428 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1        | 2021-07-12 13:48:35,583 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.25.0.113:9862
om3_1        | 2021-07-12 13:48:35,587 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1        | 2021-07-12 13:48:35,589 [Listener at om3/9862] INFO server.RaftServer$Division: om3@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2021-07-12 13:48:35,600 [Listener at om3/9862] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1        | 2021-07-12 13:48:35,604 [Listener at om3/9862] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2021-07-12 13:48:35,606 [Listener at om3/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om3
om3_1        | 2021-07-12 13:48:35,610 [Listener at om3/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om3@group-562213E44849
om3_1        | 2021-07-12 13:48:35,654 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1        | 2021-07-12 13:48:35,830 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1        | 2021-07-12 13:48:35,835 [Listener at om3/9862] INFO om.OzoneManager: Starting OM block token secret manager
om3_1        | 2021-07-12 13:48:35,835 [Listener at om3/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2021-07-12 13:48:35,837 [Listener at om3/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om3_1        | 2021-07-12 13:48:35,837 [Listener at om3/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 2021-07-12 13:48:35,840 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$392/0x000000084059b440@4ae1ee85] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1        | 2021-07-12 13:48:35,843 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1        | 2021-07-12 13:48:35,844 [Thread[Thread-16,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om3_1        | 2021-07-12 13:48:35,949 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1        | 2021-07-12 13:48:35,957 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om3_1        | 2021-07-12 13:48:35,957 [Listener at om3/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om3_1        | 2021-07-12 13:48:36,019 [Listener at om3/9862] INFO util.log: Logging initialized @45232ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1        | 2021-07-12 13:48:36,251 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1        | 2021-07-12 13:48:36,265 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1        | 2021-07-12 13:48:36,267 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1        | 2021-07-12 13:48:36,267 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1        | 2021-07-12 13:48:36,267 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1        | 2021-07-12 13:48:36,274 [Listener at om3/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om3_1        | 2021-07-12 13:48:36,368 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1        | 2021-07-12 13:48:36,370 [Listener at om3/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om3_1        | 2021-07-12 13:48:36,473 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1        | 2021-07-12 13:48:36,473 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1        | 2021-07-12 13:48:36,480 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
om3_1        | 2021-07-12 13:48:36,528 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
s3g_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.0-SNAPSHOT.jar
s3g_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
s3g_1        | STARTUP_MSG:   java = 11.0.10
s3g_1        | ************************************************************/
s3g_1        | 2021-07-12 13:45:49,991 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1        | 2021-07-12 13:45:50,146 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1        | 2021-07-12 13:45:50,171 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1        | 2021-07-12 13:45:50,177 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1        | 2021-07-12 13:45:50,328 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1        | 2021-07-12 13:45:50,328 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1        | 2021-07-12 13:45:50,335 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1        | 2021-07-12 13:45:50,525 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | 2021-07-12 13:45:50,695 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@a5bd950{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1        | 2021-07-12 13:45:50,721 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52045dbe{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1        | WARNING: An illegal reflective access operation has occurred
s3g_1        | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1        | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1        | WARNING: All illegal access operations will be denied in a future release
s3g_1        | 2021-07-12 13:45:58,208 [main] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/s3g@EXAMPLE.COM
s3g_1        | Jul 12, 2021 1:46:01 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1        | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1        | 
s3g_1        | 2021-07-12 13:46:01,477 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@51af8049{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_0-SNAPSHOT_jar-_-any-6052684026476970751/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1        | 2021-07-12 13:46:01,518 [main] INFO server.AbstractConnector: Started ServerConnector@3918c187{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1        | 2021-07-12 13:46:01,518 [main] INFO server.Server: Started @19120ms
s3g_1        | 2021-07-12 13:46:01,520 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1        | 2021-07-12 13:55:05,801 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-19410, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:05,843 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-19410
s3g_1        | 2021-07-12 13:55:12,627 [qtp1771421544-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-59247, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:12,649 [qtp1771421544-17] INFO endpoint.BucketEndpoint: Location is /bucket-59247
s3g_1        | 2021-07-12 13:55:14,126 [qtp1771421544-20] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1        | 2021-07-12 13:55:14,139 [qtp1771421544-20] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1        | 2021-07-12 13:55:14,139 [qtp1771421544-20] INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
s3g_1        | 2021-07-12 13:55:14,146 [qtp1771421544-20] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode3_1  | 2021-07-12 13:48:42,199 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2021-07-12 13:48:42,200 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2021-07-12 13:48:42,209 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2021-07-12 13:48:42,210 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2021-07-12 13:48:42,217 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2021-07-12 13:48:42,220 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ff1246ee-6927-4ea3-b470-c4d54606d42f does not exist. Creating ...
datanode3_1  | 2021-07-12 13:48:42,232 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ff1246ee-6927-4ea3-b470-c4d54606d42f/in_use.lock acquired by nodename 8@abb35404295f
datanode3_1  | 2021-07-12 13:48:42,243 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ff1246ee-6927-4ea3-b470-c4d54606d42f has been successfully formatted.
datanode3_1  | 2021-07-12 13:48:42,267 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-C4D54606D42F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2021-07-12 13:48:42,271 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:42,272 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2021-07-12 13:48:42,300 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2021-07-12 13:48:42,302 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2021-07-12 13:48:42,304 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F
datanode3_1  | 2021-07-12 13:48:42,326 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,372 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2021-07-12 13:48:42,372 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2021-07-12 13:48:42,387 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ff1246ee-6927-4ea3-b470-c4d54606d42f
datanode3_1  | 2021-07-12 13:48:42,387 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2021-07-12 13:48:42,388 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2021-07-12 13:48:42,389 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,389 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2021-07-12 13:48:42,389 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2021-07-12 13:48:42,390 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2021-07-12 13:48:42,391 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2021-07-12 13:48:42,391 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2021-07-12 13:48:42,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,399 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:42,405 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2021-07-12 13:48:42,405 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2021-07-12 13:48:42,409 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:42,409 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2021-07-12 13:48:42,410 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2021-07-12 13:48:42,411 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2021-07-12 13:48:42,412 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2021-07-12 13:48:42,412 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2021-07-12 13:48:42,431 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F
datanode3_1  | 2021-07-12 13:48:42,435 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F
datanode3_1  | 2021-07-12 13:48:42,445 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null
datanode3_1  | 2021-07-12 13:48:42,446 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2021-07-12 13:48:42,448 [pool-23-thread-1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState
datanode3_1  | 2021-07-12 13:48:42,456 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C4D54606D42F,id=8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:42,457 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F
datanode3_1  | 2021-07-12 13:48:42,468 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ff1246ee-6927-4ea3-b470-c4d54606d42f
datanode3_1  | 2021-07-12 13:48:42,469 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=ff1246ee-6927-4ea3-b470-c4d54606d42f.
datanode3_1  | 2021-07-12 13:48:42,470 [Command processor thread] INFO server.RaftServer: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: addNew group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-ABE0FEB7C4D8:java.util.concurrent.CompletableFuture@4df57955[Not completed]
datanode3_1  | 2021-07-12 13:48:42,478 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: new RaftServerImpl for group-ABE0FEB7C4D8:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode3_1  | 2021-07-12 13:48:42,478 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2021-07-12 13:48:42,478 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2021-07-12 13:48:42,478 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2021-07-12 13:48:42,479 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:42,479 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2021-07-12 13:48:42,479 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2021-07-12 13:48:42,480 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2021-07-12 13:48:42,480 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2021-07-12 13:48:42,481 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2021-07-12 13:48:42,482 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode3_1  | 2021-07-12 13:48:42,482 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 does not exist. Creating ...
datanode3_1  | 2021-07-12 13:48:42,483 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/in_use.lock acquired by nodename 8@abb35404295f
datanode3_1  | 2021-07-12 13:48:42,485 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8 has been successfully formatted.
datanode3_1  | 2021-07-12 13:48:42,486 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-ABE0FEB7C4D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2021-07-12 13:48:42,486 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:42,486 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2021-07-12 13:48:42,486 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2021-07-12 13:48:42,487 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2021-07-12 13:48:35,494 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c3fd72{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1        | 2021-07-12 13:48:35,912 [Listener at om1/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om1_1        | 2021-07-12 13:48:35,955 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7960e87a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_0-SNAPSHOT_jar-_-any-14691979723537963729/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1        | 2021-07-12 13:48:35,984 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@54d93fce{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1        | 2021-07-12 13:48:35,984 [Listener at om1/9862] INFO server.Server: Started @48641ms
om1_1        | 2021-07-12 13:48:35,993 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1        | 2021-07-12 13:48:35,993 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1        | 2021-07-12 13:48:35,996 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1        | 2021-07-12 13:48:36,004 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1        | 2021-07-12 13:48:36,008 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1        | 2021-07-12 13:48:36,227 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om1_1        | 2021-07-12 13:48:36,247 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ea206bc] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1        | 2021-07-12 13:48:38,970 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5170772151ns, electionTimeout:5168ms
om1_1        | 2021-07-12 13:48:38,972 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2021-07-12 13:48:38,972 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1        | 2021-07-12 13:48:38,976 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2021-07-12 13:48:38,976 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection1
om1_1        | 2021-07-12 13:48:38,997 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2021-07-12 13:48:41,287 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.om1
om1_1        | 2021-07-12 13:48:41,344 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 1, (t:0, i:~))
om1_1        | 2021-07-12 13:48:41,346 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-562213E44849-CANDIDATE: reject ELECTION from om2: already has voted for om1 at current term 1
om1_1        | 2021-07-12 13:48:41,401 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om2<-om1#0:FAIL-t1. Peer's state: om1@group-562213E44849:t1, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2021-07-12 13:48:41,753 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om1_1        | 2021-07-12 13:48:41,754 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:FAIL-t1
om1_1        | 2021-07-12 13:48:41,754 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om1<-om2#0:FAIL-t1
om1_1        | 2021-07-12 13:48:41,754 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om1_1        | 2021-07-12 13:48:41,755 [om1@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om1_1        | 2021-07-12 13:48:41,764 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection1
om1_1        | 2021-07-12 13:48:41,765 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-562213E44849-FollowerState
om1_1        | 2021-07-12 13:48:42,122 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 1, (t:0, i:~))
om1_1        | 2021-07-12 13:48:42,123 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om1 at current term 1
om1_1        | 2021-07-12 13:48:42,123 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-562213E44849 replies to ELECTION vote request: om3<-om1#0:FAIL-t1. Peer's state: om1@group-562213E44849:t1, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2021-07-12 13:48:46,839 [om1@group-562213E44849-FollowerState] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073986172ns, electionTimeout:5072ms
om1_1        | 2021-07-12 13:48:46,840 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-FollowerState
om1_1        | 2021-07-12 13:48:46,840 [om1@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om1_1        | 2021-07-12 13:48:46,840 [om1@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1        | 2021-07-12 13:48:46,840 [om1@group-562213E44849-FollowerState] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderElection2
om1_1        | 2021-07-12 13:48:46,842 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om1_1        | 2021-07-12 13:48:46,897 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1        | 2021-07-12 13:48:46,898 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t2
om1_1        | 2021-07-12 13:48:46,898 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2 ELECTION round 0: result PASSED
om1_1        | 2021-07-12 13:48:46,898 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: shutdown om1@group-562213E44849-LeaderElection2
om1_1        | 2021-07-12 13:48:46,898 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2021-07-12 13:46:06,172 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2021-07-12 13:46:06,172 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2021-07-12 13:46:06,173 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2021-07-12 13:46:06,179 [pool-2-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: ConfigurationManager, init=-1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|priority:0], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2021-07-12 13:46:06,179 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2021-07-12 13:46:06,200 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2021-07-12 13:46:06,201 [pool-2-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd does not exist. Creating ...
scm1.org_1   | 2021-07-12 13:46:06,250 [pool-2-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/in_use.lock acquired by nodename 89@scm1.org
scm1.org_1   | 2021-07-12 13:46:06,263 [pool-2-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd has been successfully formatted.
datanode2_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode2_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
datanode2_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
datanode2_1  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode2_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)
datanode2_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:822)
datanode2_1  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
datanode2_1  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1647)
datanode2_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1463)
datanode2_1  | 	... 12 more
datanode2_1  | 2021-07-12 13:49:02,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:05,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:08,760 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:11,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:14,905 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:17,981 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:18,137 [ChunkWriter-8-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:3216053955267.
datanode2_1  | 2021-07-12 13:49:21,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:24,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1        | 2021-07-12 13:48:36,533 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5bbda8aa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1        | 2021-07-12 13:48:36,537 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@b729651{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1        | 2021-07-12 13:48:36,745 [Listener at om3/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om3_1        | 2021-07-12 13:48:36,766 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@14a3d15b{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_0-SNAPSHOT_jar-_-any-12722231274995443876/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1        | 2021-07-12 13:48:36,781 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@5d95c7c2{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1        | 2021-07-12 13:48:36,782 [Listener at om3/9862] INFO server.Server: Started @45995ms
om3_1        | 2021-07-12 13:48:36,794 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1        | 2021-07-12 13:48:36,794 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1        | 2021-07-12 13:48:36,797 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1        | 2021-07-12 13:48:36,805 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1        | 2021-07-12 13:48:36,801 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1        | 2021-07-12 13:48:36,903 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om3_1        | 2021-07-12 13:48:36,937 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7b6bcbee] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1        | 2021-07-12 13:48:40,790 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5189180241ns, electionTimeout:5146ms
om3_1        | 2021-07-12 13:48:40,791 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2021-07-12 13:48:40,797 [om3@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1        | 2021-07-12 13:48:40,806 [om3@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om3_1        | 2021-07-12 13:48:40,807 [om3@group-562213E44849-FollowerState] INFO impl.RoleInfo: om3: start om3@group-562213E44849-LeaderElection1
om3_1        | 2021-07-12 13:48:40,849 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2021-07-12 13:48:41,544 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.om3
om3_1        | 2021-07-12 13:48:41,568 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om2, group-562213E44849, 1, (t:0, i:~))
om3_1        | 2021-07-12 13:48:41,571 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-562213E44849-CANDIDATE: reject ELECTION from om2: already has voted for om3 at current term 1
om3_1        | 2021-07-12 13:48:41,575 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 1, (t:0, i:~))
om3_1        | 2021-07-12 13:48:41,602 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om2<-om3#0:FAIL-t1. Peer's state: om3@group-562213E44849:t1, leader=null, voted=om3, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2021-07-12 13:48:41,605 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-562213E44849-CANDIDATE: reject ELECTION from om1: already has voted for om3 at current term 1
om3_1        | 2021-07-12 13:48:41,613 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om1<-om3#0:FAIL-t1. Peer's state: om3@group-562213E44849:t1, leader=null, voted=om3, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om3_1        | 2021-07-12 13:48:42,160 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om3_1        | 2021-07-12 13:48:42,160 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t1
om3_1        | 2021-07-12 13:48:42,160 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t1
om3_1        | 2021-07-12 13:48:42,160 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om3_1        | 2021-07-12 13:48:42,161 [om3@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om3_1        | 2021-07-12 13:48:42,161 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-LeaderElection1
om3_1        | 2021-07-12 13:48:42,162 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2021-07-12 13:48:46,855 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 2, (t:0, i:~))
om3_1        | 2021-07-12 13:48:46,857 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-562213E44849-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om3_1        | 2021-07-12 13:48:46,857 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:om1
om3_1        | 2021-07-12 13:48:46,857 [grpc-default-executor-1] INFO impl.RoleInfo: om3: shutdown om3@group-562213E44849-FollowerState
om3_1        | 2021-07-12 13:48:46,857 [grpc-default-executor-1] INFO impl.RoleInfo: om3: start om3@group-562213E44849-FollowerState
om3_1        | 2021-07-12 13:48:46,858 [om3@group-562213E44849-FollowerState] INFO impl.FollowerState: om3@group-562213E44849-FollowerState was interrupted: {}
om3_1        | java.lang.InterruptedException: sleep interrupted
om3_1        | 	at java.base/java.lang.Thread.sleep(Native Method)
om3_1        | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
om3_1        | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
om3_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
om3_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
om3_1        | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
om3_1        | 2021-07-12 13:48:46,894 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849 replies to ELECTION vote request: om1<-om3#0:OK-t2. Peer's state: om3@group-562213E44849:t2, leader=null, voted=om1, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:06,272 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2021-07-12 13:46:06,275 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2021-07-12 13:46:06,301 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2021-07-12 13:46:06,301 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:06,323 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2021-07-12 13:46:06,325 [pool-2-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:06,326 [pool-2-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:06,347 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:06,355 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2021-07-12 13:46:06,364 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2021-07-12 13:46:06,370 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd
scm1.org_1   | 2021-07-12 13:46:06,371 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2021-07-12 13:46:06,371 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2021-07-12 13:46:06,380 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:06,381 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:06,382 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2021-07-12 13:46:06,383 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2021-07-12 13:46:06,383 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2021-07-12 13:46:06,384 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2021-07-12 13:46:06,400 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2021-07-12 13:46:06,407 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2021-07-12 13:46:06,419 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2021-07-12 13:46:06,419 [pool-2-thread-1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2021-07-12 13:46:06,428 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2021-07-12 13:46:06,430 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2021-07-12 13:46:06,430 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2021-07-12 13:46:06,431 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2021-07-12 13:46:06,432 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2021-07-12 13:46:06,440 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2021-07-12 13:46:06,484 [pool-2-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:06,485 [pool-2-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:06,491 [pool-2-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:06,492 [pool-2-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:06,501 [main] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: start as a follower, conf=-1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:06,506 [main] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1.org_1   | 2021-07-12 13:46:06,507 [main] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState
scm1.org_1   | 2021-07-12 13:46:06,694 [main] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EE404DDD1BCD,id=1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:06,695 [main] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:06,704 [main] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:06,718 [main] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start RPC server
scm1.org_1   | 2021-07-12 13:46:06,765 [main] INFO server.GrpcService: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: GrpcService started, listening on 9894
scm1.org_1   | 2021-07-12 13:46:06,769 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$331/0x000000084030cc40@64920dc2] INFO util.JvmPauseMonitor: JvmPauseMonitor-1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Started
scm1.org_1   | 2021-07-12 13:46:11,693 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.FollowerState: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5186088984ns, electionTimeout:5172ms
scm1.org_1   | 2021-07-12 13:46:11,694 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState
scm1.org_1   | 2021-07-12 13:46:11,695 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1.org_1   | 2021-07-12 13:46:11,698 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode2_1  | 2021-07-12 13:49:27,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:30,265 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:33,337 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:36,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:39,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:42,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:44,197 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=8,entriesCount=1,lastEntry=(t:2, i:0)
datanode2_1  | 2021-07-12 13:49:45,625 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:51,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:54,844 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:49:57,913 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:00,985 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:04,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:07,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:10,205 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:13,277 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:16,345 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:19,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:22,489 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:25,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:28,633 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:31,705 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:34,780 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:40,925 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:43,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:47,069 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:50,137 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1        | 2021-07-12 13:55:14,146 [qtp1771421544-20] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1        | 2021-07-12 13:55:14,621 [qtp1771421544-20] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1        | 2021-07-12 13:55:14,621 [qtp1771421544-20] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B8761490955C->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:14,622 [qtp1771421544-20] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:16,398 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DF77AC14C38C->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:55:16,398 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:27,262 [qtp1771421544-19] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-97549, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:27,280 [qtp1771421544-19] INFO endpoint.BucketEndpoint: Location is /bucket-97549
s3g_1        | 2021-07-12 13:55:27,909 [qtp1771421544-20] INFO rpc.RpcClient: Creating Bucket: s3v/boto-bucket999, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:27,928 [qtp1771421544-20] INFO endpoint.BucketEndpoint: Location is /boto-bucket999
s3g_1        | 2021-07-12 13:55:28,043 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-649F2F2E0096->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:55:28,043 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:30,778 [qtp1771421544-20] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-27562B208404->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:30,778 [qtp1771421544-20] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:31,009 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FA85EA03370A->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:31,010 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:31,927 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C5D67FEF6831->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:55:31,927 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:32,021 [qtp1771421544-20] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-6B3A1B423A2D->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:55:32,021 [qtp1771421544-20] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:36,143 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D69E974CAE6A->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:36,144 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:36,474 [qtp1771421544-24] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-6BF47D1913CC->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:36,474 [qtp1771421544-24] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:37,519 [qtp1771421544-19] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-wvddibmnwy, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:37,544 [qtp1771421544-19] INFO endpoint.BucketEndpoint: Location is /bucket-wvddibmnwy
s3g_1        | 2021-07-12 13:55:37,837 [qtp1771421544-19] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-343DA81DB1F0->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:37,837 [qtp1771421544-19] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:41,414 [qtp1771421544-21] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-656E8800C8E9->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:41,414 [qtp1771421544-21] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:43,086 [qtp1771421544-21] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3D06753FDA77->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:55:43,086 [qtp1771421544-21] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:55:51,429 [qtp1771421544-22] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-77473, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:51,450 [qtp1771421544-22] INFO endpoint.BucketEndpoint: Location is /bucket-77473
s3g_1        | 2021-07-12 13:55:52,017 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-05803, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:52,034 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-05803
s3g_1        | 2021-07-12 13:55:52,570 [qtp1771421544-22] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-77473, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:55:52,591 [qtp1771421544-22] INFO endpoint.BucketEndpoint: Location is /bucket-77473
s3g_1        | 2021-07-12 13:55:53,154 [qtp1771421544-22] ERROR endpoint.BucketEndpoint: Error in Create Bucket Request for bucket: bucket_1
s3g_1        | INVALID_BUCKET_NAME org.apache.hadoop.ozone.om.exceptions.OMException: Bucket or Volume name has an unsupported character : _
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.verifyBucketName(RpcClient.java:519)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.createBucket(RpcClient.java:463)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.createBucket(RpcClient.java:454)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneVolume.createBucket(OzoneVolume.java:385)
s3g_1        | 	at org.apache.hadoop.ozone.client.ObjectStore.createS3Bucket(ObjectStore.java:118)
scm1.org_1   | 2021-07-12 13:46:11,698 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1
scm1.org_1   | 2021-07-12 13:46:11,702 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.LeaderElection: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:11,703 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.LeaderElection: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1.org_1   | 2021-07-12 13:46:11,703 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1
scm1.org_1   | 2021-07-12 13:46:11,704 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1.org_1   | 2021-07-12 13:46:11,704 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: change Leader from null to 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9 at term 1 for becomeLeader, leader elected after 5431ms
scm1.org_1   | 2021-07-12 13:46:11,709 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2021-07-12 13:46:11,712 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:11,712 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:11,714 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2021-07-12 13:46:11,714 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2021-07-12 13:46:11,720 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2021-07-12 13:46:11,720 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2021-07-12 13:46:11,721 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2021-07-12 13:46:11,728 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl
scm1.org_1   | 2021-07-12 13:46:11,749 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Starting segment from index:0
scm1.org_1   | 2021-07-12 13:46:11,774 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:11,860 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0
scm1.org_1   | 2021-07-12 13:46:12,770 [main] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: close
scm1.org_1   | 2021-07-12 13:46:12,771 [main] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: shutdown
scm1.org_1   | 2021-07-12 13:46:12,771 [main] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-EE404DDD1BCD,id=1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:12,771 [main] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl
scm1.org_1   | 2021-07-12 13:46:12,776 [main] INFO impl.PendingRequests: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-PendingRequests: sendNotLeaderResponses
scm1.org_1   | 2021-07-12 13:46:12,777 [main] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,778 [main] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,779 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO impl.StateMachineUpdater: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater: Took a snapshot at index 0
scm1.org_1   | 2021-07-12 13:46:12,779 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO impl.StateMachineUpdater: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1.org_1   | 2021-07-12 13:46:12,787 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,788 [main] INFO impl.StateMachineUpdater: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater: set stopIndex = 0
scm1.org_1   | 2021-07-12 13:46:12,789 [main] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: closes. applyIndex: 0
scm1.org_1   | 2021-07-12 13:46:12,790 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1.org_1   | 2021-07-12 13:46:12,791 [main] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker close()
scm1.org_1   | 2021-07-12 13:46:12,791 [main] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,792 [main] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,792 [main] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:12,792 [main] INFO server.GrpcService: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown server with port 9894 now
scm1.org_1   | 2021-07-12 13:46:12,796 [main] INFO server.GrpcService: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown server with port 9894 successfully
om1_1        | 2021-07-12 13:48:46,899 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: change Leader from null to om1 at term 2 for becomeLeader, leader elected after 14608ms
om1_1        | 2021-07-12 13:48:46,918 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1        | 2021-07-12 13:48:46,925 [om1@group-562213E44849-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:46,945 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1        | 2021-07-12 13:48:46,945 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1        | 2021-07-12 13:48:46,958 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1        | 2021-07-12 13:48:46,959 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1        | 2021-07-12 13:48:46,960 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1        | 2021-07-12 13:48:46,983 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1        | 2021-07-12 13:48:46,985 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2021-07-12 13:48:46,988 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1        | 2021-07-12 13:48:46,993 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1        | 2021-07-12 13:48:46,996 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2021-07-12 13:48:46,997 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2021-07-12 13:48:46,997 [om1@group-562213E44849-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.om1@group-562213E44849
om1_1        | 2021-07-12 13:48:47,002 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1        | 2021-07-12 13:48:47,004 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1        | 2021-07-12 13:48:47,004 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1        | 2021-07-12 13:48:47,005 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1        | 2021-07-12 13:48:47,005 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1        | 2021-07-12 13:48:47,005 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1        | 2021-07-12 13:48:47,008 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: start om1@group-562213E44849-LeaderStateImpl
om1_1        | 2021-07-12 13:48:47,085 [om1@group-562213E44849-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om1_1        | 2021-07-12 13:48:47,181 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServer$Division: om1@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om1_1        | 2021-07-12 13:48:47,622 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om1_1        | 2021-07-12 13:48:55,067 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47280
om1_1        | 2021-07-12 13:48:55,094 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:10,965 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47344
om1_1        | 2021-07-12 13:49:10,978 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:12,086 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:testuser/scm@EXAMPLE.COM
om1_1        | 2021-07-12 13:49:25,643 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47420
om1_1        | 2021-07-12 13:49:25,658 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:26,295 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47424
om1_1        | 2021-07-12 13:49:26,301 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:31,804 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47458
om1_1        | 2021-07-12 13:49:31,834 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:32,542 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47462
om1_1        | 2021-07-12 13:49:32,557 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:37,552 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47484
om1_1        | 2021-07-12 13:49:37,573 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:45,896 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47518
om1_1        | 2021-07-12 13:49:45,918 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
scm1.org_1   | 2021-07-12 13:46:12,796 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$331/0x000000084030cc40@64920dc2] INFO util.JvmPauseMonitor: JvmPauseMonitor-1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Stopped
scm1.org_1   | 2021-07-12 13:46:12,796 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2021-07-12 13:46:12,799 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd; layoutVersion=2; scmId=1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:12,813 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm1.org/172.25.0.116
scm1.org_1   | ************************************************************/
scm1.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1.org_1   | 2021-07-12 13:46:14,618 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1.org_1   | /************************************************************
scm1.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm1.org_1   | STARTUP_MSG:   host = scm1.org/172.25.0.116
scm1.org_1   | STARTUP_MSG:   args = []
scm1.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om3_1        | 2021-07-12 13:48:47,247 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: change Leader from null to om1 at term 2 for appendEntries, leader elected after 13140ms
om3_1        | 2021-07-12 13:48:47,436 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om3_1        | 2021-07-12 13:48:47,457 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om3_1        | 2021-07-12 13:48:47,971 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1        | 2021-07-12 13:49:14,676 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:testuser/scm@EXAMPLE.COM
om3_1        | 2021-07-12 13:50:19,483 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-source for user:testuser/scm@EXAMPLE.COM
om3_1        | 2021-07-12 13:50:24,206 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-target for user:testuser/scm@EXAMPLE.COM
om3_1        | 2021-07-12 13:53:16,428 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:05466-target
om3_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:53:25,384 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:05466-target
om3_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:55:52,585 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-77473 in volume:s3v
om3_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:01,902 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om3_1        | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
om3_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:119)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:41,169 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-16724/10474/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om3_1        | 2021-07-12 13:56:41,186 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 10474/multipartKey2 in Volume/Bucket s3v/bucket-16724
om3_1        | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 10474/multipartKey2. Entity too small.
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:463)
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
scm1.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm1.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm1.org_1   | STARTUP_MSG:   java = 11.0.10
scm1.org_1   | ************************************************************/
scm1.org_1   | 2021-07-12 13:46:14,627 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1.org_1   | 2021-07-12 13:46:14,699 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1.org_1   | 2021-07-12 13:46:14,699 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1.org_1   | 2021-07-12 13:46:14,780 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1.org:9894 and Ratis port: 9894
scm1.org_1   | 2021-07-12 13:46:14,780 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1.org
scm1.org_1   | 2021-07-12 13:46:14,827 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2021-07-12 13:46:14,874 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm1.org_1   | 2021-07-12 13:46:15,192 [main] INFO reflections.Reflections: Reflections took 171 ms to scan 3 urls, producing 103 keys and 210 values 
scm1.org_1   | 2021-07-12 13:46:15,792 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm1.org_1   | 2021-07-12 13:46:15,945 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm1.org_1   | 2021-07-12 13:46:15,949 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm1.org_1   | 2021-07-12 13:46:15,954 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/3115382315205.crt.
scm1.org_1   | 2021-07-12 13:46:16,131 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm1.org_1   | 2021-07-12 13:46:16,133 [main] INFO server.StorageContainerManager: SCM login successful.
scm1.org_1   | 2021-07-12 13:46:16,176 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2021-07-12 13:46:16,340 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1.org_1   | 2021-07-12 13:46:16,539 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1.org_1   | 2021-07-12 13:46:16,539 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1.org_1   | 2021-07-12 13:46:16,667 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:16,789 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1.org_1   | 2021-07-12 13:46:16,902 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm1.org_1   | 2021-07-12 13:46:16,905 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:16,906 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm1.org_1   | 2021-07-12 13:46:16,907 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:16,907 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1.org_1   | 2021-07-12 13:46:16,908 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1.org_1   | 2021-07-12 13:46:16,910 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:16,911 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1.org_1   | 2021-07-12 13:46:16,912 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2021-07-12 13:46:17,727 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1.org_1   | 2021-07-12 13:46:17,728 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2021-07-12 13:46:17,729 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1.org_1   | 2021-07-12 13:46:17,739 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2021-07-12 13:46:17,741 [main] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: found a subdirectory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd
scm1.org_1   | 2021-07-12 13:46:17,746 [main] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: addNew group-EE404DDD1BCD:[] returns group-EE404DDD1BCD:java.util.concurrent.CompletableFuture@712cd5d3[Not completed]
scm1.org_1   | 2021-07-12 13:46:17,767 [pool-13-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: new RaftServerImpl for group-EE404DDD1BCD:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2021-07-12 13:46:17,770 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1.org_1   | 2021-07-12 13:46:17,772 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1.org_1   | 2021-07-12 13:46:17,774 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1.org_1   | 2021-07-12 13:46:17,774 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1.org_1   | 2021-07-12 13:46:17,775 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode2_1  | 2021-07-12 13:50:53,211 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:56,281 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:50:59,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:02,425 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:05,501 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:08,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:11,645 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:14,713 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:17,784 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:20,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:23,928 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:30,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:33,144 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:36,217 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:39,289 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:42,360 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:45,438 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:48,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:51,576 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:54,653 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:51:57,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:00,793 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:03,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:06,941 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:10,008 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:13,081 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:19,229 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:22,297 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:42,487 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8
datanode3_1  | 2021-07-12 13:48:42,490 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,491 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2021-07-12 13:48:42,495 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2021-07-12 13:48:42,497 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode3_1  | 2021-07-12 13:48:42,497 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2021-07-12 13:48:42,498 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2021-07-12 13:48:42,500 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,500 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2021-07-12 13:48:42,500 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2021-07-12 13:48:42,501 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2021-07-12 13:48:42,501 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2021-07-12 13:48:42,501 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2021-07-12 13:48:42,503 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:42,503 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:42,503 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2021-07-12 13:48:42,503 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode2_1  | 2021-07-12 13:52:25,369 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:28,440 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:31,513 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:34,585 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:37,661 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:40,729 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:43,800 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:44,278 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=326,entriesCount=1,lastEntry=(t:2, i:1)
datanode2_1  | 2021-07-12 13:52:44,308 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=327,entriesCount=1,lastEntry=(t:2, i:2)
datanode2_1  | 2021-07-12 13:52:44,410 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=328,entriesCount=1,lastEntry=(t:2, i:3)
datanode2_1  | 2021-07-12 13:52:44,417 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=329,entriesCount=1,lastEntry=(t:2, i:4)
datanode2_1  | 2021-07-12 13:52:46,873 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:49,944 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:53,017 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:56,089 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:52:59,164 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:01,364 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=584,entriesCount=1,lastEntry=(t:2, i:5)
datanode2_1  | 2021-07-12 13:53:01,371 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=585,entriesCount=1,lastEntry=(t:2, i:6)
datanode2_1  | 2021-07-12 13:53:01,385 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=586,entriesCount=1,lastEntry=(t:2, i:7)
datanode2_1  | 2021-07-12 13:53:02,232 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:08,377 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:11,449 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:14,522 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:17,593 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:20,665 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:23,737 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:17,360 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8
datanode1_1  | 2021-07-12 13:48:17,361 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8
datanode1_1  | 2021-07-12 13:48:17,361 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:17,364 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2021-07-12 13:48:17,364 [pool-23-thread-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState
datanode1_1  | 2021-07-12 13:48:17,380 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ABE0FEB7C4D8,id=5b90731c-5249-453a-9f10-ddd353e45997
datanode1_1  | 2021-07-12 13:48:17,381 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8
datanode1_1  | 2021-07-12 13:48:17,383 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode1_1  | 2021-07-12 13:48:17,606 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-CCC63FC21839->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode1_1  | 2021-07-12 13:48:17,790 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:19,849 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode1_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode1_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode1_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode1_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode1_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode1_1  | 	... 18 more
datanode1_1  | 2021-07-12 13:48:19,858 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D381F33DC2E7->e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:20,837 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:21,470 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode1_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode1_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode1_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode1_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2021-07-12 13:48:42,504 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8
datanode3_1  | 2021-07-12 13:48:42,505 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8
datanode3_1  | 2021-07-12 13:48:42,505 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:42,505 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2021-07-12 13:48:42,505 [pool-23-thread-1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-FollowerState
datanode3_1  | 2021-07-12 13:48:42,506 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-ABE0FEB7C4D8,id=8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:42,506 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8
datanode3_1  | 2021-07-12 13:48:42,506 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8
datanode3_1  | 2021-07-12 13:48:42,570 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5C12ED1D1EFD->e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode3_1  | 2021-07-12 13:48:43,039 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:43,106 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode3_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode3_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode3_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode3_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
recon_1      | Sleeping for 5 seconds
recon_1      | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1      | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1      | 2021-07-12 13:45:42,737 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1      | /************************************************************
recon_1      | STARTUP_MSG: Starting ReconServer
recon_1      | STARTUP_MSG:   host = recon/172.25.0.115
recon_1      | STARTUP_MSG:   args = []
recon_1      | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
datanode2_1  | 2021-07-12 13:53:26,809 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:29,880 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:32,953 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:36,024 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:39,096 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:42,169 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:45,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:48,313 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:51,385 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:53:57,530 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:00,601 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:03,672 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:06,745 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:09,817 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:12,891 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:15,961 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:19,037 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:22,105 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:25,177 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:28,249 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:31,320 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:34,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:37,465 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:40,541 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:46,681 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:49,753 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:52,825 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1        | Sleeping for 5 seconds
om2_1        | Waiting for the service scm3.org:9894
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2021-07-12 13:47:17,737 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = [--init]
om2_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om1_1        | 2021-07-12 13:49:52,130 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47560
om1_1        | 2021-07-12 13:49:52,145 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:52,764 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47572
om1_1        | 2021-07-12 13:49:52,786 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:49:58,054 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47586
om1_1        | 2021-07-12 13:49:58,086 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:03,110 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47610
om1_1        | 2021-07-12 13:50:03,137 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:18,632 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47678
om1_1        | 2021-07-12 13:50:18,653 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:19,471 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-source for user:testuser/scm@EXAMPLE.COM
om1_1        | 2021-07-12 13:50:23,529 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47710
om1_1        | 2021-07-12 13:50:23,559 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:24,194 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-target for user:testuser/scm@EXAMPLE.COM
om1_1        | 2021-07-12 13:50:28,184 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47724
om1_1        | 2021-07-12 13:50:28,199 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:32,905 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47756
om1_1        | 2021-07-12 13:50:32,964 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:40,500 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47786
om1_1        | 2021-07-12 13:50:40,541 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:45,235 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47806
om1_1        | 2021-07-12 13:50:45,262 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:50,008 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47846
om1_1        | 2021-07-12 13:50:50,069 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:54,615 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47870
om1_1        | 2021-07-12 13:50:54,633 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:50:59,281 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47890
om1_1        | 2021-07-12 13:50:59,295 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:03,774 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47902
om1_1        | 2021-07-12 13:51:03,830 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:08,162 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47924
om1_1        | 2021-07-12 13:51:08,215 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:12,840 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47944
om1_1        | 2021-07-12 13:51:12,893 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:17,196 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47958
om1_1        | 2021-07-12 13:51:17,222 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:22,142 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:47998
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode3_1  | 	... 18 more
datanode3_1  | 2021-07-12 13:48:43,114 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5AA7DD503532->5b90731c-5249-453a-9f10-ddd353e45997
datanode3_1  | 2021-07-12 13:48:43,378 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode3_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode3_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode3_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode3_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode3_1  | 	... 18 more
datanode3_1  | 2021-07-12 13:48:43,390 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8.
datanode3_1  | 2021-07-12 13:48:43,397 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: new RaftServerImpl for group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode3_1  | 2021-07-12 13:48:43,397 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode3_1  | 2021-07-12 13:48:43,397 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode3_1  | 2021-07-12 13:48:43,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode3_1  | 2021-07-12 13:48:43,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:43,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode3_1  | 2021-07-12 13:48:43,398 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode3_1  | 2021-07-12 13:48:43,401 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2021-07-12 13:48:43,404 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode3_1  | 2021-07-12 13:48:43,404 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode3_1  | 2021-07-12 13:48:43,404 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1        | 2021-07-12 13:51:22,172 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:26,759 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48020
om1_1        | 2021-07-12 13:51:26,777 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:31,795 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48052
om1_1        | 2021-07-12 13:51:31,813 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:36,260 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48072
om1_1        | 2021-07-12 13:51:36,289 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:41,028 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48086
om1_1        | 2021-07-12 13:51:41,069 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:51,290 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48148
om1_1        | 2021-07-12 13:51:51,310 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:51:58,395 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48180
om1_1        | 2021-07-12 13:51:58,414 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:08,019 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48222
om1_1        | 2021-07-12 13:52:08,038 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:15,437 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48274
om1_1        | 2021-07-12 13:52:15,453 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:20,180 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48292
om1_1        | 2021-07-12 13:52:20,235 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:24,750 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48314
om1_1        | 2021-07-12 13:52:24,770 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:29,360 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48326
om1_1        | 2021-07-12 13:52:29,378 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:34,060 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48358
om1_1        | 2021-07-12 13:52:34,077 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:38,753 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48380
om1_1        | 2021-07-12 13:52:38,784 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:43,533 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48402
om1_1        | 2021-07-12 13:52:43,581 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:48,224 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48440
om1_1        | 2021-07-12 13:52:48,278 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:52,960 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48454
om1_1        | 2021-07-12 13:52:53,018 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:52:57,901 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48478
om1_1        | 2021-07-12 13:52:57,917 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:01,948 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48498
om1_1        | 2021-07-12 13:53:01,962 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:06,485 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48520
datanode2_1  | 2021-07-12 13:54:55,897 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:54:58,972 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:02,040 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:05,112 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:08,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:11,257 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:12,896 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=885,entriesCount=1,lastEntry=(t:2, i:8)
datanode2_1  | 2021-07-12 13:55:12,924 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=886,entriesCount=1,lastEntry=(t:2, i:9)
datanode2_1  | 2021-07-12 13:55:12,940 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=887,entriesCount=1,lastEntry=(t:2, i:10)
datanode2_1  | 2021-07-12 13:55:12,946 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=888,entriesCount=1,lastEntry=(t:2, i:11)
datanode2_1  | 2021-07-12 13:55:14,328 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:17,401 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:20,477 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:23,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:26,618 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:29,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:35,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:38,904 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:41,976 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:45,060 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:48,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:51,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:54,269 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:55:57,337 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:00,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:03,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:06,558 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om2_1        | STARTUP_MSG:   java = 11.0.10
om2_1        | ************************************************************/
om2_1        | 2021-07-12 13:47:17,829 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2021-07-12 13:47:28,043 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2021-07-12 13:47:28,766 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2021-07-12 13:47:28,768 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2021-07-12 13:47:28,770 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2021-07-12 13:47:30,051 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2021-07-12 13:47:30,072 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2021-07-12 13:47:30,133 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2021-07-12 13:47:35,046 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om2_1        | 2021-07-12 13:47:38,797 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om2_1        | 2021-07-12 13:47:38,804 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om2_1        | 2021-07-12 13:47:38,806 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om2_1        | 2021-07-12 13:47:43,108 [main] INFO om.OzoneManager: Init response: GETCERT
om2_1        | 2021-07-12 13:47:43,327 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.112,host:om2
om2_1        | 2021-07-12 13:47:43,352 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om2_1        | 2021-07-12 13:47:43,355 [main] ERROR client.OMCertificateClient: Invalid domain om2
om2_1        | 2021-07-12 13:47:43,381 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2021-07-12 13:47:43,381 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2021-07-12 13:47:43,382 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2021-07-12 13:47:43,382 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2021-07-12 13:47:43,386 [main] INFO om.OzoneManager: Creating csr for OM->dns:om2,ip:172.25.0.112,scmId:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:om2
om2_1        | 2021-07-12 13:47:44,219 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om2_1        | value: 9862
om2_1        | ]
om2_1        | 2021-07-12 13:47:46,178 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om2_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd;layoutVersion=0
om2_1        | 2021-07-12 13:47:46,370 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1        | /************************************************************
om2_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om2/172.25.0.112
om2_1        | ************************************************************/
om2_1        | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1        | 2021-07-12 13:47:57,130 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1        | /************************************************************
om2_1        | STARTUP_MSG: Starting OzoneManager
om2_1        | STARTUP_MSG:   host = om2/172.25.0.112
om2_1        | STARTUP_MSG:   args = []
om2_1        | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om1_1        | 2021-07-12 13:53:06,505 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:11,139 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48532
om1_1        | 2021-07-12 13:53:11,159 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:15,692 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48570
om1_1        | 2021-07-12 13:53:15,713 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:16,414 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:05466-target
om1_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:53:20,127 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48592
om1_1        | 2021-07-12 13:53:20,147 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:24,695 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48612
om1_1        | 2021-07-12 13:53:24,711 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:25,381 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:05466-target
om1_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:53:29,174 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48626
om1_1        | 2021-07-12 13:53:29,194 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:33,769 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48656
om1_1        | 2021-07-12 13:53:33,782 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:34,344 [IPC Server handler 82 on default port 9862] WARN om.OzoneManager: User testuser2/scm@EXAMPLE.COM doesn't have READ permission to access bucket /05466-target/unreadable-link/null
om1_1        | 2021-07-12 13:53:38,146 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48678
om1_1        | 2021-07-12 13:53:38,162 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:42,876 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48698
om1_1        | 2021-07-12 13:53:42,912 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:43,546 [IPC Server handler 2 on default port 9862] WARN om.OzoneManager: User testuser2/scm@EXAMPLE.COM doesn't have LIST permission to access bucket /05466-source/unreadable-bucket/
om1_1        | 2021-07-12 13:53:47,777 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48730
om1_1        | 2021-07-12 13:53:47,811 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:52,218 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48750
om1_1        | 2021-07-12 13:53:52,238 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:53:56,633 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48774
om1_1        | 2021-07-12 13:53:56,650 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:01,072 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48794
scm1.org_1   | 2021-07-12 13:46:17,775 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1.org_1   | 2021-07-12 13:46:17,776 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2021-07-12 13:46:17,780 [pool-13-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm1.org_1   | 2021-07-12 13:46:17,781 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1.org_1   | 2021-07-12 13:46:17,783 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1.org_1   | 2021-07-12 13:46:17,801 [pool-13-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/in_use.lock acquired by nodename 7@scm1.org
scm1.org_1   | 2021-07-12 13:46:17,807 [pool-13-thread-1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=1e6f42a1-e2d2-4f78-bf5d-162da394b7e9} from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/raft-meta
scm1.org_1   | 2021-07-12 13:46:17,839 [pool-13-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:17,840 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1.org_1   | 2021-07-12 13:46:17,842 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1.org_1   | 2021-07-12 13:46:17,859 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1.org_1   | 2021-07-12 13:46:17,859 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:17,868 [pool-13-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1.org_1   | 2021-07-12 13:46:17,869 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:17,870 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:17,872 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:17,879 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1.org_1   | 2021-07-12 13:46:17,879 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1.org_1   | 2021-07-12 13:46:17,885 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: new 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd
scm1.org_1   | 2021-07-12 13:46:17,885 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2021-07-12 13:46:17,886 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1.org_1   | 2021-07-12 13:46:17,887 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:17,888 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1.org_1   | 2021-07-12 13:46:17,889 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1.org_1   | 2021-07-12 13:46:17,890 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1.org_1   | 2021-07-12 13:46:17,891 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1.org_1   | 2021-07-12 13:46:17,892 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1.org_1   | 2021-07-12 13:46:17,901 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1.org_1   | 2021-07-12 13:46:17,901 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1.org_1   | 2021-07-12 13:46:17,925 [pool-13-thread-1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:17,926 [pool-13-thread-1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0
scm1.org_1   | 2021-07-12 13:46:17,935 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1.org_1   | 2021-07-12 13:46:17,935 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1.org_1   | 2021-07-12 13:46:18,006 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1.org_1   | 2021-07-12 13:46:18,007 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1.org_1   | 2021-07-12 13:46:18,007 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1.org_1   | 2021-07-12 13:46:18,008 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1.org_1   | 2021-07-12 13:46:18,009 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1.org_1   | 2021-07-12 13:46:18,009 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1.org_1   | 2021-07-12 13:46:18,028 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:18,029 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:18,034 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:18,034 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:18,044 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1.org_1   | 2021-07-12 13:46:18,045 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1.org_1   | 2021-07-12 13:46:18,046 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1.org_1   | 2021-07-12 13:46:18,309 [main] INFO ha.SequenceIdGenerator: upgrade localId to 107544261427200000
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode1_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode1_1  | 	... 18 more
datanode1_1  | 2021-07-12 13:48:21,472 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8.
datanode1_1  | 2021-07-12 13:48:21,476 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997: new RaftServerImpl for group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] with ContainerStateMachine:uninitialized
datanode1_1  | 2021-07-12 13:48:21,476 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode1_1  | 2021-07-12 13:48:21,477 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode1_1  | 2021-07-12 13:48:21,477 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode1_1  | 2021-07-12 13:48:21,477 [Command processor thread] INFO server.RaftServer: 5b90731c-5249-453a-9f10-ddd353e45997: addNew group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-562B256BCC94:java.util.concurrent.CompletableFuture@7f5cc5e2[Not completed]
datanode1_1  | 2021-07-12 13:48:21,482 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:21,483 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode1_1  | 2021-07-12 13:48:21,483 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode1_1  | 2021-07-12 13:48:21,483 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode1_1  | 2021-07-12 13:48:21,483 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: ConfigurationManager, init=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode1_1  | 2021-07-12 13:48:21,483 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode1_1  | 2021-07-12 13:48:21,484 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode1_1  | 2021-07-12 13:48:21,484 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 does not exist. Creating ...
datanode1_1  | 2021-07-12 13:48:21,494 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/in_use.lock acquired by nodename 8@27bc097164fa
datanode1_1  | 2021-07-12 13:48:21,604 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 has been successfully formatted.
datanode1_1  | 2021-07-12 13:48:21,612 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-562B256BCC94: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode1_1  | 2021-07-12 13:48:21,612 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode1_1  | 2021-07-12 13:48:21,624 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode1_1  | 2021-07-12 13:48:21,633 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode1_1  | 2021-07-12 13:48:21,636 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode1_1  | 2021-07-12 13:48:21,643 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94
datanode1_1  | 2021-07-12 13:48:21,645 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:21,648 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode1_1  | 2021-07-12 13:48:21,659 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode1_1  | 2021-07-12 13:48:21,659 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94
datanode1_1  | 2021-07-12 13:48:21,659 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode1_1  | 2021-07-12 13:48:21,676 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode1_1  | 2021-07-12 13:48:21,680 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode2_1  | 2021-07-12 13:56:09,629 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:12,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:15,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:16,466 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1158,entriesCount=1,lastEntry=(t:2, i:12)
datanode2_1  | 2021-07-12 13:56:16,479 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1159,entriesCount=1,lastEntry=(t:2, i:13)
datanode2_1  | 2021-07-12 13:56:16,488 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1160,entriesCount=1,lastEntry=(t:2, i:14)
datanode2_1  | 2021-07-12 13:56:16,489 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1161,entriesCount=1,lastEntry=(t:2, i:15)
datanode2_1  | 2021-07-12 13:56:18,845 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:24,985 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:28,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:28,119 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1414,entriesCount=1,lastEntry=(t:2, i:16)
datanode2_1  | 2021-07-12 13:56:28,119 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1415,entriesCount=1,lastEntry=(t:2, i:17)
datanode2_1  | 2021-07-12 13:56:28,119 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1416,entriesCount=1,lastEntry=(t:2, i:18)
datanode2_1  | 2021-07-12 13:56:28,146 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1419,entriesCount=1,lastEntry=(t:2, i:19)
datanode2_1  | 2021-07-12 13:56:31,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:32,200 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1669,entriesCount=1,lastEntry=(t:2, i:20)
datanode2_1  | 2021-07-12 13:56:32,212 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1670,entriesCount=1,lastEntry=(t:2, i:21)
datanode2_1  | 2021-07-12 13:56:32,450 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1671,entriesCount=1,lastEntry=(t:2, i:22)
datanode2_1  | 2021-07-12 13:56:32,486 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1672,entriesCount=1,lastEntry=(t:2, i:23)
datanode2_1  | 2021-07-12 13:56:32,486 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1673,entriesCount=1,lastEntry=(t:2, i:24)
datanode2_1  | 2021-07-12 13:56:32,597 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1682,entriesCount=1,lastEntry=(t:2, i:25)
datanode2_1  | 2021-07-12 13:56:32,597 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1683,entriesCount=1,lastEntry=(t:2, i:26)
datanode2_1  | 2021-07-12 13:56:32,822 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1705,entriesCount=1,lastEntry=(t:2, i:27)
datanode2_1  | 2021-07-12 13:56:32,898 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1714,entriesCount=1,lastEntry=(t:2, i:28)
om1_1        | 2021-07-12 13:54:01,117 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:05,217 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48806
om1_1        | 2021-07-12 13:54:05,237 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:09,824 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48826
om1_1        | 2021-07-12 13:54:09,847 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:19,987 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48888
om1_1        | 2021-07-12 13:54:20,006 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:27,210 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48918
om1_1        | 2021-07-12 13:54:27,227 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:32,028 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48952
om1_1        | 2021-07-12 13:54:32,047 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:54:36,853 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:48976
om1_1        | 2021-07-12 13:54:36,876 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:00,903 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49212
om1_1        | 2021-07-12 13:55:00,936 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:05,200 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:05,205 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:35974
om1_1        | 2021-07-12 13:55:05,223 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:09,732 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49254
om1_1        | 2021-07-12 13:55:09,757 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:12,594 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:12,597 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36004
om1_1        | 2021-07-12 13:55:12,613 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:13,224 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:13,225 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36018
om1_1        | 2021-07-12 13:55:13,238 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:16,312 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:16,313 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36050
om1_1        | 2021-07-12 13:55:16,321 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:19,569 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:19,570 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36070
om1_1        | 2021-07-12 13:55:19,576 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:20,140 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:20,141 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36074
om1_1        | 2021-07-12 13:55:20,153 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:20,706 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:20,709 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36080
om1_1        | 2021-07-12 13:55:20,715 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:24,524 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49350
recon_1      | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.0-SNAPSHOT.jar
recon_1      | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
recon_1      | STARTUP_MSG:   java = 11.0.10
recon_1      | ************************************************************/
recon_1      | 2021-07-12 13:45:42,774 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1      | 2021-07-12 13:45:46,327 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1      | 2021-07-12 13:45:48,106 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1      | 2021-07-12 13:45:48,623 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1      | 2021-07-12 13:45:49,080 [main] ERROR recon.ReconServer: Error login in as Recon service. 
recon_1      | org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: recon/recon@EXAMPLE.COM from keytab /etc/security/keytabs/recon.keytab javax.security.auth.login.LoginException: Unable to obtain password from user
recon_1      | 
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1361)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1122)
recon_1      | 	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:315)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.loginReconUser(ReconServer.java:216)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.loginReconUserIfSecurityEnabled(ReconServer.java:191)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:99)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:55)
recon_1      | 	at picocli.CommandLine.executeUserObject(CommandLine.java:1933)
recon_1      | 	at picocli.CommandLine.access$1100(CommandLine.java:145)
recon_1      | 	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2326)
recon_1      | 	at picocli.CommandLine$RunLast.handle(CommandLine.java:2291)
recon_1      | 	at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)
recon_1      | 	at picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)
recon_1      | 	at picocli.CommandLine.parseWithHandler(CommandLine.java:2465)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)
recon_1      | 	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)
recon_1      | 	at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:71)
recon_1      | Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user
recon_1      | 
recon_1      | 	at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:875)
recon_1      | 	at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:738)
recon_1      | 	at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:592)
recon_1      | 	at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:726)
recon_1      | 	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
recon_1      | 	at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
recon_1      | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1      | 	at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
recon_1      | 	at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:574)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
recon_1      | 	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
recon_1      | 	... 18 more
recon_1      | 2021-07-12 13:45:49,927 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2021-07-12 13:45:53,962 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | WARNING: An illegal reflective access operation has occurred
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.createS3Bucket(EndpointBase.java:94)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:239)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
recon_1      | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1      | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1      | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1      | WARNING: All illegal access operations will be denied in a future release
recon_1      | 2021-07-12 13:45:55,166 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1      | 2021-07-12 13:45:55,190 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1      | 2021-07-12 13:45:55,199 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1      | 2021-07-12 13:45:57,899 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1      | 2021-07-12 13:45:57,899 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
recon_1      | 2021-07-12 13:45:57,900 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.recon.http.auth.type = kerberos
recon_1      | 2021-07-12 13:45:57,919 [main] INFO util.log: Logging initialized @17447ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1      | 2021-07-12 13:45:58,235 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1      | 2021-07-12 13:45:58,272 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1      | 2021-07-12 13:45:58,280 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context recon
recon_1      | 2021-07-12 13:45:58,280 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
recon_1      | 2021-07-12 13:45:58,281 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
recon_1      | 2021-07-12 13:45:58,284 [main] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.recon.http.auth.kerberos.principal keytabKey: ozone.recon.http.auth.kerberos.keytab
recon_1      | 2021-07-12 13:45:58,706 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1      | 2021-07-12 13:45:59,329 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1      | 2021-07-12 13:45:59,358 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1      | 2021-07-12 13:45:59,462 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'id1'.
recon_1      | 2021-07-12 13:46:01,791 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2021-07-12 13:46:02,370 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2021-07-12 13:46:02,450 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1      | 2021-07-12 13:46:02,451 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1      | 2021-07-12 13:46:02,644 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1      | 2021-07-12 13:46:02,918 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1      | 2021-07-12 13:46:03,296 [main] INFO reflections.Reflections: Reflections took 322 ms to scan 3 urls, producing 103 keys and 210 values 
recon_1      | 2021-07-12 13:46:03,424 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1      | 2021-07-12 13:46:03,493 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1      | 2021-07-12 13:46:03,516 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1      | 2021-07-12 13:46:03,525 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1      | 2021-07-12 13:46:03,624 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1      | 2021-07-12 13:46:03,735 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1      | 2021-07-12 13:46:03,904 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManager: No pipeline exists in current db
recon_1      | 2021-07-12 13:46:04,135 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1      | 2021-07-12 13:46:04,135 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1      | 2021-07-12 13:46:04,325 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1      | 2021-07-12 13:46:04,356 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1      | 2021-07-12 13:46:04,356 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1      | 2021-07-12 13:46:04,904 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1      | 2021-07-12 13:46:04,906 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1      | 2021-07-12 13:46:05,063 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1      | 2021-07-12 13:46:05,063 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1      | 2021-07-12 13:46:05,064 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1      | 2021-07-12 13:46:05,136 [Listener at 0.0.0.0/9891] INFO server.session: node0 Stopped scavenging
recon_1      | Problem starting http server
recon_1      | 2021-07-12 13:46:05,222 [shutdown-hook-0] INFO recon.ReconServer: SHUTDOWN_MSG: 
recon_1      | /************************************************************
recon_1      | SHUTDOWN_MSG: Shutting down ReconServer at recon/172.25.0.115
recon_1      | ************************************************************/
datanode3_1  | 2021-07-12 13:48:43,405 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 does not exist. Creating ...
datanode3_1  | 2021-07-12 13:48:43,414 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/in_use.lock acquired by nodename 8@abb35404295f
datanode3_1  | 2021-07-12 13:48:43,419 [Command processor thread] INFO server.RaftServer: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: addNew group-562B256BCC94:[8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0] returns group-562B256BCC94:java.util.concurrent.CompletableFuture@18826850[Not completed]
datanode3_1  | 2021-07-12 13:48:43,428 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94 has been successfully formatted.
datanode3_1  | 2021-07-12 13:48:43,428 [pool-23-thread-1] INFO ratis.ContainerStateMachine: group-562B256BCC94: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode3_1  | 2021-07-12 13:48:43,477 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode3_1  | 2021-07-12 13:48:43,481 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode3_1  | 2021-07-12 13:48:43,482 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode3_1  | 2021-07-12 13:48:43,496 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2021-07-12 13:48:43,500 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:43,501 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:43,505 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode3_1  | 2021-07-12 13:48:43,515 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode3_1  | 2021-07-12 13:48:43,516 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94
datanode3_1  | 2021-07-12 13:48:43,516 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode3_1  | 2021-07-12 13:48:43,517 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode3_1  | 2021-07-12 13:48:43,518 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:43,524 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode3_1  | 2021-07-12 13:48:43,525 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode3_1  | 2021-07-12 13:48:43,526 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode3_1  | 2021-07-12 13:48:43,528 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode3_1  | 2021-07-12 13:48:43,529 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode3_1  | 2021-07-12 13:48:43,530 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode3_1  | 2021-07-12 13:48:43,539 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:43,540 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode3_1  | 2021-07-12 13:48:43,540 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode3_1  | 2021-07-12 13:48:43,549 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode3_1  | 2021-07-12 13:48:43,549 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode3_1  | 2021-07-12 13:48:43,550 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode3_1  | 2021-07-12 13:48:43,550 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode3_1  | 2021-07-12 13:48:43,550 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode3_1  | 2021-07-12 13:48:43,550 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode3_1  | 2021-07-12 13:48:43,551 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:43,551 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:43,552 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:43,552 [pool-23-thread-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode3_1  | 2021-07-12 13:48:43,553 [pool-23-thread-1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-FollowerState
datanode3_1  | 2021-07-12 13:48:43,559 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562B256BCC94,id=8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:43,559 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:43,561 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=660497cf-d26f-4475-b297-562b256bcc94
datanode3_1  | 2021-07-12 13:48:43,572 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D6710C4D0313->5b90731c-5249-453a-9f10-ddd353e45997
om3_1        | 2021-07-12 13:56:42,400 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om3_1        | partName: "etag1"
om3_1        | , partNumber: 2
om3_1        | partName: "etag2"
om3_1        | ]
om3_1        | 2021-07-12 13:56:42,410 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om3_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:42,990 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om3_1        | partName: "etag1"
om3_1        | , partNumber: 1
om3_1        | partName: "etag2"
om3_1        | ]
om3_1        | 2021-07-12 13:56:42,993 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om3_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:49,224 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om3_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567971869294625
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:49,780 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om3_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567972107190306
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:50,341 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-16724/19004/multipartKey3
om3_1        | 2021-07-12 13:56:50,344 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om3_1        | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3 because parts are in Invalid order.
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:411)
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:180)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.8.9.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.8.9.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar
om2_1        | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
om2_1        | STARTUP_MSG:   java = 11.0.10
om2_1        | ************************************************************/
om2_1        | 2021-07-12 13:47:57,233 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1        | 2021-07-12 13:48:06,843 [main] INFO ha.OMHANodeDetails: ServiceID for OzoneManager is id1
om2_1        | 2021-07-12 13:48:07,381 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1        | 2021-07-12 13:48:07,385 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.http-address with value of key ozone.om.http-address.id1.om2: om2
om2_1        | 2021-07-12 13:48:07,386 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.id1.om2: om2
om2_1        | 2021-07-12 13:48:07,454 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2021-07-12 13:48:07,748 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om2_1        | 2021-07-12 13:48:09,262 [main] INFO reflections.Reflections: Reflections took 1288 ms to scan 1 urls, producing 94 keys and 256 values [using 2 cores]
om2_1        | 2021-07-12 13:48:10,974 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
om2_1        | 2021-07-12 13:48:10,996 [main] INFO om.OzoneManager: Ozone Manager login successful.
om2_1        | 2021-07-12 13:48:10,997 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2021-07-12 13:48:19,204 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om2_1        | 2021-07-12 13:48:20,024 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/ROOTCA-1.crt.
om2_1        | 2021-07-12 13:48:20,049 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-3115382315205.crt.
om2_1        | 2021-07-12 13:48:20,080 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/3216702657551.crt.
om2_1        | 2021-07-12 13:48:20,292 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1        | 2021-07-12 13:48:21,057 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1        | 2021-07-12 13:48:21,068 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1        | 2021-07-12 13:48:22,299 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om2_1        | 2021-07-12 13:48:22,300 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om2_1        | 2021-07-12 13:48:23,265 [main] INFO om.OzoneManager: Created Volume s3v With Owner root required for S3Gateway operations.
datanode1_1  | 2021-07-12 13:48:21,680 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode1_1  | 2021-07-12 13:48:21,688 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2021-07-12 13:48:21,688 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode1_1  | 2021-07-12 13:48:21,689 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode1_1  | 2021-07-12 13:48:21,689 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode1_1  | 2021-07-12 13:48:21,690 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode1_1  | 2021-07-12 13:48:21,692 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:21,693 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:21,693 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode1_1  | 2021-07-12 13:48:21,752 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode1_1  | 2021-07-12 13:48:21,752 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode1_1  | 2021-07-12 13:48:21,752 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode1_1  | 2021-07-12 13:48:21,752 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode1_1  | 2021-07-12 13:48:21,752 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode1_1  | 2021-07-12 13:48:21,755 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode1_1  | 2021-07-12 13:48:21,755 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94
datanode1_1  | 2021-07-12 13:48:21,756 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94
datanode1_1  | 2021-07-12 13:48:21,757 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: start as a follower, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:21,760 [pool-23-thread-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode1_1  | 2021-07-12 13:48:21,760 [pool-23-thread-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState
datanode1_1  | 2021-07-12 13:48:21,761 [pool-23-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562B256BCC94,id=5b90731c-5249-453a-9f10-ddd353e45997
datanode1_1  | 2021-07-12 13:48:21,761 [pool-23-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94
datanode1_1  | 2021-07-12 13:48:21,781 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=660497cf-d26f-4475-b297-562b256bcc94
datanode1_1  | 2021-07-12 13:48:21,805 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1B690B6EBDEB->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode1_1  | 2021-07-12 13:48:22,156 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState] INFO impl.FollowerState: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5176297585ns, electionTimeout:5113ms
datanode1_1  | 2021-07-12 13:48:22,163 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState
datanode1_1  | 2021-07-12 13:48:22,192 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2021-07-12 13:48:22,196 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2021-07-12 13:48:22,196 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-FollowerState] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1
datanode1_1  | 2021-07-12 13:48:22,247 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:1], old=null
datanode1_1  | 2021-07-12 13:48:22,251 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode1_1  | 2021-07-12 13:48:22,264 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1
datanode1_1  | 2021-07-12 13:48:22,267 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode1_1  | 2021-07-12 13:48:22,269 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7844FDF8175C with new leaderId: 5b90731c-5249-453a-9f10-ddd353e45997
datanode1_1  | 2021-07-12 13:48:22,272 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: change Leader from null to 5b90731c-5249-453a-9f10-ddd353e45997 at term 1 for becomeLeader, leader elected after 6217ms
datanode1_1  | 2021-07-12 13:48:22,295 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode1_1  | 2021-07-12 13:48:22,330 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C
datanode1_1  | 2021-07-12 13:48:22,363 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1.org_1   | 2021-07-12 13:46:18,309 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1.org_1   | 2021-07-12 13:46:18,313 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1.org_1   | 2021-07-12 13:46:18,315 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1.org_1   | 2021-07-12 13:46:18,390 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1.org_1   | 2021-07-12 13:46:18,402 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1.org_1   | 2021-07-12 13:46:18,414 [main] INFO pipeline.PipelineStateManager: No pipeline exists in current db
scm1.org_1   | 2021-07-12 13:46:18,457 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1.org_1   | 2021-07-12 13:46:18,469 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1.org_1   | 2021-07-12 13:46:18,470 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1.org_1   | 2021-07-12 13:46:18,515 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1.org_1   | 2021-07-12 13:46:18,536 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1.org_1   | 2021-07-12 13:46:18,547 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1.org_1   | 2021-07-12 13:46:18,551 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2021-07-12 13:46:18,565 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 0 containers.
scm1.org_1   | 2021-07-12 13:46:18,567 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1.org_1   | 2021-07-12 13:46:18,571 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:46:18,573 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2021-07-12 13:46:18,609 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2021-07-12 13:46:18,619 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm1.org_1   | 2021-07-12 13:46:18,620 [main] INFO server.StorageContainerManager: Storing sub-ca certificate serialId 3115382315205 on primary SCM
scm1.org_1   | 2021-07-12 13:46:18,630 [main] INFO server.StorageContainerManager: Storing root certificate serialId 1
scm1.org_1   | 2021-07-12 13:46:18,669 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2021-07-12 13:46:18,712 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm1.org_1   | 2021-07-12 13:46:19,533 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2021-07-12 13:46:19,535 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1.org_1   | 2021-07-12 13:46:19,594 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2021-07-12 13:46:19,595 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1.org_1   | 2021-07-12 13:46:19,615 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1.org_1   | 2021-07-12 13:46:19,616 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1.org_1   | 2021-07-12 13:46:19,780 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1.org_1   | Container Balancer status:
scm1.org_1   | Key                            Value
scm1.org_1   | Running                        true
scm1.org_1   | Container Balancer Configuration values:
scm1.org_1   | Key                            Value
scm1.org_1   | Threshold                      0.1
scm1.org_1   | Max Datanodes to Balance       5
scm1.org_1   | Max Size to Move               10737418240B
scm1.org_1   | 
scm1.org_1   | 2021-07-12 13:46:19,781 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2021-07-12 13:46:19,781 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1.org_1   | 2021-07-12 13:46:19,799 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1.org_1   | 2021-07-12 13:46:19,800 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1.org_1   | 2021-07-12 13:46:19,800 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: start as a follower, conf=0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:19,802 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1.org_1   | 2021-07-12 13:46:19,803 [Listener at 0.0.0.0/9860] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState
scm1.org_1   | 2021-07-12 13:46:19,813 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EE404DDD1BCD,id=1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:19,814 [Listener at 0.0.0.0/9860] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:19,815 [Listener at 0.0.0.0/9860] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:19,825 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start RPC server
scm1.org_1   | 2021-07-12 13:46:19,865 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: GrpcService started, listening on 9894
scm1.org_1   | 2021-07-12 13:46:19,868 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2021-07-12 13:46:19,868 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1.org_1   | 2021-07-12 13:46:19,874 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm1.org_1   | 2021-07-12 13:46:19,874 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:53,693 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName 50306/multipartKey5 in VolumeName/Bucket s3v/bucket-16724
om3_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-16724key: 50306/multipartKey5
om3_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:148)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:56:54,254 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-16724, Key75644/multipartKey. Exception:{}
om3_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om3_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:708)
om3_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:600)
om3_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:577)
om3_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:278)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1        | 2021-07-12 13:58:35,585 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-97294, Key:27526/multidelete/f4.
om3_1        | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om3_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:133)
om3_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om3_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om3_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:55:53,483 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidBucketName</Code>
s3g_1        |   <Message>The specified bucket is not valid.</Message>
s3g_1        |   <Resource>bucket_1</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:88)
scm2.org_1   | Sleeping for 5 seconds
scm2.org_1   | Waiting for the service scm1.org:9894
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2021-07-12 13:46:08,817 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm2.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
om1_1        | 2021-07-12 13:55:24,542 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:27,242 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:27,243 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36108
om1_1        | 2021-07-12 13:55:27,254 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:27,892 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:27,892 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36116
om1_1        | 2021-07-12 13:55:27,897 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:27,962 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:27,962 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36118
om1_1        | 2021-07-12 13:55:27,972 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:30,710 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:30,710 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36138
om1_1        | 2021-07-12 13:55:30,715 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:30,931 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:30,931 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36150
om1_1        | 2021-07-12 13:55:30,938 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:31,202 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:31,203 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36156
om1_1        | 2021-07-12 13:55:31,221 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:31,445 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:31,446 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36158
om1_1        | 2021-07-12 13:55:31,465 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:31,541 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:31,542 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36162
om1_1        | 2021-07-12 13:55:31,564 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:35,575 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:35,578 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36178
om1_1        | 2021-07-12 13:55:35,596 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:35,819 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:35,820 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36180
om1_1        | 2021-07-12 13:55:35,840 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:36,005 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:36,005 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36184
om1_1        | 2021-07-12 13:55:36,035 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:36,124 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:36,125 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36186
om1_1        | 2021-07-12 13:55:36,159 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,402 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:37,402 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36202
om1_1        | 2021-07-12 13:55:37,409 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,496 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
om2_1        | 2021-07-12 13:48:23,341 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2021-07-12 13:48:23,344 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1        | 2021-07-12 13:48:23,374 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1        | 2021-07-12 13:48:24,441 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1        | 2021-07-12 13:48:24,768 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1        | 2021-07-12 13:48:24,985 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om2:9872, om1:9872, om3:9872
om2_1        | 2021-07-12 13:48:25,102 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1        | 2021-07-12 13:48:26,474 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1        | 2021-07-12 13:48:26,916 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om2_1        | 2021-07-12 13:48:26,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2021-07-12 13:48:26,920 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om2_1        | 2021-07-12 13:48:26,920 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2021-07-12 13:48:26,920 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1        | 2021-07-12 13:48:26,923 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1        | 2021-07-12 13:48:26,942 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2021-07-12 13:48:26,943 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1        | 2021-07-12 13:48:26,961 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1        | 2021-07-12 13:48:31,107 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1        | 2021-07-12 13:48:31,131 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2021-07-12 13:48:31,135 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2021-07-12 13:48:31,245 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2021-07-12 13:48:31,313 [main] INFO server.RaftServer: om2: addNew group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] returns group-562213E44849:java.util.concurrent.CompletableFuture@54d62c35[Not completed]
om2_1        | 2021-07-12 13:48:31,314 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1        | 2021-07-12 13:48:31,457 [pool-24-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-562213E44849:[om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om2_1        | 2021-07-12 13:48:31,495 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1        | 2021-07-12 13:48:31,516 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1        | 2021-07-12 13:48:31,516 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1        | 2021-07-12 13:48:31,516 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1        | 2021-07-12 13:48:31,517 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1        | 2021-07-12 13:48:31,520 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1        | 2021-07-12 13:48:31,523 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1        | 2021-07-12 13:48:31,621 [pool-24-thread-1] INFO server.RaftServer$Division: om2@group-562213E44849: ConfigurationManager, init=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null, confs=<EMPTY_MAP>
om2_1        | 2021-07-12 13:48:31,624 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1        | 2021-07-12 13:48:31,703 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1        | 2021-07-12 13:48:31,726 [pool-24-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om2_1        | 2021-07-12 13:48:31,796 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1        | 2021-07-12 13:48:31,858 [pool-24-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 7@om2
om2_1        | 2021-07-12 13:48:31,880 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1        | 2021-07-12 13:48:32,071 [pool-24-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om2_1        | 2021-07-12 13:48:32,077 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1        | 2021-07-12 13:48:32,096 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1        | 2021-07-12 13:48:32,133 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1        | 2021-07-12 13:48:32,134 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1        | 2021-07-12 13:48:32,149 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om2@group-562213E44849
om2_1        | 2021-07-12 13:48:32,226 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2021-07-12 13:48:32,475 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1        | 2021-07-12 13:48:32,476 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1        | 2021-07-12 13:48:32,490 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: new om2@group-562213E44849-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om2_1        | 2021-07-12 13:48:32,494 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1        | 2021-07-12 13:48:32,497 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1        | 2021-07-12 13:48:32,519 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1        | 2021-07-12 13:48:32,528 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1        | 2021-07-12 13:48:32,528 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode1_1  | 2021-07-12 13:48:22,389 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode1_1  | 2021-07-12 13:48:22,434 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode1_1  | 2021-07-12 13:48:22,437 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode1_1  | 2021-07-12 13:48:22,441 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode1_1  | 2021-07-12 13:48:22,506 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderStateImpl
datanode1_1  | 2021-07-12 13:48:22,543 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO impl.FollowerState: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5178604882ns, electionTimeout:5157ms
datanode1_1  | 2021-07-12 13:48:22,567 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState
datanode1_1  | 2021-07-12 13:48:22,567 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode1_1  | 2021-07-12 13:48:22,570 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode1_1  | 2021-07-12 13:48:22,572 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2
datanode1_1  | 2021-07-12 13:48:22,659 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:22,769 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2021-07-12 13:48:23,222 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode1_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode1_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode1_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode1_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode1_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode1_1  | 	... 18 more
om2_1        | 2021-07-12 13:48:32,533 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1        | 2021-07-12 13:48:32,550 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1        | 2021-07-12 13:48:32,551 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1        | 2021-07-12 13:48:32,640 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1        | 2021-07-12 13:48:32,643 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1        | 2021-07-12 13:48:32,681 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1        | 2021-07-12 13:48:32,681 [pool-24-thread-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1        | 2021-07-12 13:48:32,714 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1        | 2021-07-12 13:48:32,744 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1        | 2021-07-12 13:48:32,766 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1        | 2021-07-12 13:48:32,774 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1        | 2021-07-12 13:48:32,777 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1        | 2021-07-12 13:48:32,780 [pool-24-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1        | 2021-07-12 13:48:32,937 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om2@group-562213E44849
om2_1        | 2021-07-12 13:48:32,958 [pool-24-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om2@group-562213E44849
om2_1        | 2021-07-12 13:48:33,193 [Listener at om2/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om2_1        | 2021-07-12 13:48:33,512 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1        | 2021-07-12 13:48:33,574 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1        | 2021-07-12 13:48:33,576 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1        | 2021-07-12 13:48:33,749 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.25.0.112:9862
om2_1        | 2021-07-12 13:48:33,749 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1        | 2021-07-12 13:48:33,751 [Listener at om2/9862] INFO server.RaftServer$Division: om2@group-562213E44849: start as a follower, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2021-07-12 13:48:33,757 [Listener at om2/9862] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1        | 2021-07-12 13:48:33,760 [Listener at om2/9862] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2021-07-12 13:48:33,768 [Listener at om2/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om2
om2_1        | 2021-07-12 13:48:33,776 [Listener at om2/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om2@group-562213E44849
om2_1        | 2021-07-12 13:48:33,801 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1        | 2021-07-12 13:48:33,955 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1        | 2021-07-12 13:48:33,973 [Listener at om2/9862] INFO om.OzoneManager: Starting OM block token secret manager
om2_1        | 2021-07-12 13:48:33,973 [Listener at om2/9862] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2021-07-12 13:48:33,975 [Listener at om2/9862] INFO om.OzoneManager: Starting OM delegation token secret manager
om2_1        | 2021-07-12 13:48:33,978 [Listener at om2/9862] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om2_1        | 2021-07-12 13:48:33,979 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$392/0x000000084059b440@79927a7c] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1        | 2021-07-12 13:48:33,997 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1        | 2021-07-12 13:48:34,003 [Thread[Thread-16,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om2_1        | 2021-07-12 13:48:34,138 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1        | 2021-07-12 13:48:34,142 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
om2_1        | 2021-07-12 13:48:34,143 [Listener at om2/9862] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = kerberos
om2_1        | 2021-07-12 13:48:34,214 [Listener at om2/9862] INFO util.log: Logging initialized @46445ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1        | 2021-07-12 13:48:34,627 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1        | 2021-07-12 13:48:34,651 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1        | 2021-07-12 13:48:34,678 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1        | 2021-07-12 13:48:34,687 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1        | 2021-07-12 13:48:34,687 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1        | 2021-07-12 13:48:34,709 [Listener at om2/9862] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: ozone.om.http.auth.kerberos.principal keytabKey: ozone.om.http.auth.kerberos.keytab
om2_1        | 2021-07-12 13:48:34,880 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1        | 2021-07-12 13:48:34,887 [Listener at om2/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om2_1        | 2021-07-12 13:48:35,030 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1        | 2021-07-12 13:48:35,031 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1        | 2021-07-12 13:48:35,032 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1        | 2021-07-12 13:48:35,101 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
datanode1_1  | 2021-07-12 13:48:23,249 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-9943DCE46722->e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:23,366 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-LeaderElection1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C: set configuration 0: [5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:1], old=null
datanode1_1  | 2021-07-12 13:48:23,911 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:24,151 [5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-7844FDF8175C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7ef1c9bf-43c8-43b8-b00a-7844fdf8175c/current/log_inprogress_0
datanode1_1  | 2021-07-12 13:48:25,318 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.5b90731c-5249-453a-9f10-ddd353e45997
datanode2_1  | 2021-07-12 13:56:32,905 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1715,entriesCount=1,lastEntry=(t:2, i:29)
datanode2_1  | 2021-07-12 13:56:34,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:37,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:40,345 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:43,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:46,489 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:49,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:52,633 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:55,709 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:56:58,777 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:01,848 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:04,921 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:07,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:14,137 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:17,213 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:20,284 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:23,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:26,425 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:29,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:30,685 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1988,entriesCount=1,lastEntry=(t:2, i:30)
datanode2_1  | 2021-07-12 13:57:30,692 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1989,entriesCount=1,lastEntry=(t:2, i:31)
datanode2_1  | 2021-07-12 13:57:30,692 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1990,entriesCount=1,lastEntry=(t:2, i:32)
datanode2_1  | 2021-07-12 13:57:30,802 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1991,entriesCount=1,lastEntry=(t:2, i:33)
datanode2_1  | 2021-07-12 13:57:30,820 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1993,entriesCount=1,lastEntry=(t:2, i:34)
datanode2_1  | 2021-07-12 13:57:30,832 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=1994,entriesCount=1,lastEntry=(t:2, i:35)
datanode2_1  | 2021-07-12 13:57:32,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1.org_1   | 2021-07-12 13:46:19,880 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$421/0x0000000840523c40@5b01b3f8] INFO util.JvmPauseMonitor: JvmPauseMonitor-1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Started
om2_1        | 2021-07-12 13:48:35,105 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79228440{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1        | 2021-07-12 13:48:35,112 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c3fd72{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1        | 2021-07-12 13:48:35,505 [Listener at om2/9862] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/om@EXAMPLE.COM
om2_1        | 2021-07-12 13:48:35,602 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7960e87a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_0-SNAPSHOT_jar-_-any-5022644248524682373/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1        | 2021-07-12 13:48:35,643 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@54d93fce{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1        | 2021-07-12 13:48:35,643 [Listener at om2/9862] INFO server.Server: Started @47874ms
om2_1        | 2021-07-12 13:48:35,657 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1        | 2021-07-12 13:48:35,658 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1        | 2021-07-12 13:48:35,659 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1        | 2021-07-12 13:48:35,676 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1        | 2021-07-12 13:48:35,679 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1        | 2021-07-12 13:48:35,865 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om2_1        | 2021-07-12 13:48:35,929 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5ea206bc] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1        | 2021-07-12 13:48:38,927 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5167935151ns, electionTimeout:5156ms
om2_1        | 2021-07-12 13:48:38,928 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2021-07-12 13:48:38,929 [om2@group-562213E44849-FollowerState] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1        | 2021-07-12 13:48:38,932 [om2@group-562213E44849-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1        | 2021-07-12 13:48:38,932 [om2@group-562213E44849-FollowerState] INFO impl.RoleInfo: om2: start om2@group-562213E44849-LeaderElection1
om2_1        | 2021-07-12 13:48:38,946 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2021-07-12 13:48:41,304 [grpc-default-executor-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.om2
om2_1        | 2021-07-12 13:48:41,319 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 1, (t:0, i:~))
om2_1        | 2021-07-12 13:48:41,321 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-562213E44849-CANDIDATE: reject ELECTION from om1: already has voted for om2 at current term 1
om2_1        | 2021-07-12 13:48:41,359 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om1<-om2#0:FAIL-t1. Peer's state: om2@group-562213E44849:t1, leader=null, voted=om2, raftlog=om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2021-07-12 13:48:41,756 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
om2_1        | 2021-07-12 13:48:41,757 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:FAIL-t1
om2_1        | 2021-07-12 13:48:41,757 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Response 1: om2<-om3#0:FAIL-t1
om2_1        | 2021-07-12 13:48:41,757 [om2@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om2@group-562213E44849-LeaderElection1 ELECTION round 0: result REJECTED
om2_1        | 2021-07-12 13:48:41,758 [om2@group-562213E44849-LeaderElection1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om2_1        | 2021-07-12 13:48:41,758 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-LeaderElection1
om2_1        | 2021-07-12 13:48:41,761 [om2@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2021-07-12 13:48:42,131 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om3, group-562213E44849, 1, (t:0, i:~))
om2_1        | 2021-07-12 13:48:42,131 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: reject ELECTION from om3: already has voted for om2 at current term 1
om2_1        | 2021-07-12 13:48:42,131 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om3<-om2#0:FAIL-t1. Peer's state: om2@group-562213E44849:t1, leader=null, voted=om2, raftlog=om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
om2_1        | 2021-07-12 13:48:46,866 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: receive requestVote(ELECTION, om1, group-562213E44849, 2, (t:0, i:~))
om2_1        | 2021-07-12 13:48:46,867 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-562213E44849-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om2_1        | 2021-07-12 13:48:46,872 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:om1
om2_1        | 2021-07-12 13:48:46,872 [grpc-default-executor-1] INFO impl.RoleInfo: om2: shutdown om2@group-562213E44849-FollowerState
om2_1        | 2021-07-12 13:48:46,873 [om2@group-562213E44849-FollowerState] INFO impl.FollowerState: om2@group-562213E44849-FollowerState was interrupted: {}
om2_1        | java.lang.InterruptedException: sleep interrupted
om2_1        | 	at java.base/java.lang.Thread.sleep(Native Method)
om2_1        | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
om2_1        | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
om2_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
om2_1        | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
om2_1        | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
om2_1        | 2021-07-12 13:48:46,875 [grpc-default-executor-1] INFO impl.RoleInfo: om2: start om2@group-562213E44849-FollowerState
om2_1        | 2021-07-12 13:48:46,879 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849 replies to ELECTION vote request: om1<-om2#0:OK-t2. Peer's state: om2@group-562213E44849:t2, leader=null, voted=om1, raftlog=om2@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1|rpc:om1:9872|priority:0, om3|rpc:om3:9872|priority:0, om2|rpc:om2:9872|priority:0], old=null
datanode2_1  | 2021-07-12 13:57:35,641 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:37,231 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2245,entriesCount=1,lastEntry=(t:2, i:36)
datanode2_1  | 2021-07-12 13:57:37,236 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2246,entriesCount=1,lastEntry=(t:2, i:37)
datanode2_1  | 2021-07-12 13:57:37,240 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2247,entriesCount=1,lastEntry=(t:2, i:38)
datanode2_1  | 2021-07-12 13:57:37,248 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2248,entriesCount=1,lastEntry=(t:2, i:39)
datanode2_1  | 2021-07-12 13:57:38,712 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:41,786 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:43,809 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2500,entriesCount=1,lastEntry=(t:2, i:40)
datanode2_1  | 2021-07-12 13:57:43,821 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2501,entriesCount=1,lastEntry=(t:2, i:41)
datanode2_1  | 2021-07-12 13:57:43,832 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2502,entriesCount=1,lastEntry=(t:2, i:42)
datanode2_1  | 2021-07-12 13:57:43,986 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2503,entriesCount=1,lastEntry=(t:2, i:43)
datanode2_1  | 2021-07-12 13:57:43,996 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2504,entriesCount=1,lastEntry=(t:2, i:44)
datanode2_1  | 2021-07-12 13:57:44,010 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2506,entriesCount=1,lastEntry=(t:2, i:45)
datanode2_1  | 2021-07-12 13:57:44,856 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:47,928 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:51,002 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:54,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:57:57,145 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:00,862 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2756,entriesCount=1,lastEntry=(t:2, i:46)
datanode2_1  | 2021-07-12 13:58:01,032 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2757,entriesCount=1,lastEntry=(t:2, i:47)
datanode2_1  | 2021-07-12 13:58:01,033 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2758,entriesCount=1,lastEntry=(t:2, i:48)
datanode2_1  | 2021-07-12 13:58:01,191 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2759,entriesCount=1,lastEntry=(t:2, i:49)
datanode2_1  | 2021-07-12 13:58:01,223 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2760,entriesCount=1,lastEntry=(t:2, i:50)
datanode2_1  | 2021-07-12 13:58:01,250 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2762,entriesCount=1,lastEntry=(t:2, i:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:00,229 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-76397, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:56:00,241 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-76397
s3g_1        | 2021-07-12 13:56:00,772 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-96672, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:56:00,786 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-96672
s3g_1        | 2021-07-12 13:56:01,915 [qtp1771421544-21] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
datanode3_1  | 2021-07-12 13:48:43,942 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode3_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode3_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode3_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode3_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode3_1  | 	... 18 more
datanode3_1  | 2021-07-12 13:48:43,954 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4865A687B399->e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode3_1  | 2021-07-12 13:48:44,089 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode3_1  | 2021-07-12 13:48:44,089 [grpc-default-executor-0] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 1 for appendEntries, leader elected after 651ms
datanode3_1  | 2021-07-12 13:48:44,097 [grpc-default-executor-0] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
datanode3_1  | 2021-07-12 13:48:44,122 [grpc-default-executor-0] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: inconsistency entries. Reply:e7ed1272-c70a-4bce-ab84-6d03b85621dc<-8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#9:FAIL-t0,INCONSISTENCY,nextIndex=0,followerCommit=-1
datanode3_1  | 2021-07-12 13:48:44,174 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ABE0FEB7C4D8 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode3_1  | 2021-07-12 13:48:44,175 [grpc-default-executor-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 2 for appendEntries, leader elected after 1688ms
datanode3_1  | 2021-07-12 13:48:44,176 [grpc-default-executor-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: Failed appendEntries as previous log entry ((t:2, i:0)) is not found
datanode3_1  | 2021-07-12 13:48:44,178 [grpc-default-executor-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: inconsistency entries. Reply:e7ed1272-c70a-4bce-ab84-6d03b85621dc<-8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#7:FAIL-t0,INCONSISTENCY,nextIndex=0,followerCommit=-1
datanode3_1  | 2021-07-12 13:48:44,206 [grpc-default-executor-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:44,208 [grpc-default-executor-0] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:44,214 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2021-07-12 13:48:44,226 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2021-07-12 13:48:44,305 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode3_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode3_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode3_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode3_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode3_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode3_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode3_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode3_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode3_1  | 	... 18 more
datanode3_1  | 2021-07-12 13:48:44,312 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=660497cf-d26f-4475-b297-562b256bcc94.
datanode3_1  | 2021-07-12 13:48:44,377 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0
datanode3_1  | 2021-07-12 13:48:44,380 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/current/log_inprogress_0
datanode3_1  | 2021-07-12 13:48:46,109 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:47,267 [grpc-default-executor-1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: shutdown 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-FollowerState
datanode3_1  | 2021-07-12 13:48:47,267 [grpc-default-executor-1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode3_1  | 2021-07-12 13:48:47,268 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-FollowerState] INFO impl.FollowerState: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-FollowerState was interrupted: {}
datanode3_1  | java.lang.InterruptedException: sleep interrupted
datanode3_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode3_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode3_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode3_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode3_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode3_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode3_1  | 2021-07-12 13:48:47,271 [grpc-default-executor-1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1
datanode3_1  | 2021-07-12 13:48:47,288 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: change Leader from e7ed1272-c70a-4bce-ab84-6d03b85621dc to null at term 1 for ELECTION
datanode3_1  | 2021-07-12 13:48:47,297 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.LeaderElection: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:47,491 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState] INFO impl.FollowerState: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5043430761ns, electionTimeout:5042ms
datanode3_1  | 2021-07-12 13:48:47,505 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: shutdown 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState
datanode3_1  | 2021-07-12 13:48:47,505 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode3_1  | 2021-07-12 13:48:47,505 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode3_1  | 2021-07-12 13:48:47,505 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-FollowerState] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2
datanode3_1  | 2021-07-12 13:48:47,519 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO impl.LeaderElection: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1], old=null
datanode3_1  | 2021-07-12 13:48:47,530 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO impl.LeaderElection: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode3_1  | 2021-07-12 13:48:47,530 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: shutdown 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2
datanode3_1  | 2021-07-12 13:48:47,531 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode3_1  | 2021-07-12 13:48:47,531 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C4D54606D42F with new leaderId: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:47,531 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: change Leader from null to 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 at term 1 for becomeLeader, leader elected after 5261ms
datanode3_1  | 2021-07-12 13:48:47,533 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2021-07-12 13:48:47,576 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F
datanode3_1  | 2021-07-12 13:48:47,592 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2021-07-12 13:48:47,594 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode3_1  | 2021-07-12 13:48:47,634 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2021-07-12 13:48:47,636 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2021-07-12 13:48:47,637 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2021-07-12 13:48:47,720 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderStateImpl
datanode3_1  | 2021-07-12 13:48:47,740 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker: Starting segment from index:0
datanode3_1  | 2021-07-12 13:48:47,744 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ff1246ee-6927-4ea3-b470-c4d54606d42f/current/log_inprogress_0
datanode3_1  | 2021-07-12 13:48:47,795 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F-LeaderElection2] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-C4D54606D42F: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1], old=null
datanode3_1  | 2021-07-12 13:48:48,258 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.LeaderElection: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode3_1  | 2021-07-12 13:48:48,259 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.LeaderElection:   Response 0: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t2
datanode3_1  | 2021-07-12 13:48:48,259 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.LeaderElection: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1 ELECTION round 0: result PASSED
datanode3_1  | 2021-07-12 13:48:48,259 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: shutdown 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1
datanode3_1  | 2021-07-12 13:48:48,259 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode3_1  | 2021-07-12 13:48:48,260 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode3_1  | 2021-07-12 13:48:48,260 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: change Leader from null to 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 at term 2 for becomeLeader, leader elected after 971ms
datanode3_1  | 2021-07-12 13:48:48,261 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode3_1  | 2021-07-12 13:48:48,261 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode3_1  | 2021-07-12 13:48:48,261 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:48,261 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode3_1  | 2021-07-12 13:48:48,262 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode3_1  | 2021-07-12 13:48:48,262 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode3_1  | 2021-07-12 13:48:48,263 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode3_1  | 2021-07-12 13:48:48,264 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode3_1  | 2021-07-12 13:48:48,282 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: Completed APPEND_ENTRIES, lastRequest: e7ed1272-c70a-4bce-ab84-6d03b85621dc->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7#10-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY
datanode3_1  | 2021-07-12 13:48:48,283 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2021-07-12 13:48:48,283 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2021-07-12 13:48:48,284 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode3_1  | 2021-07-12 13:48:48,291 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2021-07-12 13:48:48,291 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2021-07-12 13:48:48,291 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2021-07-12 13:48:48,292 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode3_1  | 2021-07-12 13:48:48,311 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode3_1  | 2021-07-12 13:48:48,316 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO impl.RoleInfo: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: start 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderStateImpl
datanode3_1  | 2021-07-12 13:48:48,317 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode3_1  | 2021-07-12 13:48:48,320 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0 to /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_0-0
datanode3_1  | 2021-07-12 13:48:48,325 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-LeaderElection1] INFO server.RaftServer$Division: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94: set configuration 1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode3_1  | 2021-07-12 13:48:48,340 [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_1
datanode3_1  | 2021-07-12 13:48:49,176 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:52,253 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:48:54,324 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode3_1  | java.net.NoRouteToHostException: No Route to Host from  abb35404295f/172.25.0.104 to recon:9891 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
datanode3_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode3_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode3_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode3_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode3_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode3_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:855)
datanode3_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode3_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode3_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode3_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode3_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode3_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode3_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode3_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode3_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode3_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode3_1  | Caused by: java.net.NoRouteToHostException: No route to host
datanode3_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode3_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
datanode3_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
datanode3_1  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode3_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)
datanode3_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:822)
datanode3_1  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
datanode3_1  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1647)
datanode3_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1463)
datanode3_1  | 	... 12 more
datanode3_1  | 2021-07-12 13:48:58,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:01,465 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:04,541 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:07,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:10,681 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:13,757 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:16,825 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:17,709 [ChunkWriter-1-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:3216053955267.
datanode3_1  | 2021-07-12 13:49:19,898 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:22,969 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:26,041 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:29,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:32,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:35,256 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:38,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:41,402 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:47,544 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:50,616 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:53,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:56,761 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:49:59,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:02,904 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:25,554 [grpc-default-executor-0] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: receive requestVote(ELECTION, e7ed1272-c70a-4bce-ab84-6d03b85621dc, group-ABE0FEB7C4D8, 1, (t:0, i:0))
datanode1_1  | 2021-07-12 13:48:25,570 [grpc-default-executor-0] INFO impl.VoteContext: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-CANDIDATE: reject ELECTION from e7ed1272-c70a-4bce-ab84-6d03b85621dc: already has voted for 5b90731c-5249-453a-9f10-ddd353e45997 at current term 1
datanode1_1  | 2021-07-12 13:48:25,689 [grpc-default-executor-0] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8 replies to ELECTION vote request: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:FAIL-t1. Peer's state: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8:t1, leader=null, voted=5b90731c-5249-453a-9f10-ddd353e45997, raftlog=5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:26,044 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: receive requestVote(ELECTION, e7ed1272-c70a-4bce-ab84-6d03b85621dc, group-562B256BCC94, 1, (t:0, i:0))
datanode1_1  | 2021-07-12 13:48:26,044 [grpc-default-executor-1] INFO impl.VoteContext: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FOLLOWER: accept ELECTION from e7ed1272-c70a-4bce-ab84-6d03b85621dc: our priority 0 <= candidate's priority 0
datanode1_1  | 2021-07-12 13:48:26,045 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:26,045 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState
datanode1_1  | 2021-07-12 13:48:26,045 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState
datanode1_1  | 2021-07-12 13:48:26,046 [5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState] INFO impl.FollowerState: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode1_1  | 2021-07-12 13:48:26,090 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94 replies to ELECTION vote request: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t1. Peer's state: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94:t1, leader=null, voted=e7ed1272-c70a-4bce-ab84-6d03b85621dc, raftlog=5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:26,214 [Command processor thread] WARN commandhandler.CreatePipelineCommandHandler: Add group failed for e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
datanode1_1  | java.io.IOException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:92)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:217)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:178)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:96)
datanode1_1  | 	at org.apache.ratis.client.impl.BlockingImpl.sendRequest(BlockingImpl.java:130)
datanode1_1  | 	at org.apache.ratis.client.impl.GroupManagementImpl.add(GroupManagementImpl.java:51)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.lambda$handle$1(CreatePipelineCommandHandler.java:99)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
datanode1_1  | 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
datanode1_1  | 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
datanode1_1  | 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
datanode1_1  | 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CreatePipelineCommandHandler.handle(CreatePipelineCommandHandler.java:95)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$2(DatanodeStateMachine.java:550)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:262)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:243)
datanode1_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:156)
datanode1_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:413)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$5(GrpcClientProtocolClient.java:180)
datanode1_1  | 	at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:215)
datanode1_1  | 	... 18 more
datanode1_1  | 2021-07-12 13:48:26,214 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=660497cf-d26f-4475-b297-562b256bcc94.
datanode1_1  | 2021-07-12 13:48:26,214 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode1_1  | 2021-07-12 13:48:26,296 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2: ELECTION REJECTED received 1 response(s) and 1 exception(s):
datanode1_1  | 2021-07-12 13:48:26,299 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection:   Response 0: 5b90731c-5249-453a-9f10-ddd353e45997<-e7ed1272-c70a-4bce-ab84-6d03b85621dc#0:FAIL-t1
datanode1_1  | 2021-07-12 13:48:26,300 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: group-ABE0FEB7C4D8 not found.
datanode1_1  | 2021-07-12 13:48:26,300 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.LeaderElection: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2 ELECTION round 0: result REJECTED
datanode1_1  | 2021-07-12 13:48:26,304 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode1_1  | 2021-07-12 13:48:26,305 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2
datanode1_1  | 2021-07-12 13:48:26,306 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-LeaderElection2] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState
datanode1_1  | 2021-07-12 13:48:26,430 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:26,446 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 1 for appendEntries, leader elected after 4817ms
datanode1_1  | 2021-07-12 13:48:26,547 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:26,548 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2021-07-12 13:48:26,558 [5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0
datanode1_1  | 2021-07-12 13:48:26,968 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:30,041 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:31,220 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: receive requestVote(ELECTION, e7ed1272-c70a-4bce-ab84-6d03b85621dc, group-ABE0FEB7C4D8, 2, (t:0, i:0))
datanode1_1  | 2021-07-12 13:48:31,221 [grpc-default-executor-1] INFO impl.VoteContext: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FOLLOWER: accept ELECTION from e7ed1272-c70a-4bce-ab84-6d03b85621dc: our priority 0 <= candidate's priority 1
datanode1_1  | 2021-07-12 13:48:31,221 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:31,221 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState
datanode1_1  | 2021-07-12 13:48:31,221 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState] INFO impl.FollowerState: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode1_1  | 2021-07-12 13:48:31,228 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-FollowerState
datanode1_1  | 2021-07-12 13:48:31,232 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8 replies to ELECTION vote request: e7ed1272-c70a-4bce-ab84-6d03b85621dc<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t2. Peer's state: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8:t2, leader=null, voted=e7ed1272-c70a-4bce-ab84-6d03b85621dc, raftlog=5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:31,479 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-ABE0FEB7C4D8 with new leaderId: e7ed1272-c70a-4bce-ab84-6d03b85621dc
datanode1_1  | 2021-07-12 13:48:31,479 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: change Leader from null to e7ed1272-c70a-4bce-ab84-6d03b85621dc at term 2 for appendEntries, leader elected after 14209ms
datanode1_1  | 2021-07-12 13:48:31,505 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8: set configuration 0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:0, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:1, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:31,517 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode1_1  | 2021-07-12 13:48:31,541 [5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-ABE0FEB7C4D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7135b98c-5758-43ad-b6de-abe0feb7c4d8/current/log_inprogress_0
datanode1_1  | 2021-07-12 13:48:33,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:36,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:39,260 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:42,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:45,401 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:48,223 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: receive requestVote(ELECTION, 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, group-562B256BCC94, 2, (t:1, i:0))
datanode1_1  | 2021-07-12 13:48:48,224 [grpc-default-executor-1] INFO impl.VoteContext: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FOLLOWER: accept ELECTION from 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7: our priority 0 <= candidate's priority 1
datanode1_1  | 2021-07-12 13:48:48,224 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: change Leader from e7ed1272-c70a-4bce-ab84-6d03b85621dc to null at term 2 for updateCurrentTerm
datanode1_1  | 2021-07-12 13:48:48,224 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode1_1  | 2021-07-12 13:48:48,224 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: shutdown 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState
datanode1_1  | 2021-07-12 13:48:48,225 [grpc-default-executor-1] INFO impl.RoleInfo: 5b90731c-5249-453a-9f10-ddd353e45997: start 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState
datanode1_1  | 2021-07-12 13:48:48,225 [5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState] INFO impl.FollowerState: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-FollowerState was interrupted: {}
datanode1_1  | java.lang.InterruptedException: sleep interrupted
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode1_1  | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode1_1  | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode1_1  | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode1_1  | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode1_1  | 2021-07-12 13:48:48,236 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94 replies to ELECTION vote request: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7<-5b90731c-5249-453a-9f10-ddd353e45997#0:OK-t2. Peer's state: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94:t2, leader=null, voted=8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, raftlog=5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLog:OPENED:c0,f0,i0, conf=0: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:48,272 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 5b90731c-5249-453a-9f10-ddd353e45997: Completed APPEND_ENTRIES, lastRequest: e7ed1272-c70a-4bce-ab84-6d03b85621dc->5b90731c-5249-453a-9f10-ddd353e45997#1-t1,previous=(t:0, i:0),leaderCommit=0,initializing? true,entries: size=1, first=(t:1, i:0), CONFIGURATIONENTRY
datanode1_1  | 2021-07-12 13:48:48,349 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-562B256BCC94 with new leaderId: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
datanode1_1  | 2021-07-12 13:48:48,350 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: change Leader from null to 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7 at term 2 for appendEntries, leader elected after 124ms
datanode1_1  | 2021-07-12 13:48:48,382 [grpc-default-executor-1] INFO server.RaftServer$Division: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94: set configuration 1: [8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7|rpc:172.25.0.104:9856|admin:172.25.0.104:9857|client:172.25.0.104:9858|dataStream:|priority:1, e7ed1272-c70a-4bce-ab84-6d03b85621dc|rpc:172.25.0.103:9856|admin:172.25.0.103:9857|client:172.25.0.103:9858|dataStream:|priority:0, 5b90731c-5249-453a-9f10-ddd353e45997|rpc:172.25.0.102:9856|admin:172.25.0.102:9857|client:172.25.0.102:9858|dataStream:|priority:0], old=null
datanode1_1  | 2021-07-12 13:48:48,383 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
datanode1_1  | 2021-07-12 13:48:48,386 [5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_0 to /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_0-0
datanode1_1  | 2021-07-12 13:48:48,392 [5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5b90731c-5249-453a-9f10-ddd353e45997@group-562B256BCC94-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/660497cf-d26f-4475-b297-562b256bcc94/current/log_inprogress_1
datanode1_1  | 2021-07-12 13:48:48,473 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1        | 2021-07-12 13:55:37,497 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36204
om1_1        | 2021-07-12 13:55:37,513 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,576 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:37,577 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36206
om1_1        | 2021-07-12 13:55:37,585 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,641 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:37,641 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36208
om1_1        | 2021-07-12 13:55:37,650 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,689 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:37,690 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36210
om1_1        | 2021-07-12 13:55:37,701 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:37,768 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:37,769 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36214
om1_1        | 2021-07-12 13:55:37,787 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:40,492 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:40,492 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36222
om1_1        | 2021-07-12 13:55:40,505 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:40,595 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:40,596 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36226
om1_1        | 2021-07-12 13:55:40,613 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:40,665 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:40,666 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36228
om1_1        | 2021-07-12 13:55:40,673 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:40,896 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:40,897 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36234
om1_1        | 2021-07-12 13:55:40,908 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:40,997 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:40,998 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36236
om1_1        | 2021-07-12 13:55:41,002 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:41,044 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:41,045 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36238
om1_1        | 2021-07-12 13:55:41,057 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:41,112 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:41,113 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36240
om1_1        | 2021-07-12 13:55:41,119 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:42,516 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:42,517 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36250
om1_1        | 2021-07-12 13:55:42,555 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:44,472 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:44,472 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36282
om1_1        | 2021-07-12 13:55:44,509 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:48,646 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49560
om1_1        | 2021-07-12 13:55:48,666 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:51,414 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:51,415 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36318
om1_1        | 2021-07-12 13:55:51,423 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:51,995 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:51,996 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36322
om1_1        | 2021-07-12 13:55:52,010 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:52,556 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:52,557 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36328
om1_1        | 2021-07-12 13:55:52,565 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om2_1        | 2021-07-12 13:48:47,278 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: change Leader from null to om1 at term 2 for appendEntries, leader elected after 15200ms
om2_1        | 2021-07-12 13:48:47,441 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-562213E44849: set configuration 0: [om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0], old=null
om2_1        | 2021-07-12 13:48:47,463 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1        | 2021-07-12 13:48:48,022 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om2_1        | 2021-07-12 13:49:14,337 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:testuser/scm@EXAMPLE.COM
om2_1        | 2021-07-12 13:50:19,480 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-source for user:testuser/scm@EXAMPLE.COM
om2_1        | 2021-07-12 13:50:24,202 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:05466-target for user:testuser/scm@EXAMPLE.COM
om2_1        | 2021-07-12 13:53:16,423 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:link2 in volume:05466-target
om2_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:53:25,400 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket3 in volume:05466-target
om2_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:55:52,587 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-77473 in volume:s3v
om2_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:01,908 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om2_1        | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
om2_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:119)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:41,164 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-16724/10474/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om2_1        | 2021-07-12 13:56:41,167 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 10474/multipartKey2 in Volume/Bucket s3v/bucket-16724
om2_1        | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 10474/multipartKey2. Entity too small.
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:463)
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | 2021-07-12 13:48:51,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:54,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:57,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:48:59,762 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode1_1  | java.net.NoRouteToHostException: No Route to Host from  27bc097164fa/172.25.0.102 to recon:9891 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
datanode1_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode1_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode1_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode1_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode1_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode1_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:855)
datanode1_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode1_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode1_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode1_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode1_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode1_1  | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
datanode1_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode1_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode1_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode1_1  | Caused by: java.net.NoRouteToHostException: No route to host
datanode1_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
datanode1_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
datanode1_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
datanode1_1  | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
datanode1_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)
datanode1_1  | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:822)
datanode1_1  | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
datanode1_1  | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1647)
datanode1_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1463)
datanode1_1  | 	... 12 more
datanode1_1  | 2021-07-12 13:49:03,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:06,909 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:09,977 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:13,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:16,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:18,149 [ChunkWriter-7-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:3216053955267.
datanode1_1  | 2021-07-12 13:49:19,204 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:22,269 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:25,336 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:28,408 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:31,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:34,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:37,625 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:40,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1        |   <Code>NoSuchBucket</Code>
s3g_1        |   <Message>The specified bucket does not exist</Message>
s3g_1        |   <Resource>nosuchbucket</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
datanode1_1  | 2021-07-12 13:49:43,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:46,840 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:52,985 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:56,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:49:59,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:02,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:05,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:08,345 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:11,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:14,489 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:17,565 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:20,633 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:23,705 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:26,781 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:29,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:32,921 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:35,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:42,137 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:45,213 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:48,281 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:51,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:54,425 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:50:57,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:00,572 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:03,641 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:06,713 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:09,789 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1.org_1   | 2021-07-12 13:46:19,965 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1.org_1   | 2021-07-12 13:46:19,976 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1.org_1   | 2021-07-12 13:46:19,977 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1.org_1   | 2021-07-12 13:46:20,358 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1.org_1   | 2021-07-12 13:46:20,360 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2021-07-12 13:46:20,376 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1.org_1   | 2021-07-12 13:46:20,411 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1.org_1   | 2021-07-12 13:46:20,417 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1.org_1   | 2021-07-12 13:46:20,418 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2021-07-12 13:46:20,418 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1.org_1   | 2021-07-12 13:46:20,443 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm1.org_1   | 2021-07-12 13:46:20,460 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm1.org_1   | 2021-07-12 13:46:20,463 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2021-07-12 13:46:20,468 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm1.org_1   | 2021-07-12 13:46:20,468 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm1.org_1   | 2021-07-12 13:46:20,531 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@773ad729] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1.org_1   | 2021-07-12 13:46:20,536 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1.org_1   | 2021-07-12 13:46:20,537 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm1.org_1   | 2021-07-12 13:46:20,538 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm1.org_1   | 2021-07-12 13:46:20,562 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7225ms to org.eclipse.jetty.util.log.Slf4jLog
scm1.org_1   | 2021-07-12 13:46:20,648 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1.org_1   | 2021-07-12 13:46:20,654 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1.org_1   | 2021-07-12 13:46:20,656 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm1.org_1   | 2021-07-12 13:46:20,656 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm1.org_1   | 2021-07-12 13:46:20,656 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm1.org_1   | 2021-07-12 13:46:20,659 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm1.org_1   | 2021-07-12 13:46:20,690 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1.org_1   | 2021-07-12 13:46:20,692 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm1.org_1   | 2021-07-12 13:46:20,719 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1.org_1   | 2021-07-12 13:46:20,719 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1.org_1   | 2021-07-12 13:46:20,721 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm1.org_1   | 2021-07-12 13:46:20,742 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2021-07-12 13:46:20,750 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a5244a1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1.org_1   | 2021-07-12 13:46:20,751 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4848681d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1.org_1   | 2021-07-12 13:46:20,848 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm1.org_1   | 2021-07-12 13:46:20,859 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4ad04b05{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_0-SNAPSHOT_jar-_-any-12972888742717899054/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/scm}
scm1.org_1   | 2021-07-12 13:46:20,870 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@4b6ad2eb{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1.org_1   | 2021-07-12 13:46:20,870 [Listener at 0.0.0.0/9860] INFO server.Server: Started @7533ms
scm1.org_1   | 2021-07-12 13:46:20,872 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1.org_1   | 2021-07-12 13:46:20,872 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1.org_1   | 2021-07-12 13:46:20,875 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1.org_1   | 2021-07-12 13:46:21,885 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:45504
scm1.org_1   | 2021-07-12 13:46:21,920 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:46:24,488 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:38870
scm1.org_1   | 2021-07-12 13:46:24,506 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:46:24,821 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.FollowerState: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5018758875ns, electionTimeout:5008ms
scm1.org_1   | 2021-07-12 13:46:24,823 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState
scm1.org_1   | 2021-07-12 13:46:24,823 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1.org_1   | 2021-07-12 13:46:24,826 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1.org_1   | 2021-07-12 13:46:24,826 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-FollowerState] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1
scm1.org_1   | 2021-07-12 13:46:24,845 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.LeaderElection: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:24,846 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.LeaderElection: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1.org_1   | 2021-07-12 13:46:24,847 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: shutdown 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1
scm1.org_1   | 2021-07-12 13:46:24,848 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1.org_1   | 2021-07-12 13:46:24,848 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1.org_1   | 2021-07-12 13:46:24,848 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1.org_1   | 2021-07-12 13:46:24,852 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: change Leader from null to 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9 at term 2 for becomeLeader, leader elected after 7008ms
scm1.org_1   | 2021-07-12 13:46:24,858 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1.org_1   | 2021-07-12 13:46:24,860 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:24,860 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
om2_1        | 2021-07-12 13:56:42,398 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om2_1        | partName: "etag1"
om2_1        | , partNumber: 2
om2_1        | partName: "etag2"
om2_1        | ]
om2_1        | 2021-07-12 13:56:42,402 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om2_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:42,982 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om2_1        | partName: "etag1"
om2_1        | , partNumber: 1
om2_1        | partName: "etag2"
om2_1        | ]
om2_1        | 2021-07-12 13:56:42,985 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om2_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:49,225 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om2_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567971869294625
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:49,772 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
datanode1_1  | 2021-07-12 13:51:12,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:15,929 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:19,001 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:22,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:25,145 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:31,291 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:34,361 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:37,433 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:40,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:43,577 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:46,649 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:49,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:52,793 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:55,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:51:58,941 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:02,009 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:05,081 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:08,153 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:11,225 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:14,296 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:20,445 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:23,513 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:26,585 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:29,660 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:32,728 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:35,800 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:38,872 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:41,945 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3.org_1   | Sleeping for 5 seconds
scm3.org_1   | Waiting for the service scm2.org:9894
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2021-07-12 13:46:47,978 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = [--bootstrap]
scm3.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
datanode2_1  | 2021-07-12 13:58:01,422 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2771,entriesCount=1,lastEntry=(t:2, i:52)
datanode2_1  | 2021-07-12 13:58:01,422 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2772,entriesCount=1,lastEntry=(t:2, i:53)
datanode2_1  | 2021-07-12 13:58:01,735 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2798,entriesCount=1,lastEntry=(t:2, i:54)
datanode2_1  | 2021-07-12 13:58:01,808 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2805,entriesCount=1,lastEntry=(t:2, i:55)
datanode2_1  | 2021-07-12 13:58:01,833 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=2807,entriesCount=1,lastEntry=(t:2, i:56)
datanode2_1  | 2021-07-12 13:58:03,288 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:06,360 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:08,059 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3040,entriesCount=1,lastEntry=(t:2, i:57)
datanode2_1  | 2021-07-12 13:58:08,110 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3041,entriesCount=1,lastEntry=(t:2, i:58)
datanode2_1  | 2021-07-12 13:58:08,176 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3042,entriesCount=1,lastEntry=(t:2, i:59)
datanode2_1  | 2021-07-12 13:58:08,240 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3043,entriesCount=1,lastEntry=(t:2, i:60)
datanode2_1  | 2021-07-12 13:58:08,260 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3045,entriesCount=1,lastEntry=(t:2, i:61)
datanode2_1  | 2021-07-12 13:58:08,276 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3047,entriesCount=1,lastEntry=(t:2, i:62)
datanode2_1  | 2021-07-12 13:58:09,432 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:12,289 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3299,entriesCount=1,lastEntry=(t:2, i:63)
datanode2_1  | 2021-07-12 13:58:12,319 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3300,entriesCount=1,lastEntry=(t:2, i:64)
datanode2_1  | 2021-07-12 13:58:12,322 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3301,entriesCount=1,lastEntry=(t:2, i:65)
datanode2_1  | 2021-07-12 13:58:12,450 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3302,entriesCount=1,lastEntry=(t:2, i:66)
datanode2_1  | 2021-07-12 13:58:12,452 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3303,entriesCount=1,lastEntry=(t:2, i:67)
datanode2_1  | 2021-07-12 13:58:12,469 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3305,entriesCount=1,lastEntry=(t:2, i:68)
datanode2_1  | 2021-07-12 13:58:12,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:15,576 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:18,649 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:19,470 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3551,entriesCount=1,lastEntry=(t:2, i:69)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:08,555 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-01186, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:56:08,580 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-01186
s3g_1        | 2021-07-12 13:56:09,591 [qtp1771421544-21] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchBucket</Code>
s3g_1        |   <Message>The specified bucket does not exist</Message>
s3g_1        |   <Resource>ozonenosuchbucketqqweqwe</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.10
scm3.org_1   | ************************************************************/
scm3.org_1   | 2021-07-12 13:46:48,012 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2021-07-12 13:46:48,174 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2021-07-12 13:46:48,181 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2021-07-12 13:46:48,287 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2021-07-12 13:46:48,296 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2021-07-12 13:46:48,316 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2021-07-12 13:46:48,950 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2021-07-12 13:46:48,951 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2021-07-12 13:46:50,149 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm3.org_1   | 2021-07-12 13:46:51,107 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm3.org_1   | 2021-07-12 13:46:51,112 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm3.org_1   | 2021-07-12 13:46:51,114 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm3.org_1   | 2021-07-12 13:46:53,461 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm3.org_1   | 2021-07-12 13:46:53,530 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.118,host:scm3.org
scm3.org_1   | 2021-07-12 13:46:53,530 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm3.org_1   | 2021-07-12 13:46:53,534 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm3.org,scmId:cae79eb6-d521-4d9d-a048-244f8c0679d9,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:scm-sub@scm3.org
scm3.org_1   | 2021-07-12 13:46:55,273 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm3.org_1   | 2021-07-12 13:46:55,329 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd, SCMID cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:46:55,330 [main] INFO server.StorageContainerManager: Primary SCM Node ID 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm3.org_1   | 2021-07-12 13:46:55,378 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm3.org/172.25.0.118
scm3.org_1   | ************************************************************/
scm3.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3.org_1   | 2021-07-12 13:46:57,227 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3.org_1   | /************************************************************
scm3.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm3.org_1   | STARTUP_MSG:   host = scm3.org/172.25.0.118
scm3.org_1   | STARTUP_MSG:   args = []
scm3.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
scm3.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm3.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm3.org_1   | STARTUP_MSG:   java = 11.0.10
scm3.org_1   | ************************************************************/
scm3.org_1   | 2021-07-12 13:46:57,236 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3.org_1   | 2021-07-12 13:46:57,357 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3.org_1   | 2021-07-12 13:46:57,357 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3.org_1   | 2021-07-12 13:46:57,409 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3.org:9894 and Ratis port: 9894
scm3.org_1   | 2021-07-12 13:46:57,410 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3.org
scm3.org_1   | 2021-07-12 13:46:57,440 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2021-07-12 13:46:57,467 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm3.org_1   | 2021-07-12 13:46:57,817 [main] INFO reflections.Reflections: Reflections took 185 ms to scan 3 urls, producing 103 keys and 210 values 
scm3.org_1   | 2021-07-12 13:46:59,034 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm3.org_1   | 2021-07-12 13:46:59,221 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/3165257652688.crt.
scm3.org_1   | 2021-07-12 13:46:59,229 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm3.org_1   | 2021-07-12 13:46:59,240 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm3.org_1   | 2021-07-12 13:46:59,480 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm3.org_1   | 2021-07-12 13:46:59,480 [main] INFO server.StorageContainerManager: SCM login successful.
scm3.org_1   | 2021-07-12 13:46:59,513 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3.org_1   | 2021-07-12 13:46:59,851 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode3_1  | 2021-07-12 13:50:05,977 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:09,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:12,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:15,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.10
scm2.org_1   | ************************************************************/
scm2.org_1   | 2021-07-12 13:46:08,835 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2021-07-12 13:46:08,984 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2.org_1   | 2021-07-12 13:46:08,985 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2021-07-12 13:46:09,023 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2021-07-12 13:46:09,024 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2021-07-12 13:46:09,030 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2021-07-12 13:46:09,343 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2021-07-12 13:46:09,343 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2021-07-12 13:46:11,636 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:13,640 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:15,642 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm1.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:17,644 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:19,648 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:22,084 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1e6f42a1-e2d2-4f78-bf5d-162da394b7e9 is not the leader. Could not determine the leader node.
scm2.org_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:196)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:108)
scm2.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13937)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm2.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm2.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm2.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm2.org_1   | , while invoking $Proxy15.send over nodeId=scm1,nodeAddress=scm1.org/172.25.0.116:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:24,086 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm2.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm2,nodeAddress=scm2.org/172.25.0.117:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:26,087 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From scm2.org/172.25.0.117 to scm3.org:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy15.send over nodeId=scm3,nodeAddress=scm3.org/172.25.0.118:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
scm2.org_1   | 2021-07-12 13:46:28,189 [main] INFO ha.HASecurityUtils: Initializing secure StorageContainerManager.
scm2.org_1   | 2021-07-12 13:46:28,825 [main] ERROR client.SCMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
scm2.org_1   | 2021-07-12 13:46:28,825 [main] INFO client.SCMCertificateClient: Certificate client init case: 0
scm2.org_1   | 2021-07-12 13:46:28,826 [main] INFO client.SCMCertificateClient: Creating keypair for client as keypair and certificate not found.
scm2.org_1   | 2021-07-12 13:46:30,258 [main] INFO ha.HASecurityUtils: Init response: GETCERT
scm2.org_1   | 2021-07-12 13:46:30,302 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.25.0.117,host:scm2.org
scm2.org_1   | 2021-07-12 13:46:30,302 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
scm2.org_1   | 2021-07-12 13:46:30,306 [main] INFO ha.HASecurityUtils: Creating csr for SCM->hostName:scm2.org,scmId:de93e951-704c-492b-918a-7b9e03404729,clusterId:CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd,subject:scm-sub@scm2.org
scm2.org_1   | 2021-07-12 13:46:32,259 [main] INFO ha.HASecurityUtils: Successfully stored SCM signed certificate.
scm2.org_1   | 2021-07-12 13:46:32,335 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-63d3b4db-26eb-4acd-b27c-ee404ddd1bcd, SCMID de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:32,336 [main] INFO server.StorageContainerManager: Primary SCM Node ID 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm2.org_1   | 2021-07-12 13:46:32,369 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm2.org/172.25.0.117
scm2.org_1   | ************************************************************/
scm2.org_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2.org_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2.org_1   | 2021-07-12 13:46:34,123 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2.org_1   | /************************************************************
scm2.org_1   | STARTUP_MSG: Starting StorageContainerManager
scm2.org_1   | STARTUP_MSG:   host = scm2.org/172.25.0.117
scm2.org_1   | STARTUP_MSG:   args = []
scm2.org_1   | STARTUP_MSG:   version = 1.2.0-SNAPSHOT
scm2.org_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0-a398b19-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.1.0-ff8aa66-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar
scm2.org_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/de23d45d4457280600c34bc04dbf00f927bb3557 ; compiled by 'runner' on 2021-07-12T12:44Z
scm2.org_1   | STARTUP_MSG:   java = 11.0.10
scm2.org_1   | ************************************************************/
scm2.org_1   | 2021-07-12 13:46:34,153 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2.org_1   | 2021-07-12 13:46:34,319 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
datanode1_1  | 2021-07-12 13:52:45,021 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:48,089 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:51,165 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:54,233 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:52:57,305 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:00,377 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:03,449 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:09,593 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:12,665 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:15,737 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:18,809 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:21,885 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:24,953 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:28,024 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:31,097 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:34,168 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:37,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:40,312 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:43,385 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:46,461 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:49,529 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:52,601 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:53:58,745 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:01,817 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:04,889 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:07,961 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:11,033 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567972107190306
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:50,353 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-16724/19004/multipartKey3
om2_1        | 2021-07-12 13:56:50,355 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om2_1        | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3 because parts are in Invalid order.
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:411)
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:180)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode3_1  | 2021-07-12 13:50:18,267 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:21,336 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:24,408 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:27,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:30,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:36,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:39,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:42,841 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:45,913 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:48,989 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:52,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:55,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:50:58,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:01,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:04,344 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:07,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:10,493 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:13,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:16,633 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:19,705 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:25,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:28,921 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:31,992 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:35,065 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:38,137 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:41,213 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:44,281 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:19,617 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3552,entriesCount=1,lastEntry=(t:2, i:70)
datanode2_1  | 2021-07-12 13:58:19,670 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3556,entriesCount=1,lastEntry=(t:2, i:71)
datanode2_1  | 2021-07-12 13:58:19,702 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3560,entriesCount=1,lastEntry=(t:2, i:72)
datanode2_1  | 2021-07-12 13:58:19,735 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3564,entriesCount=1,lastEntry=(t:2, i:73)
datanode2_1  | 2021-07-12 13:58:19,881 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3578,entriesCount=1,lastEntry=(t:2, i:74)
datanode2_1  | 2021-07-12 13:58:19,892 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3579,entriesCount=1,lastEntry=(t:2, i:75)
datanode2_1  | 2021-07-12 13:58:19,894 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3580,entriesCount=1,lastEntry=(t:2, i:76)
datanode2_1  | 2021-07-12 13:58:21,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:24,793 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:27,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:28,211 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3832,entriesCount=1,lastEntry=(t:2, i:77)
datanode2_1  | 2021-07-12 13:58:28,297 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3833,entriesCount=1,lastEntry=(t:2, i:78)
datanode2_1  | 2021-07-12 13:58:28,344 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3834,entriesCount=1,lastEntry=(t:2, i:79)
datanode2_1  | 2021-07-12 13:58:28,377 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3835,entriesCount=1,lastEntry=(t:2, i:80)
datanode2_1  | 2021-07-12 13:58:28,410 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3836,entriesCount=1,lastEntry=(t:2, i:81)
datanode2_1  | 2021-07-12 13:58:28,487 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3844,entriesCount=1,lastEntry=(t:2, i:82)
datanode2_1  | 2021-07-12 13:58:28,886 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3885,entriesCount=1,lastEntry=(t:2, i:83)
datanode2_1  | 2021-07-12 13:58:30,937 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:34,008 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:34,041 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4136,entriesCount=1,lastEntry=(t:2, i:84)
datanode2_1  | 2021-07-12 13:58:34,118 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4137,entriesCount=1,lastEntry=(t:2, i:85)
datanode2_1  | 2021-07-12 13:58:34,175 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4138,entriesCount=1,lastEntry=(t:2, i:86)
datanode2_1  | 2021-07-12 13:58:34,175 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4139,entriesCount=1,lastEntry=(t:2, i:87)
scm3.org_1   | 2021-07-12 13:47:00,266 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar!/network-topology-default.xml]
scm3.org_1   | 2021-07-12 13:47:00,266 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3.org_1   | 2021-07-12 13:47:00,561 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:47:00,723 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3.org_1   | 2021-07-12 13:47:00,863 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm3.org_1   | 2021-07-12 13:47:00,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2021-07-12 13:47:00,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm3.org_1   | 2021-07-12 13:47:00,869 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2021-07-12 13:47:00,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3.org_1   | 2021-07-12 13:47:00,871 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3.org_1   | 2021-07-12 13:47:00,877 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2021-07-12 13:47:00,878 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3.org_1   | 2021-07-12 13:47:00,879 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3.org_1   | 2021-07-12 13:47:02,038 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3.org_1   | 2021-07-12 13:47:02,040 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2021-07-12 13:47:02,044 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2021-07-12 13:47:02,094 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2021-07-12 13:47:02,100 [main] INFO server.RaftServer: cae79eb6-d521-4d9d-a048-244f8c0679d9: addNew group-EE404DDD1BCD:[] returns group-EE404DDD1BCD:java.util.concurrent.CompletableFuture@6c07add6[Not completed]
scm3.org_1   | 2021-07-12 13:47:02,152 [pool-13-thread-1] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9: new RaftServerImpl for group-EE404DDD1BCD:[] with SCMStateMachine:uninitialized
scm3.org_1   | 2021-07-12 13:47:02,154 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3.org_1   | 2021-07-12 13:47:02,154 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3.org_1   | 2021-07-12 13:47:02,154 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3.org_1   | 2021-07-12 13:47:02,158 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3.org_1   | 2021-07-12 13:47:02,158 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3.org_1   | 2021-07-12 13:47:02,159 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3.org_1   | 2021-07-12 13:47:02,159 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3.org_1   | 2021-07-12 13:47:02,164 [pool-13-thread-1] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm3.org_1   | 2021-07-12 13:47:02,168 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3.org_1   | 2021-07-12 13:47:02,172 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3.org_1   | 2021-07-12 13:47:02,180 [pool-13-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd does not exist. Creating ...
scm3.org_1   | 2021-07-12 13:47:02,205 [pool-13-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/in_use.lock acquired by nodename 7@scm3.org
scm3.org_1   | 2021-07-12 13:47:02,240 [pool-13-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd has been successfully formatted.
scm3.org_1   | 2021-07-12 13:47:02,244 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3.org_1   | 2021-07-12 13:47:02,246 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3.org_1   | 2021-07-12 13:47:02,265 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3.org_1   | 2021-07-12 13:47:02,269 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3.org_1   | 2021-07-12 13:47:02,289 [pool-13-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3.org_1   | 2021-07-12 13:47:02,290 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD
scm3.org_1   | 2021-07-12 13:47:02,291 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm3.org_1   | 2021-07-12 13:47:02,293 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm3.org_1   | 2021-07-12 13:47:02,300 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3.org_1   | 2021-07-12 13:47:02,304 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3.org_1   | 2021-07-12 13:47:02,309 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: new cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd
scm3.org_1   | 2021-07-12 13:47:02,310 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3.org_1   | 2021-07-12 13:47:02,311 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3.org_1   | 2021-07-12 13:47:02,312 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm3.org_1   | 2021-07-12 13:47:02,313 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm3.org_1   | 2021-07-12 13:47:02,314 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3.org_1   | 2021-07-12 13:47:02,322 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3.org_1   | 2021-07-12 13:47:02,322 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3.org_1   | 2021-07-12 13:47:02,323 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3.org_1   | 2021-07-12 13:47:02,334 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3.org_1   | 2021-07-12 13:47:02,334 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1        | 2021-07-12 13:55:52,578 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-77473 in volume:s3v
om1_1        | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:198)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:55:53,130 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:55:53,131 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36334
om1_1        | 2021-07-12 13:55:53,145 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:55:57,459 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49614
om1_1        | 2021-07-12 13:55:57,478 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:00,205 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:00,206 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36364
om1_1        | 2021-07-12 13:56:00,223 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:00,749 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:00,749 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36370
om1_1        | 2021-07-12 13:56:00,762 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:01,324 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:01,327 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36382
om1_1        | 2021-07-12 13:56:01,343 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:01,872 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:01,872 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36388
om1_1        | 2021-07-12 13:56:01,881 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:01,900 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om1_1        | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket not exists
om1_1        | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:119)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:53,689 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName 50306/multipartKey5 in VolumeName/Bucket s3v/bucket-16724
om2_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-16724key: 50306/multipartKey5
om2_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:148)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:56:54,258 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-16724, Key75644/multipartKey. Exception:{}
om2_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om2_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:708)
om2_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:600)
om2_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:577)
om2_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:278)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1        | 2021-07-12 13:58:35,571 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-97294, Key:27526/multidelete/f4.
om2_1        | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om2_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:133)
om2_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om2_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om2_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:05,740 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49660
om1_1        | 2021-07-12 13:56:05,761 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:08,531 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:08,532 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36418
om1_1        | 2021-07-12 13:56:08,545 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:09,068 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:09,068 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36422
om1_1        | 2021-07-12 13:56:09,070 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:09,572 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:09,572 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36426
om1_1        | 2021-07-12 13:56:09,584 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:13,361 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49706
om1_1        | 2021-07-12 13:56:13,377 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:16,212 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:16,213 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36474
om1_1        | 2021-07-12 13:56:16,217 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:16,748 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:16,749 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36480
om1_1        | 2021-07-12 13:56:16,762 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:20,598 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:49758
om1_1        | 2021-07-12 13:56:20,610 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:23,329 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:23,330 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36508
om1_1        | 2021-07-12 13:56:23,338 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:23,877 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:23,878 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36514
om1_1        | 2021-07-12 13:56:23,881 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:24,526 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:24,527 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36518
om1_1        | 2021-07-12 13:56:24,533 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:25,163 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:25,163 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36524
om1_1        | 2021-07-12 13:56:25,166 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:26,285 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:26,285 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36542
om1_1        | 2021-07-12 13:56:26,288 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:29,656 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:29,657 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36554
om1_1        | 2021-07-12 13:56:29,658 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:30,392 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:30,392 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36570
datanode2_1  | 2021-07-12 13:58:34,343 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4140,entriesCount=1,lastEntry=(t:2, i:88)
datanode2_1  | 2021-07-12 13:58:34,390 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4144,entriesCount=1,lastEntry=(t:2, i:89)
datanode2_1  | 2021-07-12 13:58:37,082 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:37,664 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4393,entriesCount=1,lastEntry=(t:2, i:90)
datanode2_1  | 2021-07-12 13:58:37,665 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4394,entriesCount=1,lastEntry=(t:2, i:91)
datanode2_1  | 2021-07-12 13:58:37,676 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4395,entriesCount=1,lastEntry=(t:2, i:92)
datanode2_1  | 2021-07-12 13:58:37,681 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4396,entriesCount=1,lastEntry=(t:2, i:93)
datanode2_1  | 2021-07-12 13:58:40,153 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:43,225 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:46,300 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:52,441 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:55,513 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:58:56,682 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4652,entriesCount=1,lastEntry=(t:2, i:94)
datanode2_1  | 2021-07-12 13:58:56,690 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4653,entriesCount=1,lastEntry=(t:2, i:95)
datanode2_1  | 2021-07-12 13:58:56,693 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4654,entriesCount=1,lastEntry=(t:2, i:96)
datanode2_1  | 2021-07-12 13:58:56,698 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4655,entriesCount=1,lastEntry=(t:2, i:97)
datanode2_1  | 2021-07-12 13:58:58,585 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:01,656 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:04,729 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:07,801 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:10,872 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:13,945 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:15,738 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4912,entriesCount=1,lastEntry=(t:2, i:98)
datanode2_1  | 2021-07-12 13:59:15,745 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4913,entriesCount=1,lastEntry=(t:2, i:99)
datanode2_1  | 2021-07-12 13:59:15,751 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4914,entriesCount=1,lastEntry=(t:2, i:100)
datanode3_1  | 2021-07-12 13:51:47,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:50,425 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:53,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:56,570 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:51:59,640 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:02,717 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:05,784 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:08,861 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:15,000 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:18,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:21,145 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:24,221 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:27,289 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:30,365 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:33,432 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:36,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:39,577 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:42,653 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:45,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:48,793 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:51,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:54,941 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:52:58,009 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:04,153 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:07,225 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:10,297 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:13,369 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:16,441 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:19,513 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:22,587 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:25,657 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:28,728 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:31,801 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:34,872 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:37,946 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:41,017 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:44,089 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:47,161 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:53,304 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:56,377 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:53:59,448 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:02,521 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:05,593 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:08,665 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:11,737 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:14,809 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:17,881 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:20,952 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:24,024 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:27,097 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:30,169 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:33,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:36,313 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:42,461 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:45,531 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:09,592 [qtp1771421544-21] ERROR endpoint.BucketEndpoint: Exception occurred in headBucket
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
scm3.org_1   | 2021-07-12 13:47:02,345 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2021-07-12 13:47:02,348 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3.org_1   | 2021-07-12 13:47:02,353 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3.org_1   | 2021-07-12 13:47:02,359 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3.org_1   | 2021-07-12 13:47:02,359 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3.org_1   | 2021-07-12 13:47:02,360 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3.org_1   | 2021-07-12 13:47:02,363 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3.org_1   | 2021-07-12 13:47:02,368 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3.org_1   | 2021-07-12 13:47:02,401 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD
scm3.org_1   | 2021-07-12 13:47:02,408 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm3.org_1   | 2021-07-12 13:47:02,411 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD
scm3.org_1   | 2021-07-12 13:47:02,412 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm3.org_1   | 2021-07-12 13:47:02,427 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3.org_1   | 2021-07-12 13:47:02,433 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3.org_1   | 2021-07-12 13:47:02,435 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3.org_1   | 2021-07-12 13:47:03,050 [main] INFO ha.SequenceIdGenerator: upgrade localId to 107544261427200000
scm3.org_1   | 2021-07-12 13:47:03,051 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3.org_1   | 2021-07-12 13:47:03,056 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3.org_1   | 2021-07-12 13:47:03,058 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3.org_1   | 2021-07-12 13:47:03,155 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3.org_1   | 2021-07-12 13:47:03,182 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3.org_1   | 2021-07-12 13:47:03,204 [main] INFO pipeline.PipelineStateManager: No pipeline exists in current db
scm3.org_1   | 2021-07-12 13:47:03,286 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3.org_1   | 2021-07-12 13:47:03,305 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3.org_1   | 2021-07-12 13:47:03,305 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3.org_1   | 2021-07-12 13:47:03,372 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3.org_1   | 2021-07-12 13:47:03,454 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3.org_1   | 2021-07-12 13:47:03,465 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3.org_1   | 2021-07-12 13:47:03,471 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2021-07-12 13:47:03,494 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 0 containers.
scm3.org_1   | 2021-07-12 13:47:03,497 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3.org_1   | 2021-07-12 13:47:03,505 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:03,516 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2021-07-12 13:47:03,588 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm3.org_1   | 2021-07-12 13:47:03,660 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2021-07-12 13:47:03,740 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm3.org_1   | 2021-07-12 13:47:05,539 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2021-07-12 13:47:05,552 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3.org_1   | 2021-07-12 13:47:05,600 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2021-07-12 13:47:05,605 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3.org_1   | 2021-07-12 13:47:05,700 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3.org_1   | 2021-07-12 13:47:05,714 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3.org_1   | 2021-07-12 13:47:06,076 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3.org_1   | Container Balancer status:
scm3.org_1   | Key                            Value
scm3.org_1   | Running                        true
scm3.org_1   | Container Balancer Configuration values:
scm3.org_1   | Key                            Value
scm3.org_1   | Threshold                      0.1
scm3.org_1   | Max Datanodes to Balance       5
scm3.org_1   | Max Size to Move               10737418240B
scm3.org_1   | 
scm3.org_1   | 2021-07-12 13:47:06,098 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3.org_1   | 2021-07-12 13:47:06,098 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3.org_1   | 2021-07-12 13:47:06,118 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3.org_1   | 2021-07-12 13:47:06,120 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3.org_1   | 2021-07-12 13:47:06,121 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: start with initializing state, conf=-1: [], old=null
scm2.org_1   | 2021-07-12 13:46:34,325 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2.org_1   | 2021-07-12 13:46:34,488 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2.org:9894 and Ratis port: 9894
scm2.org_1   | 2021-07-12 13:46:34,489 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2.org
scm2.org_1   | 2021-07-12 13:46:34,573 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2021-07-12 13:46:34,653 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm2.org_1   | 2021-07-12 13:46:35,194 [main] INFO reflections.Reflections: Reflections took 271 ms to scan 3 urls, producing 103 keys and 210 values 
scm2.org_1   | 2021-07-12 13:46:36,315 [main] INFO client.SCMCertificateClient: Loading certificate from location:/data/metadata/scm/sub-ca/certs.
scm2.org_1   | 2021-07-12 13:46:36,626 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/CA-1.crt.
scm2.org_1   | 2021-07-12 13:46:36,633 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/3142262501708.crt.
scm2.org_1   | 2021-07-12 13:46:36,636 [main] INFO client.SCMCertificateClient: Added certificate from file:/data/metadata/scm/sub-ca/certs/certificate.crt.
scm2.org_1   | 2021-07-12 13:46:36,920 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file scm.keytab. Keytab auto renewal enabled : false
scm2.org_1   | 2021-07-12 13:46:36,920 [main] INFO server.StorageContainerManager: SCM login successful.
scm2.org_1   | 2021-07-12 13:46:36,971 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2021-07-12 13:46:37,357 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2.org_1   | 2021-07-12 13:46:37,773 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.0-SNAPSHOT.jar!/network-topology-default.xml]
scm2.org_1   | 2021-07-12 13:46:37,780 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2.org_1   | 2021-07-12 13:46:38,018 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:38,211 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2.org_1   | 2021-07-12 13:46:38,415 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
scm2.org_1   | 2021-07-12 13:46:38,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2021-07-12 13:46:38,423 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
scm2.org_1   | 2021-07-12 13:46:38,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2021-07-12 13:46:38,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2.org_1   | 2021-07-12 13:46:38,431 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2.org_1   | 2021-07-12 13:46:38,434 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2021-07-12 13:46:38,434 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2.org_1   | 2021-07-12 13:46:38,437 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2.org_1   | 2021-07-12 13:46:39,655 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2.org_1   | 2021-07-12 13:46:39,692 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2021-07-12 13:46:39,692 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2021-07-12 13:46:39,728 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2021-07-12 13:46:39,739 [main] INFO server.RaftServer: de93e951-704c-492b-918a-7b9e03404729: addNew group-EE404DDD1BCD:[] returns group-EE404DDD1BCD:java.util.concurrent.CompletableFuture@6c07add6[Not completed]
scm2.org_1   | 2021-07-12 13:46:39,797 [pool-13-thread-1] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729: new RaftServerImpl for group-EE404DDD1BCD:[] with SCMStateMachine:uninitialized
scm1.org_1   | 2021-07-12 13:46:24,862 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1.org_1   | 2021-07-12 13:46:24,863 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1.org_1   | 2021-07-12 13:46:24,868 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1.org_1   | 2021-07-12 13:46:24,868 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1.org_1   | 2021-07-12 13:46:24,869 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1.org_1   | 2021-07-12 13:46:24,880 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO impl.RoleInfo: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: start 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl
scm1.org_1   | 2021-07-12 13:46:24,888 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1.org_1   | 2021-07-12 13:46:24,892 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_0-0
scm1.org_1   | 2021-07-12 13:46:24,894 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderElection1] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:24,904 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1
scm1.org_1   | 2021-07-12 13:46:24,909 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1.org_1   | 2021-07-12 13:46:24,909 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1.org_1   | 2021-07-12 13:46:24,910 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:46:24,911 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1.org_1   | 2021-07-12 13:46:24,912 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1.org_1   | 2021-07-12 13:46:24,912 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm1.org_1   | 2021-07-12 13:46:24,926 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1.org_1   | 2021-07-12 13:46:24,926 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1.org_1   | 2021-07-12 13:46:30,608 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:48994
scm1.org_1   | 2021-07-12 13:46:30,621 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:46:30,828 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm2.org, nodeId: de93e951-704c-492b-918a-7b9e03404729
scm1.org_1   | 2021-07-12 13:46:31,290 [IPC Server handler 0 on default port 9961] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C07DB0C44B61->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:31,290 [IPC Server handler 0 on default port 9961] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:32,038 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:32,039 [grpc-default-executor-0] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:32,114 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:46:32,115 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1.org_1   | 2021-07-12 13:46:32,115 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1.org_1   | 2021-07-12 13:46:32,134 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 1011082.12us
scm1.org_1   | 2021-07-12 13:46:42,242 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39160
scm1.org_1   | 2021-07-12 13:46:42,282 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:46:43,503 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:45860
scm1.org_1   | 2021-07-12 13:46:43,533 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:46:43,535 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Submitting SetConfiguration request to Ratis server with new SCM peers list: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:46:43,537 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: receive setConfiguration SetConfigurationRequest:client-BF3917BA1410->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD, cid=0, seq=0, RW, null, peers:[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:46:43,537 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-BF3917BA1410->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD, cid=0, seq=0, RW, null, peers:[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:46:43,556 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2021-07-12 13:46:43,556 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:43,557 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:46:44,008 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2021-07-12 13:46:44,009 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2021-07-12 13:46:44,009 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2021-07-12 13:46:44,010 [IPC Server handler 0 on default port 9863] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD
scm1.org_1   | 2021-07-12 13:46:44,012 [IPC Server handler 0 on default port 9863] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:46,131 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 5: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2021-07-12 13:46:46,214 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 7: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:46:46,248 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: de93e951-704c-492b-918a-7b9e03404729.
scm1.org_1   | 2021-07-12 13:46:48,357 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:49266
scm1.org_1   | 2021-07-12 13:46:48,388 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:46:49,994 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:48424
scm1.org_1   | 2021-07-12 13:46:50,049 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:46:53,993 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38138
scm1.org_1   | 2021-07-12 13:46:54,011 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:46:54,016 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for scm scm3.org, nodeId: cae79eb6-d521-4d9d-a048-244f8c0679d9
scm1.org_1   | 2021-07-12 13:46:54,094 [IPC Server handler 0 on default port 9961] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B726BBBCB64E->de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:39,808 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2.org_1   | 2021-07-12 13:46:39,809 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2.org_1   | 2021-07-12 13:46:39,809 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2.org_1   | 2021-07-12 13:46:39,809 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2.org_1   | 2021-07-12 13:46:39,809 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2.org_1   | 2021-07-12 13:46:39,810 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2.org_1   | 2021-07-12 13:46:39,810 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2.org_1   | 2021-07-12 13:46:39,815 [pool-13-thread-1] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: ConfigurationManager, init=-1: [], old=null, confs=<EMPTY_MAP>
scm2.org_1   | 2021-07-12 13:46:39,823 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2.org_1   | 2021-07-12 13:46:39,828 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2.org_1   | 2021-07-12 13:46:39,837 [pool-13-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd does not exist. Creating ...
scm2.org_1   | 2021-07-12 13:46:39,851 [pool-13-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/in_use.lock acquired by nodename 7@scm2.org
scm2.org_1   | 2021-07-12 13:46:39,889 [pool-13-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd has been successfully formatted.
scm2.org_1   | 2021-07-12 13:46:39,895 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2.org_1   | 2021-07-12 13:46:39,897 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2.org_1   | 2021-07-12 13:46:39,916 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2.org_1   | 2021-07-12 13:46:39,916 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2.org_1   | 2021-07-12 13:46:39,923 [pool-13-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2.org_1   | 2021-07-12 13:46:39,934 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD
scm2.org_1   | 2021-07-12 13:46:39,935 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm2.org_1   | 2021-07-12 13:46:39,937 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm2.org_1   | 2021-07-12 13:46:39,950 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2.org_1   | 2021-07-12 13:46:39,951 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2.org_1   | 2021-07-12 13:46:39,963 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: new de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd
scm2.org_1   | 2021-07-12 13:46:39,963 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2.org_1   | 2021-07-12 13:46:39,963 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2.org_1   | 2021-07-12 13:46:39,964 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
scm2.org_1   | 2021-07-12 13:46:39,964 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2.org_1   | 2021-07-12 13:46:39,965 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2.org_1   | 2021-07-12 13:46:39,967 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2.org_1   | 2021-07-12 13:46:39,968 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2.org_1   | 2021-07-12 13:46:39,968 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2.org_1   | 2021-07-12 13:46:39,979 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2.org_1   | 2021-07-12 13:46:39,980 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2.org_1   | 2021-07-12 13:46:39,988 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2021-07-12 13:46:39,988 [pool-13-thread-1] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2.org_1   | 2021-07-12 13:46:39,995 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2.org_1   | 2021-07-12 13:46:39,999 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2.org_1   | 2021-07-12 13:46:39,999 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2.org_1   | 2021-07-12 13:46:40,000 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2.org_1   | 2021-07-12 13:46:40,001 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2.org_1   | 2021-07-12 13:46:40,001 [pool-13-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2.org_1   | 2021-07-12 13:46:40,040 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD
scm2.org_1   | 2021-07-12 13:46:40,045 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm2.org_1   | 2021-07-12 13:46:40,052 [pool-13-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD
scm2.org_1   | 2021-07-12 13:46:40,052 [pool-13-thread-1] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm2.org_1   | 2021-07-12 13:46:40,066 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2.org_1   | 2021-07-12 13:46:40,071 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2.org_1   | 2021-07-12 13:46:40,077 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2.org_1   | 2021-07-12 13:46:40,608 [main] INFO ha.SequenceIdGenerator: upgrade localId to 107544261427200000
scm1.org_1   | 2021-07-12 13:46:54,094 [IPC Server handler 0 on default port 9961] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:54,725 [java.util.concurrent.ThreadPoolExecutor$Worker@188b5bf3[State = -1, empty queue]] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B726BBBCB64E->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9
scm1.org_1   | 2021-07-12 13:46:54,726 [java.util.concurrent.ThreadPoolExecutor$Worker@188b5bf3[State = -1, empty queue]] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:46:55,025 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39348
scm1.org_1   | 2021-07-12 13:46:55,045 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:46:55,094 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:46:55,113 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 1020330.88us
scm1.org_1   | 2021-07-12 13:47:06,651 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:48660
scm1.org_1   | 2021-07-12 13:47:06,681 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:47:06,682 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Submitting SetConfiguration request to Ratis server with new SCM peers list: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:47:06,682 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: receive setConfiguration SetConfigurationRequest:client-BF3917BA1410->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD, cid=1, seq=0, RW, null, peers:[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:47:06,684 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-BF3917BA1410->1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD, cid=1, seq=0, RW, null, peers:[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|priority:0]
scm1.org_1   | 2021-07-12 13:47:06,684 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1.org_1   | 2021-07-12 13:47:06,685 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:47:06,685 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 33554432 (custom)
scm1.org_1   | 2021-07-12 13:47:06,719 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39512
scm1.org_1   | 2021-07-12 13:47:07,015 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1.org_1   | 2021-07-12 13:47:07,015 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1.org_1   | 2021-07-12 13:47:07,015 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1.org_1   | 2021-07-12 13:47:07,331 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:47:10,738 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 11: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm1.org_1   | 2021-07-12 13:47:10,795 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-LeaderStateImpl] INFO server.RaftServer$Division: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD: set configuration 13: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm1.org_1   | 2021-07-12 13:47:10,869 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: cae79eb6-d521-4d9d-a048-244f8c0679d9.
scm1.org_1   | 2021-07-12 13:47:18,650 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.118:38358
scm1.org_1   | 2021-07-12 13:47:18,705 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for scm/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:20,749 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:20,759 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 215639.471us
scm1.org_1   | 2021-07-12 13:47:34,339 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:51126
scm1.org_1   | 2021-07-12 13:47:34,724 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:47:34,808 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:54542
scm1.org_1   | 2021-07-12 13:47:34,829 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:47518
scm1.org_1   | 2021-07-12 13:47:34,916 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:47:35,057 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:47:35,095 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33948
scm1.org_1   | 2021-07-12 13:47:35,192 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:35,208 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn abb35404295f, UUID: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 13:47:35,572 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:35,614 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 120692.936us
scm1.org_1   | 2021-07-12 13:47:37,289 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:41660
scm1.org_1   | 2021-07-12 13:47:37,387 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:37,399 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn f6c2a59662b6, UUID: e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:47:37,782 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:37,793 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 69342.52us
scm1.org_1   | 2021-07-12 13:47:38,962 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38386
scm1.org_1   | 2021-07-12 13:47:39,104 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:39,108 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for dn 27bc097164fa, UUID: 5b90731c-5249-453a-9f10-ddd353e45997
scm1.org_1   | 2021-07-12 13:47:39,420 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:39,429 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 105427.531us
scm1.org_1   | 2021-07-12 13:47:44,621 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:53548
scm1.org_1   | 2021-07-12 13:47:44,660 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:44,677 [IPC Server handler 1 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om1, UUID: a968cfac-e8ee-4930-b9fe-9e179013deda
scm1.org_1   | 2021-07-12 13:47:45,207 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:45,220 [IPC Server handler 1 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 232242.566us
scm1.org_1   | 2021-07-12 13:47:45,337 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:44718
scm1.org_1   | 2021-07-12 13:47:45,385 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:45,396 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om2, UUID: 98238d61-ef8a-468b-8446-a6867776e769
scm1.org_1   | 2021-07-12 13:47:45,641 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:45,703 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 132220.837us
scm1.org_1   | 2021-07-12 13:47:47,544 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33976
scm1.org_1   | 2021-07-12 13:47:47,582 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:48,309 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39628
scm2.org_1   | 2021-07-12 13:46:40,608 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2.org_1   | 2021-07-12 13:46:40,620 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2.org_1   | 2021-07-12 13:46:40,626 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2.org_1   | 2021-07-12 13:46:40,768 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2.org_1   | 2021-07-12 13:46:40,796 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2.org_1   | 2021-07-12 13:46:40,821 [main] INFO pipeline.PipelineStateManager: No pipeline exists in current db
scm2.org_1   | 2021-07-12 13:46:40,901 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2.org_1   | 2021-07-12 13:46:40,913 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2.org_1   | 2021-07-12 13:46:40,913 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2.org_1   | 2021-07-12 13:46:41,022 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2.org_1   | 2021-07-12 13:46:41,082 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2.org_1   | 2021-07-12 13:46:41,100 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2.org_1   | 2021-07-12 13:46:41,112 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2021-07-12 13:46:41,137 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 0 containers.
scm2.org_1   | 2021-07-12 13:46:41,138 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2.org_1   | 2021-07-12 13:46:41,148 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:46:41,151 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2021-07-12 13:46:41,209 [main] INFO authority.DefaultCAServer: CertificateServer validation is successful
scm2.org_1   | 2021-07-12 13:46:41,300 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2021-07-12 13:46:41,379 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm2.org_1   | 2021-07-12 13:46:42,925 [Listener at 0.0.0.0/9961] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2021-07-12 13:46:42,931 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2.org_1   | 2021-07-12 13:46:42,955 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2021-07-12 13:46:42,956 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2.org_1   | 2021-07-12 13:46:42,979 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2.org_1   | 2021-07-12 13:46:42,984 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2.org_1   | 2021-07-12 13:46:43,134 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2.org_1   | Container Balancer status:
scm2.org_1   | Key                            Value
scm2.org_1   | Running                        true
scm2.org_1   | Container Balancer Configuration values:
scm2.org_1   | Key                            Value
scm2.org_1   | Threshold                      0.1
scm2.org_1   | Max Datanodes to Balance       5
scm2.org_1   | Max Size to Move               10737418240B
scm2.org_1   | 
scm2.org_1   | 2021-07-12 13:46:43,134 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2.org_1   | 2021-07-12 13:46:43,134 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2.org_1   | 2021-07-12 13:46:43,145 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2.org_1   | 2021-07-12 13:46:43,148 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2.org_1   | 2021-07-12 13:46:43,149 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: start with initializing state, conf=-1: [], old=null
scm2.org_1   | 2021-07-12 13:46:43,150 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2.org_1   | 2021-07-12 13:46:43,152 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EE404DDD1BCD,id=de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:43,153 [Listener at 0.0.0.0/9860] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD
scm2.org_1   | 2021-07-12 13:46:43,153 [Listener at 0.0.0.0/9860] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm2.org_1   | 2021-07-12 13:46:43,162 [Listener at 0.0.0.0/9860] INFO server.RaftServer: de93e951-704c-492b-918a-7b9e03404729: start RPC server
scm2.org_1   | 2021-07-12 13:46:43,236 [Listener at 0.0.0.0/9860] INFO server.GrpcService: de93e951-704c-492b-918a-7b9e03404729: GrpcService started, listening on 9894
scm2.org_1   | 2021-07-12 13:46:43,243 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$402/0x0000000840502440@18d63996] INFO util.JvmPauseMonitor: JvmPauseMonitor-de93e951-704c-492b-918a-7b9e03404729: Started
scm2.org_1   | 2021-07-12 13:46:43,267 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:43,267 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:43,267 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:45,155 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:45,156 [grpc-default-executor-0] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 13:47:48,459 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:33736
scm1.org_1   | 2021-07-12 13:47:48,472 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:48,473 [IPC Server handler 0 on default port 9961] INFO server.SCMSecurityProtocolServer: Processing CSR for om om3, UUID: 76f38467-fe7b-4392-b87f-e5323e21aae1
scm1.org_1   | 2021-07-12 13:47:48,524 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:47:48,697 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:47:48,713 [IPC Server handler 0 on default port 9961] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.security.x509.certificate.authority.CertificateStore.storeValidCertificate(java.math.BigInteger,java.security.cert.X509Certificate,org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 81478.222us
scm1.org_1   | 2021-07-12 13:47:49,485 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:41692
scm1.org_1   | 2021-07-12 13:47:49,528 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:47:50,755 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38420
scm1.org_1   | 2021-07-12 13:47:50,792 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:48:05,383 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37524
scm1.org_1   | 2021-07-12 13:48:05,429 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:05,971 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$421/0x0000000840523c40@5b01b3f8] WARN util.JvmPauseMonitor: JvmPauseMonitor-1e6f42a1-e2d2-4f78-bf5d-162da394b7e9: Detected pause in JVM or host machine (eg GC): pause of approximately 103322252ns.
scm1.org_1   | GC pool 'ParNew' had collection(s): count=1 time=137ms
scm1.org_1   | 2021-07-12 13:48:09,467 [IPC Server handler 18 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 13:48:09,616 [IPC Server handler 18 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3206477232763, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2021-07-12 13:48:09,646 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54814
scm1.org_1   | 2021-07-12 13:48:09,660 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:09,762 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2021-07-12 13:48:09,815 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ff1246ee-6927-4ea3-b470-c4d54606d42f to datanode:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 13:48:10,013 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1.org_1   | 2021-07-12 13:48:10,483 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]].
scm1.org_1   | 2021-07-12 13:48:10,501 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:10,511 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 163370.14us
scm1.org_1   | 2021-07-12 13:48:10,906 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49464
scm1.org_1   | 2021-07-12 13:48:10,920 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:11,977 [IPC Server handler 18 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:48:11,980 [IPC Server handler 18 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3208765789747, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2021-07-12 13:48:12,001 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2021-07-12 13:48:12,012 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1.org_1   | 2021-07-12 13:48:12,035 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7f7f3d83-0cbd-41c6-b795-89c5f88bc674 to datanode:e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:48:12,097 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]].
scm1.org_1   | 2021-07-12 13:48:12,109 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:12,138 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 96947.923us
scm1.org_1   | 2021-07-12 13:48:13,002 [IPC Server handler 18 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5b90731c-5249-453a-9f10-ddd353e45997
scm1.org_1   | 2021-07-12 13:48:13,012 [IPC Server handler 18 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3210368079521, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1.org_1   | 2021-07-12 13:48:13,017 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2021-07-12 13:48:13,030 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7ef1c9bf-43c8-43b8-b00a-7844fdf8175c to datanode:5b90731c-5249-453a-9f10-ddd353e45997
scm1.org_1   | 2021-07-12 13:48:13,094 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1.org_1   | 2021-07-12 13:48:13,096 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1.org_1   | 2021-07-12 13:48:13,098 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1.org_1   | 2021-07-12 13:48:13,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2021-07-12 13:48:13,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1.org_1   | 2021-07-12 13:48:13,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1.org_1   | 2021-07-12 13:48:13,098 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]].
scm1.org_1   | 2021-07-12 13:48:13,121 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:13,140 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 109421.926us
scm1.org_1   | 2021-07-12 13:48:13,198 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8 to datanode:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 13:48:13,206 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8 to datanode:e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:48:13,221 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8 to datanode:5b90731c-5249-453a-9f10-ddd353e45997
scm1.org_1   | 2021-07-12 13:48:13,289 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7135b98c-5758-43ad-b6de-abe0feb7c4d8, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.198Z[UTC]].
scm1.org_1   | 2021-07-12 13:48:13,297 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:13,299 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 77218.818us
scm1.org_1   | 2021-07-12 13:48:13,306 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=660497cf-d26f-4475-b297-562b256bcc94 to datanode:5b90731c-5249-453a-9f10-ddd353e45997
scm1.org_1   | 2021-07-12 13:48:13,329 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=660497cf-d26f-4475-b297-562b256bcc94 to datanode:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 13:48:13,330 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=660497cf-d26f-4475-b297-562b256bcc94 to datanode:e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:48:13,346 [RatisPipelineUtilsThread - 0] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-1_36 to index:36
scm2.org_1   | 2021-07-12 13:46:45,331 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2.org_1   | 2021-07-12 13:46:45,331 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: change Leader from null to 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9 at term 2 for appendEntries, leader elected after 5435ms
scm2.org_1   | 2021-07-12 13:46:45,334 [grpc-default-executor-0] INFO impl.RoleInfo: de93e951-704c-492b-918a-7b9e03404729: start de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-FollowerState
scm2.org_1   | 2021-07-12 13:46:45,585 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2021-07-12 13:46:45,585 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2021-07-12 13:46:45,610 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: Starting segment from index:0
scm2.org_1   | 2021-07-12 13:46:45,716 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2.org_1   | 2021-07-12 13:46:45,901 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0
scm2.org_1   | 2021-07-12 13:46:45,908 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_0-0
scm2.org_1   | 2021-07-12 13:46:45,929 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1
scm2.org_1   | 2021-07-12 13:46:45,977 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:46:45,978 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2.org_1   | 2021-07-12 13:46:45,979 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2.org_1   | 2021-07-12 13:46:45,979 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm2.org_1   | 2021-07-12 13:46:46,120 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2.org_1   | 2021-07-12 13:46:46,120 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2021-07-12 13:46:46,167 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 5: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm2.org_1   | 2021-07-12 13:46:46,219 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 7: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2021-07-12 13:46:46,362 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:46:46,362 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2.org_1   | 2021-07-12 13:46:46,366 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2.org_1   | 2021-07-12 13:46:46,377 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-EE404DDD1BCD:[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm2.org_1   | 2021-07-12 13:46:46,377 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2.org_1   | 2021-07-12 13:46:46,384 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm2.org_1   | 2021-07-12 13:46:46,384 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm2.org_1   | 2021-07-12 13:46:46,679 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2.org_1   | 2021-07-12 13:46:46,728 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2.org_1   | 2021-07-12 13:46:46,728 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2.org_1   | 2021-07-12 13:46:47,475 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2.org_1   | 2021-07-12 13:46:47,484 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2.org_1   | 2021-07-12 13:46:47,485 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2021-07-12 13:46:47,745 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2.org_1   | 2021-07-12 13:46:47,745 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2.org_1   | 2021-07-12 13:46:47,746 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2021-07-12 13:46:47,764 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2.org_1   | 2021-07-12 13:46:47,819 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm2.org_1   | 2021-07-12 13:46:47,819 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm2.org_1   | 2021-07-12 13:46:47,821 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2.org_1   | 2021-07-12 13:46:47,822 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm2.org_1   | 2021-07-12 13:46:47,822 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
datanode1_1  | 2021-07-12 13:54:14,105 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:17,177 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:20,249 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:23,321 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:26,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:29,465 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:32,537 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:35,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:38,680 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:41,753 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:47,897 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:50,969 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:54,044 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:54:57,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:00,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:03,257 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:06,328 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:09,401 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:12,473 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:15,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:18,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:21,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:24,761 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:27,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:30,905 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:37,048 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:40,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:17,017 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:20,089 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:23,160 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:26,234 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:28,148 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5167,entriesCount=1,lastEntry=(t:2, i:101)
datanode2_1  | 2021-07-12 13:59:28,159 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5168,entriesCount=1,lastEntry=(t:2, i:102)
datanode2_1  | 2021-07-12 13:59:28,176 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5170,entriesCount=1,lastEntry=(t:2, i:103)
datanode2_1  | 2021-07-12 13:59:28,192 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5173,entriesCount=1,lastEntry=(t:2, i:104)
datanode2_1  | 2021-07-12 13:59:29,304 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:31,271 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5423,entriesCount=1,lastEntry=(t:2, i:105)
datanode2_1  | 2021-07-12 13:59:31,278 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5424,entriesCount=1,lastEntry=(t:2, i:106)
datanode2_1  | 2021-07-12 13:59:31,285 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5425,entriesCount=1,lastEntry=(t:2, i:107)
datanode2_1  | 2021-07-12 13:59:31,286 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5426,entriesCount=1,lastEntry=(t:2, i:108)
datanode2_1  | 2021-07-12 13:59:32,377 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:35,449 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:41,597 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:44,664 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:47,737 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:50,809 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:53,880 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 13:59:56,953 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:00,025 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:03,099 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:06,169 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:09,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:12,313 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:15,385 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:15,549 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode2_1  | 2021-07-12 14:00:15,549 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode2_1  | 2021-07-12 14:00:15,571 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 130.
datanode2_1  | 2021-07-12 14:00:16,991 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5693,entriesCount=1,lastEntry=(t:2, i:109)
datanode2_1  | 2021-07-12 14:00:16,992 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5694,entriesCount=1,lastEntry=(t:2, i:110)
datanode2_1  | 2021-07-12 14:00:17,001 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5695,entriesCount=1,lastEntry=(t:2, i:111)
datanode2_1  | 2021-07-12 14:00:17,021 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5696,entriesCount=1,lastEntry=(t:2, i:112)
datanode2_1  | 2021-07-12 14:00:18,457 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:21,529 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:24,601 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:30,744 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:33,816 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:36,889 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:39,960 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:43,033 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:46,109 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:49,177 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:52,252 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:55,320 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:00:58,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:01,465 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:04,541 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:07,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:10,680 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:13,756 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:19,896 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:22,969 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:26,040 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:29,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:32,184 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:35,261 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:48,601 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:51,673 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:54,745 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:54:57,817 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:00,888 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:03,961 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:07,033 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:10,105 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:13,180 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:16,249 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:19,321 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:22,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:25,469 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:31,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:34,681 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:37,756 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:40,825 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:43,899 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:46,969 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:50,041 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:53,112 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:56,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:55:59,257 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:02,328 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:05,400 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:08,473 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:11,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:14,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1        | 2021-07-12 13:56:30,393 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:34,054 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:34,054 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36590
om1_1        | 2021-07-12 13:56:34,056 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:34,927 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:34,927 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36598
om1_1        | 2021-07-12 13:56:34,929 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:35,488 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:35,489 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36604
om1_1        | 2021-07-12 13:56:35,497 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:36,488 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:36,489 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36616
om1_1        | 2021-07-12 13:56:36,490 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:37,103 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:37,103 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36620
om1_1        | 2021-07-12 13:56:37,117 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:40,392 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:40,393 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36642
om1_1        | 2021-07-12 13:56:40,394 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:41,141 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:41,142 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36648
om1_1        | 2021-07-12 13:56:41,143 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:41,164 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-16724/10474/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om1_1        | 2021-07-12 13:56:41,165 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 10474/multipartKey2 in Volume/Bucket s3v/bucket-16724
om1_1        | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 10474/multipartKey2. Entity too small.
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:463)
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:41,721 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:41,721 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36654
om1_1        | 2021-07-12 13:56:41,723 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:42,366 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:42,366 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36660
om1_1        | 2021-07-12 13:56:42,370 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:42,395 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om1_1        | partName: "etag1"
om1_1        | , partNumber: 2
om1_1        | partName: "etag2"
om1_1        | ]
om1_1        | 2021-07-12 13:56:42,395 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.newError(S3ErrorTable.java:131)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getBucket(EndpointBase.java:68)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.head(BucketEndpoint.java:295)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
scm1.org_1   | 2021-07-12 13:48:13,421 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_1-36
scm1.org_1   | 2021-07-12 13:48:13,441 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_37
scm1.org_1   | 2021-07-12 13:48:13,455 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 660497cf-d26f-4475-b297-562b256bcc94, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.306Z[UTC]].
scm1.org_1   | 2021-07-12 13:48:13,472 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:13,510 [RatisPipelineUtilsThread - 0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 165511.64us
scm1.org_1   | 2021-07-12 13:48:13,537 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerV2Impl: Pipeline: PipelineID=660497cf-d26f-4475-b297-562b256bcc94 contains same datanodes as previous pipelines: PipelineID=7135b98c-5758-43ad-b6de-abe0feb7c4d8 nodeIds: 5b90731c-5249-453a-9f10-ddd353e45997, 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 13:48:14,251 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:54654
scm1.org_1   | 2021-07-12 13:48:14,337 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:48:14,902 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]] moved to OPEN state
scm1.org_1   | 2021-07-12 13:48:14,960 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:15,043 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 113012.826us
scm1.org_1   | 2021-07-12 13:48:15,117 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:15,545 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:51252
scm1.org_1   | 2021-07-12 13:48:15,666 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:48:16,060 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:16,220 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5b90731c-5249-453a-9f10-ddd353e45997, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]] moved to OPEN state
scm1.org_1   | 2021-07-12 13:48:16,269 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:16,300 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 79255.019us
scm1.org_1   | 2021-07-12 13:48:16,320 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:17,352 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:19,043 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:47648
scm1.org_1   | 2021-07-12 13:48:19,151 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:48:20,568 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:47:06,129 [Listener at 0.0.0.0/9860] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3.org_1   | 2021-07-12 13:47:06,131 [Listener at 0.0.0.0/9860] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EE404DDD1BCD,id=cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:47:06,132 [Listener at 0.0.0.0/9860] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD
scm3.org_1   | 2021-07-12 13:47:06,138 [Listener at 0.0.0.0/9860] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm3.org_1   | 2021-07-12 13:47:06,147 [Listener at 0.0.0.0/9860] INFO server.RaftServer: cae79eb6-d521-4d9d-a048-244f8c0679d9: start RPC server
scm3.org_1   | 2021-07-12 13:47:06,241 [Listener at 0.0.0.0/9860] INFO server.GrpcService: cae79eb6-d521-4d9d-a048-244f8c0679d9: GrpcService started, listening on 9894
scm3.org_1   | 2021-07-12 13:47:06,254 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$405/0x0000000840501440@18d63996] INFO util.JvmPauseMonitor: JvmPauseMonitor-cae79eb6-d521-4d9d-a048-244f8c0679d9: Started
scm3.org_1   | 2021-07-12 13:47:06,266 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm3.org_1   | 2021-07-12 13:47:06,268 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm3.org_1   | 2021-07-12 13:47:06,269 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm3.org_1   | 2021-07-12 13:47:09,855 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:47:09,856 [grpc-default-executor-0] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm3.org_1   | 2021-07-12 13:47:10,102 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3.org_1   | 2021-07-12 13:47:10,102 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: change Leader from null to 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9 at term 2 for appendEntries, leader elected after 7859ms
scm3.org_1   | 2021-07-12 13:47:10,154 [grpc-default-executor-0] INFO impl.RoleInfo: cae79eb6-d521-4d9d-a048-244f8c0679d9: start cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-FollowerState
scm3.org_1   | 2021-07-12 13:47:10,704 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 0: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2021-07-12 13:47:10,704 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 1: [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2021-07-12 13:47:10,705 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 5: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm3.org_1   | 2021-07-12 13:47:10,705 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 7: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2021-07-12 13:47:10,736 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Starting segment from index:0
scm3.org_1   | 2021-07-12 13:47:10,858 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3.org_1   | 2021-07-12 13:47:11,004 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 11: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm3.org_1   | 2021-07-12 13:47:11,065 [grpc-default-executor-0] INFO server.RaftServer$Division: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD: set configuration 13: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm3.org_1   | 2021-07-12 13:47:11,381 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-EE404DDD1BCD:[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm3.org_1   | 2021-07-12 13:47:11,395 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3.org_1   | 2021-07-12 13:47:11,397 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Starting token manager
scm3.org_1   | 2021-07-12 13:47:11,400 [Listener at 0.0.0.0/9860] INFO token.ContainerTokenSecretManager: Updating the current master key for generating tokens
scm3.org_1   | 2021-07-12 13:47:12,045 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0
scm3.org_1   | 2021-07-12 13:47:12,077 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_0 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_0-0
scm3.org_1   | 2021-07-12 13:47:12,182 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1
scm3.org_1   | 2021-07-12 13:47:12,298 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:12,321 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3.org_1   | 2021-07-12 13:47:12,343 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3.org_1   | 2021-07-12 13:47:12,348 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm3.org_1   | 2021-07-12 13:47:12,402 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3.org_1   | 2021-07-12 13:47:12,532 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3.org_1   | 2021-07-12 13:47:12,534 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3.org_1   | 2021-07-12 13:47:14,825 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3.org_1   | 2021-07-12 13:47:15,377 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2021-07-12 13:47:15,441 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3.org_1   | 2021-07-12 13:47:15,513 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2021-07-12 13:47:15,514 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3.org_1   | 2021-07-12 13:47:16,373 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3.org_1   | 2021-07-12 13:47:16,407 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3.org_1   | 2021-07-12 13:47:16,463 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2021-07-12 13:47:16,463 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3.org_1   | 2021-07-12 13:47:16,991 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm3.org_1   | 2021-07-12 13:47:16,991 [Listener at 0.0.0.0/9860] INFO server.SCMSecurityProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm3.org_1   | 2021-07-12 13:47:16,992 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3.org_1   | 2021-07-12 13:47:17,036 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm3.org_1   | 2021-07-12 13:47:17,072 [Listener at 0.0.0.0/9860] INFO server.SCMUpdateServiceGrpcServer: SCMUpdateService starting
scm3.org_1   | 2021-07-12 13:47:17,127 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:17,188 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3.org_1   | 2021-07-12 13:47:17,188 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3.org_1   | 2021-07-12 13:47:17,614 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:17,770 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm3.org_1   | 2021-07-12 13:47:17,770 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:16,221 [qtp1771421544-22] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-97773, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:56:16,235 [qtp1771421544-22] INFO endpoint.BucketEndpoint: Location is /bucket-97773
s3g_1        | 2021-07-12 13:56:23,344 [qtp1771421544-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-16724, with Versioning false and Storage Type set to DISK and Encryption set to false 
scm2.org_1   | 2021-07-12 13:46:48,053 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.client.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:48,053 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.block.client.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.block.client.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:48,060 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm2.org_1   | 2021-07-12 13:46:48,524 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:48,531 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 3115382315205 on Scm Bootstrap Node de93e951-704c-492b-918a-7b9e03404729
scm2.org_1   | 2021-07-12 13:46:48,593 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2ef89e43] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2.org_1   | 2021-07-12 13:46:48,637 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2.org_1   | 2021-07-12 13:46:48,637 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm2.org_1   | 2021-07-12 13:46:48,644 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm2.org_1   | 2021-07-12 13:46:48,713 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @16100ms to org.eclipse.jetty.util.log.Slf4jLog
scm2.org_1   | 2021-07-12 13:46:49,134 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2.org_1   | 2021-07-12 13:46:49,139 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2.org_1   | 2021-07-12 13:46:49,149 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm2.org_1   | 2021-07-12 13:46:49,149 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm2.org_1   | 2021-07-12 13:46:49,149 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm2.org_1   | 2021-07-12 13:46:49,151 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm2.org_1   | 2021-07-12 13:46:49,273 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2.org_1   | 2021-07-12 13:46:49,278 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm2.org_1   | 2021-07-12 13:46:49,363 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2.org_1   | 2021-07-12 13:46:49,370 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2.org_1   | 2021-07-12 13:46:49,372 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm2.org_1   | 2021-07-12 13:46:49,405 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2021-07-12 13:46:49,409 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@597ef918{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2.org_1   | 2021-07-12 13:46:49,409 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@45c3cf0c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2.org_1   | 2021-07-12 13:46:49,623 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm2.org_1   | 2021-07-12 13:46:49,667 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7a725c89{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_0-SNAPSHOT_jar-_-any-6985710509343260944/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/scm}
scm2.org_1   | 2021-07-12 13:46:49,702 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@56ed6e17{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2.org_1   | 2021-07-12 13:46:49,702 [Listener at 0.0.0.0/9860] INFO server.Server: Started @17090ms
scm2.org_1   | 2021-07-12 13:46:49,709 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2.org_1   | 2021-07-12 13:46:49,716 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2.org_1   | 2021-07-12 13:46:49,721 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2.org_1   | 2021-07-12 13:46:55,104 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:10,780 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 11: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=[de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0]
scm2.org_1   | 2021-07-12 13:47:10,815 [grpc-default-executor-0] INFO server.RaftServer$Division: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD: set configuration 13: [de93e951-704c-492b-918a-7b9e03404729|rpc:scm2.org:9894|admin:|client:|dataStream:|priority:0, cae79eb6-d521-4d9d-a048-244f8c0679d9|rpc:scm3.org:9894|admin:|client:|dataStream:|priority:0, 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0], old=null
scm2.org_1   | 2021-07-12 13:47:20,792 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:35,597 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:37,803 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:39,432 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:45,228 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:20,571 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:20,597 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 59366.014us
scm1.org_1   | 2021-07-12 13:48:21,713 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:25,079 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:53706
scm1.org_1   | 2021-07-12 13:48:25,099 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:48:25,904 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:44882
scm1.org_1   | 2021-07-12 13:48:25,939 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:48:27,153 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39784
scm1.org_1   | 2021-07-12 13:48:27,228 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:48:28,872 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:33904
scm1.org_1   | 2021-07-12 13:48:28,899 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:48:39,507 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39842
scm1.org_1   | 2021-07-12 13:48:39,605 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:48:41,120 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37712
scm1.org_1   | 2021-07-12 13:48:41,196 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:42,311 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]] moved to OPEN state
scm1.org_1   | 2021-07-12 13:48:42,342 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:42,344 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 29873.706us
scm1.org_1   | 2021-07-12 13:48:42,346 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:42,490 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:43,382 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55010
scm1.org_1   | 2021-07-12 13:48:43,399 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:43,402 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7135b98c-5758-43ad-b6de-abe0feb7c4d8, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:13.198Z[UTC]] moved to OPEN state
scm1.org_1   | 2021-07-12 13:48:43,415 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:43,443 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1.org_1   | 2021-07-12 13:48:43,485 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 82155.516us
scm1.org_1   | 2021-07-12 13:48:43,485 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 660497cf-d26f-4475-b297-562b256bcc94, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:13.306Z[UTC]] moved to OPEN state
scm1.org_1   | 2021-07-12 13:48:43,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:48:43,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1.org_1   | 2021-07-12 13:48:43,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1.org_1   | 2021-07-12 13:48:43,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1.org_1   | 2021-07-12 13:48:43,496 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1.org_1   | 2021-07-12 13:48:43,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1.org_1   | 2021-07-12 13:48:43,497 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1.org_1   | 2021-07-12 13:48:43,528 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 43002.008us
scm1.org_1   | 2021-07-12 13:48:44,621 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49660
scm1.org_1   | 2021-07-12 13:48:44,636 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:48:49,148 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:39924
scm1.org_1   | 2021-07-12 13:48:49,170 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:49:01,304 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55098
scm1.org_1   | 2021-07-12 13:49:01,330 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:11,116 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37866
scm1.org_1   | 2021-07-12 13:49:11,125 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:12,889 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:54970
scm1.org_1   | 2021-07-12 13:49:12,906 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:49:13,001 [IPC Server handler 90 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract java.lang.Boolean org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManager.allocateBatch(java.lang.String,java.lang.Long,java.lang.Long) on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 72672.911us
scm1.org_1   | 2021-07-12 13:49:13,002 [IPC Server handler 90 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1.org_1   | 2021-07-12 13:49:13,167 [IPC Server handler 90 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 105182.316us
scm1.org_1   | 2021-07-12 13:49:13,231 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 107544261427200000.
scm1.org_1   | 2021-07-12 13:49:13,241 [IPC Server handler 90 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract java.lang.Boolean org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManager.allocateBatch(java.lang.String,java.lang.Long,java.lang.Long) on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 55484.709us
scm1.org_1   | 2021-07-12 13:49:13,297 [IPC Server handler 90 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract java.lang.Boolean org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManager.allocateBatch(java.lang.String,java.lang.Long,java.lang.Long) on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 54477.308us
scm1.org_1   | 2021-07-12 13:49:13,300 [IPC Server handler 90 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 107544261427200000 to 107544261427201000.
scm1.org_1   | 2021-07-12 13:49:17,756 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34394
scm1.org_1   | 2021-07-12 13:49:17,773 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:49:18,283 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38832
scm1.org_1   | 2021-07-12 13:49:18,321 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:42110
datanode2_1  | 2021-07-12 14:01:38,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:41,400 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:44,472 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:47,544 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:50,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:53,689 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:56,761 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:01:59,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:02,904 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:09,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:12,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:15,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:18,265 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:21,337 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:24,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:27,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:30,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:33,625 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:36,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:39,773 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:42,840 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:45,913 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:48,985 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:52,056 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:02:58,204 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:01,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:04,345 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:07,416 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:10,488 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:12,898 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode2_1  | 2021-07-12 14:03:12,903 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode2_1  | 2021-07-12 14:03:12,918 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 114.
datanode2_1  | 2021-07-12 14:03:13,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:16,632 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:19,704 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:22,776 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:25,848 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:28,920 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:31,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:32,357 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6023,entriesCount=1,lastEntry=(t:2, i:113)
datanode2_1  | 2021-07-12 14:03:32,372 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6024,entriesCount=1,lastEntry=(t:2, i:114)
datanode2_1  | 2021-07-12 14:03:32,423 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6025,entriesCount=1,lastEntry=(t:2, i:115)
datanode2_1  | 2021-07-12 14:03:32,443 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6026,entriesCount=1,lastEntry=(t:2, i:116)
datanode2_1  | 2021-07-12 14:03:35,064 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:38,136 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:41,208 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:47,352 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:50,424 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:53,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:56,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:03:59,645 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:02,713 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:43,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:46,265 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:49,337 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:52,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:55,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:55:58,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:01,624 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:04,696 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:07,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:10,840 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:13,912 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:16,984 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:20,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:26,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:29,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:32,344 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:35,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:38,489 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:41,560 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:44,632 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:47,706 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:50,776 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:53,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:56,934 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:56:59,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:03,065 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:06,140 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:09,208 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1        | 2021-07-12 13:56:23,353 [qtp1771421544-21] INFO endpoint.BucketEndpoint: Location is /bucket-16724
s3g_1        | 2021-07-12 13:56:25,335 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E6876CFA314D->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:25,335 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:26,363 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7CA1589C4679->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:26,363 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:30,474 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2860721A043C->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:56:30,474 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:34,143 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D38AD6A72243->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:34,148 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:37,177 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-AEEB7CA0927E->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:56:37,177 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:40,448 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D68345DDF9A9->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:40,448 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:41,170 [qtp1771421544-23] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 10474/multipartKey2
s3g_1        | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 10474/multipartKey2. Entity too small.
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm2.org_1   | 2021-07-12 13:47:45,653 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:47:48,716 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:05,456 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35342
scm2.org_1   | 2021-07-12 13:48:05,475 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:09,562 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm2.org_1   | 2021-07-12 13:48:09,564 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47430
scm2.org_1   | 2021-07-12 13:48:09,584 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3206477232763, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2021-07-12 13:48:09,665 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2021-07-12 13:48:09,692 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:09,792 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2.org_1   | 2021-07-12 13:48:10,856 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]].
scm2.org_1   | 2021-07-12 13:48:10,880 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:11,007 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37030
scm2.org_1   | 2021-07-12 13:48:11,057 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:11,876 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm2.org_1   | 2021-07-12 13:48:11,877 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3208765789747, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2021-07-12 13:48:11,877 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2021-07-12 13:48:11,880 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2.org_1   | 2021-07-12 13:48:12,113 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]].
scm2.org_1   | 2021-07-12 13:48:12,114 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:12,973 [IPC Server handler 49 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5b90731c-5249-453a-9f10-ddd353e45997
scm2.org_1   | 2021-07-12 13:48:12,974 [IPC Server handler 49 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3210368079521, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2.org_1   | 2021-07-12 13:48:12,974 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2021-07-12 13:48:12,976 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2.org_1   | 2021-07-12 13:48:12,976 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2.org_1   | 2021-07-12 13:48:12,976 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2.org_1   | 2021-07-12 13:48:12,976 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2.org_1   | 2021-07-12 13:48:12,994 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2.org_1   | 2021-07-12 13:48:12,996 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2.org_1   | 2021-07-12 13:48:13,132 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]].
datanode2_1  | 2021-07-12 14:04:05,785 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:08,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:11,929 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:12,887 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6290,entriesCount=1,lastEntry=(t:2, i:117)
datanode2_1  | 2021-07-12 14:04:12,892 [java.util.concurrent.ThreadPoolExecutor$Worker@255e6364[State = -1, empty queue]] WARN server.GrpcLogAppender: e7ed1272-c70a-4bce-ab84-6d03b85621dc@group-ABE0FEB7C4D8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6291,entriesCount=1,lastEntry=(t:2, i:118)
datanode2_1  | 2021-07-12 14:04:15,001 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:18,072 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:21,144 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:24,217 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:27,289 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:30,361 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:36,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:39,577 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:42,649 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:45,720 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:48,792 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:51,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:54,937 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:04:58,008 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:01,080 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:04,157 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:07,228 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:10,296 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:13,368 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode2_1  | 2021-07-12 14:05:16,445 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:20,760 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:23,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:26,909 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:29,977 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:33,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:36,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:39,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:42,268 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:45,336 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:48,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:51,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:54,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:56:57,624 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:00,698 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:03,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:09,913 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:12,989 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:16,057 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:19,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:22,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:25,273 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:28,344 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:31,419 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:34,489 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:37,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:40,633 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:43,704 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3.org_1   | 2021-07-12 13:47:17,778 [Listener at 0.0.0.0/9860] INFO ha.SCMNodeInfo: ConfigKey ozone.scm.datanode.address is deprecated, For configuring different ports for each SCM use PortConfigKey ozone.scm.datanode.port appended with serviceId and nodeId
scm3.org_1   | 2021-07-12 13:47:19,044 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 1 on Scm Bootstrap Node cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:47:19,146 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: Persist certificate serialId 3115382315205 on Scm Bootstrap Node cae79eb6-d521-4d9d-a048-244f8c0679d9
scm3.org_1   | 2021-07-12 13:47:19,444 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@65640109] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3.org_1   | 2021-07-12 13:47:19,613 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3.org_1   | 2021-07-12 13:47:19,616 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
scm3.org_1   | 2021-07-12 13:47:19,618 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HttpAuthType: hdds.scm.http.auth.type = kerberos
scm3.org_1   | 2021-07-12 13:47:19,834 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @24004ms to org.eclipse.jetty.util.log.Slf4jLog
scm3.org_1   | 2021-07-12 13:47:20,812 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:21,059 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3.org_1   | 2021-07-12 13:47:21,118 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3.org_1   | 2021-07-12 13:47:21,126 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm3.org_1   | 2021-07-12 13:47:21,126 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm3.org_1   | 2021-07-12 13:47:21,126 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm3.org_1   | 2021-07-12 13:47:21,138 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Initialize spnego with host: 0.0.0.0 userKey: hdds.scm.http.auth.kerberos.principal keytabKey: hdds.scm.http.auth.kerberos.keytab
scm3.org_1   | 2021-07-12 13:47:21,471 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3.org_1   | 2021-07-12 13:47:21,491 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm3.org_1   | 2021-07-12 13:47:21,846 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3.org_1   | 2021-07-12 13:47:21,856 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3.org_1   | 2021-07-12 13:47:21,868 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2.org_1   | 2021-07-12 13:48:13,133 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:13,276 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7135b98c-5758-43ad-b6de-abe0feb7c4d8, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.198Z[UTC]].
scm2.org_1   | 2021-07-12 13:48:13,277 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:13,438 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-1_36 to index:36
scm2.org_1   | 2021-07-12 13:48:13,439 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_1-36
scm2.org_1   | 2021-07-12 13:48:13,441 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_37
scm2.org_1   | 2021-07-12 13:48:13,485 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 660497cf-d26f-4475-b297-562b256bcc94, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.306Z[UTC]].
scm2.org_1   | 2021-07-12 13:48:13,486 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:14,824 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]] moved to OPEN state
scm2.org_1   | 2021-07-12 13:48:14,899 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@47eaae7d, cost 72755.917us
scm2.org_1   | 2021-07-12 13:48:15,005 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:16,122 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:16,206 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5b90731c-5249-453a-9f10-ddd353e45997, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]] moved to OPEN state
scm2.org_1   | 2021-07-12 13:48:16,207 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@47eaae7d, cost 799.3us
scm2.org_1   | 2021-07-12 13:48:16,275 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:17,323 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:20,588 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:20,726 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:21,813 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1.org_1   | 2021-07-12 13:49:18,325 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:49:18,361 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:49:18,852 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49820
scm1.org_1   | 2021-07-12 13:49:18,950 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55188
scm1.org_1   | 2021-07-12 13:49:18,957 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:19,040 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:20,548 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 11769.201us
scm1.org_1   | 2021-07-12 13:49:26,324 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55042
scm1.org_1   | 2021-07-12 13:49:26,326 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:49:26,354 [IPC Server handler 40 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 26431.904us
scm1.org_1   | 2021-07-12 13:49:38,228 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55104
scm1.org_1   | 2021-07-12 13:49:38,231 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:49:48,197 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38054
scm1.org_1   | 2021-07-12 13:49:48,236 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:48,810 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:49978
scm1.org_1   | 2021-07-12 13:49:48,841 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:49:48,933 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55346
scm1.org_1   | 2021-07-12 13:49:48,970 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:03,763 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58192
scm1.org_1   | 2021-07-12 13:50:03,771 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:50:18,262 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38190
scm1.org_1   | 2021-07-12 13:50:18,281 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:18,803 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50118
scm1.org_1   | 2021-07-12 13:50:18,825 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:18,942 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55488
scm1.org_1   | 2021-07-12 13:50:18,994 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:20,554 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 17903.001us
scm1.org_1   | 2021-07-12 13:50:33,527 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55376
scm1.org_1   | 2021-07-12 13:50:33,530 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:50:34,284 [IPC Server handler 17 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract java.lang.Boolean org.apache.hadoop.hdds.scm.ha.SequenceIdGenerator$StateManager.allocateBatch(java.lang.String,java.lang.Long,java.lang.Long) on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 31553.401us
scm1.org_1   | 2021-07-12 13:50:34,288 [IPC Server handler 17 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm1.org_1   | 2021-07-12 13:50:34,333 [IPC Server handler 17 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 20432.701us
scm1.org_1   | 2021-07-12 13:50:48,173 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38348
scm1.org_1   | 2021-07-12 13:50:48,191 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:48,821 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50268
scm1.org_1   | 2021-07-12 13:50:48,872 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:50:49,006 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55636
scm1.org_1   | 2021-07-12 13:50:49,020 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:18,246 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38490
scm1.org_1   | 2021-07-12 13:51:18,270 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:18,567 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 2 containers.
scm1.org_1   | 2021-07-12 13:51:18,832 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50412
scm1.org_1   | 2021-07-12 13:51:18,851 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:18,984 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55780
scm1.org_1   | 2021-07-12 13:51:19,001 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:20,551 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 14866.9us
scm1.org_1   | 2021-07-12 13:51:42,057 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55714
scm1.org_1   | 2021-07-12 13:51:42,064 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:51:44,370 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50544
scm1.org_1   | 2021-07-12 13:51:44,419 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55912
scm1.org_1   | 2021-07-12 13:51:44,449 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:44,542 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:44,544 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38640
scm1.org_1   | 2021-07-12 13:51:44,613 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:51:52,038 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58732
scm1.org_1   | 2021-07-12 13:51:52,045 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:51:59,130 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55800
scm1.org_1   | 2021-07-12 13:51:59,140 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:52:08,730 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58806
scm1.org_1   | 2021-07-12 13:52:08,736 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:52:14,429 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56056
scm1.org_1   | 2021-07-12 13:52:14,431 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50688
scm1.org_1   | 2021-07-12 13:52:14,483 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:52:14,498 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:52:14,538 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38788
scm1.org_1   | 2021-07-12 13:52:14,543 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:52:20,555 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 18725.999us
scm1.org_1   | 2021-07-12 13:52:34,211 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:55976
scm1.org_1   | 2021-07-12 13:52:34,226 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:52:34,248 [IPC Server handler 65 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 21013.998us
scm1.org_1   | 2021-07-12 13:52:44,435 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56210
scm1.org_1   | 2021-07-12 13:52:44,497 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38942
scm1.org_1   | 2021-07-12 13:52:44,497 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:52:44,501 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50842
scm1.org_1   | 2021-07-12 13:52:44,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:41,122 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35536
scm2.org_1   | 2021-07-12 13:48:41,184 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:42,344 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]] moved to OPEN state
scm2.org_1   | 2021-07-12 13:48:42,349 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@47eaae7d, cost 1378.4us
scm2.org_1   | 2021-07-12 13:48:42,371 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:42,498 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:43,459 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:43,515 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:43,538 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2.org_1   | 2021-07-12 13:48:43,650 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47636
scm2.org_1   | 2021-07-12 13:48:43,672 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2.org_1   | 2021-07-12 13:48:43,682 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2.org_1   | 2021-07-12 13:48:44,671 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37232
scm2.org_1   | 2021-07-12 13:48:44,685 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:01,318 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47724
scm2.org_1   | 2021-07-12 13:49:01,369 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:11,226 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35690
scm2.org_1   | 2021-07-12 13:49:11,237 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:13,269 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 107544261427200000.
scm2.org_1   | 2021-07-12 13:49:18,864 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37390
scm2.org_1   | 2021-07-12 13:49:18,960 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:18,974 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47814
scm2.org_1   | 2021-07-12 13:49:19,058 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:48,189 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:35878
scm2.org_1   | 2021-07-12 13:49:48,223 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:48,836 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37550
scm2.org_1   | 2021-07-12 13:49:48,851 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:49:48,920 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:47972
scm2.org_1   | 2021-07-12 13:49:48,946 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
datanode3_1  | 2021-07-12 13:57:46,780 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:49,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:52,921 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:57:59,065 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:02,137 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:05,209 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:08,281 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:11,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:14,426 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:17,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:20,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:23,640 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:26,717 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:29,785 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:32,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:35,928 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:39,001 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:42,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:48,217 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:51,293 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:54,361 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:58:57,433 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:00,504 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:03,577 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:06,653 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:09,720 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:12,793 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:15,865 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:18,937 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:22,009 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:25,081 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:28,153 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:31,224 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:37,369 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:40,440 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:43,513 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:46,585 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:49,657 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:52,729 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:55,805 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 13:59:58,873 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:01,945 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:05,017 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:08,089 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:11,161 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:14,233 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:15,549 [pool-37-thread-9] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode3_1  | 2021-07-12 14:00:15,549 [pool-37-thread-9] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode3_1  | 2021-07-12 14:00:15,568 [pool-37-thread-9] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 130.
datanode3_1  | 2021-07-12 14:00:17,304 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:20,377 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:26,525 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:29,593 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:32,668 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:35,737 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:38,809 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:41,884 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:44,953 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:48,025 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:51,096 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:54,169 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:00:57,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:00,312 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:03,384 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:06,456 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:09,528 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:15,673 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:18,749 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:21,817 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:24,889 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:27,965 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:31,032 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:34,104 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:37,177 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:40,248 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:43,320 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:46,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:49,469 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:52,536 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:55,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:01:58,685 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:04,825 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:07,897 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:10,968 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:15,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:18,430 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:21,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:24,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:27,641 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:30,713 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:33,785 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:36,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:39,929 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:43,001 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:46,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:49,145 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:52,217 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:55,289 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:57:58,361 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:04,505 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:07,577 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:10,653 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:13,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:16,792 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:19,869 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:22,937 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:26,009 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:29,081 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:32,153 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:35,225 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:38,297 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:41,369 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:44,450 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:47,517 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:53,657 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:56,729 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:58:59,801 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:02,873 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:05,945 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:09,017 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:12,088 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:15,160 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:18,232 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:21,305 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:24,376 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:27,449 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:30,521 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:33,593 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:36,664 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:42,808 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:45,881 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:48,953 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:52,025 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:55,097 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 13:59:58,169 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:01,241 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:04,313 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:07,385 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:10,457 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:13,529 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:15,661 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode1_1  | 2021-07-12 14:00:15,661 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 130.
datanode1_1  | 2021-07-12 14:00:15,714 [pool-37-thread-10] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 130.
datanode1_1  | 2021-07-12 14:00:16,600 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:19,672 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
scm3.org_1   | 2021-07-12 13:47:21,970 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2021-07-12 13:47:21,987 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1911427a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3.org_1   | 2021-07-12 13:47:21,993 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d1a4522{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3.org_1   | 2021-07-12 13:47:22,931 [Listener at 0.0.0.0/9860] INFO server.KerberosAuthenticationHandler: Using keytab /etc/security/keytabs/HTTP.keytab, for principal HTTP/scm@EXAMPLE.COM
scm3.org_1   | 2021-07-12 13:47:23,049 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@509d0d21{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_0-SNAPSHOT_jar-_-any-15847348466569501192/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.0-SNAPSHOT.jar!/webapps/scm}
scm3.org_1   | 2021-07-12 13:47:23,131 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@31b9108b{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3.org_1   | 2021-07-12 13:47:23,148 [Listener at 0.0.0.0/9860] INFO server.Server: Started @27302ms
scm3.org_1   | 2021-07-12 13:47:23,150 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3.org_1   | 2021-07-12 13:47:23,150 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3.org_1   | 2021-07-12 13:47:23,173 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3.org_1   | 2021-07-12 13:47:35,620 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:37,811 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:39,411 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:45,214 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:45,678 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:47:48,754 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:05,531 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56768
scm3.org_1   | 2021-07-12 13:48:05,577 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:09,396 [IPC Server handler 61 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm3.org_1   | 2021-07-12 13:48:09,434 [IPC Server handler 61 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3206477232763, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2021-07-12 13:48:09,501 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2021-07-12 13:48:09,533 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52022
scm3.org_1   | 2021-07-12 13:48:09,533 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3.org_1   | 2021-07-12 13:48:09,678 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:10,855 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]].
scm3.org_1   | 2021-07-12 13:48:10,896 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:10,921 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39154
scm3.org_1   | 2021-07-12 13:48:11,056 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:11,886 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm3.org_1   | 2021-07-12 13:48:11,887 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3208765789747, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2021-07-12 13:48:11,887 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2021-07-12 13:48:11,890 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3.org_1   | 2021-07-12 13:48:12,111 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]].
scm3.org_1   | 2021-07-12 13:48:12,116 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
datanode1_1  | 2021-07-12 14:00:22,745 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:25,821 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:31,961 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:35,032 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:38,105 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:41,177 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:44,249 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:47,321 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:50,393 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:53,465 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:56,541 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:00:59,609 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:02,681 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:05,752 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:08,825 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:11,896 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:14,973 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:21,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:24,184 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:27,261 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:30,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:33,400 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:36,472 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:39,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:42,617 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:45,688 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:48,761 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:51,832 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:54,904 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:01:57,976 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:01,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:04,120 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:10,265 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:13,337 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:16,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:19,481 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:22,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:25,624 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:28,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:31,773 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:34,841 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:37,912 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:40,984 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:44,056 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:47,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:50,204 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:53,274 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:02:59,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:02,491 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:05,561 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:08,637 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:11,705 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:12,907 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode1_1  | 2021-07-12 14:03:12,907 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode1_1  | 2021-07-12 14:03:12,971 [pool-36-thread-4] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 114.
datanode1_1  | 2021-07-12 14:03:12,976 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode1_1  | 2021-07-12 14:03:14,777 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:17,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:14,040 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:17,113 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:20,185 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:23,256 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:26,329 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:29,400 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:32,472 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:35,545 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:38,620 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:41,688 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:44,761 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:47,833 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:53,976 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:02:57,049 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:00,121 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:03,193 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:06,265 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:09,336 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:12,409 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:12,953 [pool-36-thread-3] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode3_1  | 2021-07-12 14:03:12,955 [pool-36-thread-3] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 114.
datanode3_1  | 2021-07-12 14:03:12,971 [pool-36-thread-3] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 114.
datanode3_1  | 2021-07-12 14:03:12,973 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode3_1  | 2021-07-12 14:03:15,480 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:18,553 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:21,625 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:24,697 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:27,769 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:30,840 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:33,913 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1.org_1   | 2021-07-12 13:52:44,519 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:14,458 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:50986
scm1.org_1   | 2021-07-12 13:53:14,467 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56354
scm1.org_1   | 2021-07-12 13:53:14,505 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:14,509 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:14,551 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39086
scm1.org_1   | 2021-07-12 13:53:14,554 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:20,550 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 13226.298us
scm1.org_1   | 2021-07-12 13:53:44,474 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51142
scm1.org_1   | 2021-07-12 13:53:44,480 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56510
scm1.org_1   | 2021-07-12 13:53:44,493 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:44,572 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39238
scm1.org_1   | 2021-07-12 13:53:44,576 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:53:44,589 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:10,517 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:56446
scm1.org_1   | 2021-07-12 13:54:10,533 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:54:14,374 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51290
scm1.org_1   | 2021-07-12 13:54:14,399 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56654
scm1.org_1   | 2021-07-12 13:54:14,409 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:14,420 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:14,462 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39382
scm1.org_1   | 2021-07-12 13:54:14,478 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:20,546 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9578.399us
scm1.org_1   | 2021-07-12 13:54:20,775 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:59472
scm1.org_1   | 2021-07-12 13:54:20,782 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:54:44,463 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51438
scm1.org_1   | 2021-07-12 13:54:44,479 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56806
scm1.org_1   | 2021-07-12 13:54:44,549 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39536
scm1.org_1   | 2021-07-12 13:54:44,557 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:44,621 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:54:44,658 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:13,380 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:56896
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:41,173 [qtp1771421544-23] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>EntityTooSmall</Code>
s3g_1        |   <Message>Your proposed upload is smaller than the minimum allowed object size. Each part must be at least 5 MB in size, except the last part.</Message>
s3g_1        |   <Resource>10474/multipartKey2</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:102)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
scm1.org_1   | 2021-07-12 13:55:13,389 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:55:14,411 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51724
scm1.org_1   | 2021-07-12 13:55:14,415 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57088
scm1.org_1   | 2021-07-12 13:55:14,440 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:14,473 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:14,493 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39816
scm1.org_1   | 2021-07-12 13:55:14,537 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:20,556 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 19215.696us
scm1.org_1   | 2021-07-12 13:55:27,997 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:56996
scm1.org_1   | 2021-07-12 13:55:28,001 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:55:40,553 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:60064
scm1.org_1   | 2021-07-12 13:55:40,555 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:55:44,429 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57338
scm1.org_1   | 2021-07-12 13:55:44,434 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:51970
scm1.org_1   | 2021-07-12 13:55:44,455 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:44,530 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40064
scm1.org_1   | 2021-07-12 13:55:44,569 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:55:44,608 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:14,416 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57514
scm1.org_1   | 2021-07-12 13:56:14,438 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52146
scm1.org_1   | 2021-07-12 13:56:14,459 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:14,487 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40242
scm1.org_1   | 2021-07-12 13:56:14,497 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:14,517 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:18,570 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 2 containers.
scm1.org_1   | 2021-07-12 13:56:20,548 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 10785.298us
scm1.org_1   | 2021-07-12 13:56:25,257 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:57404
scm1.org_1   | 2021-07-12 13:56:25,259 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:56:34,227 [IPC Server handler 68 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 26091.793us
scm1.org_1   | 2021-07-12 13:56:35,561 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:60446
scm1.org_1   | 2021-07-12 13:56:35,563 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:56:44,423 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52384
scm1.org_1   | 2021-07-12 13:56:44,430 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57740
scm1.org_1   | 2021-07-12 13:56:44,445 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:44,487 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40474
scm1.org_1   | 2021-07-12 13:56:44,502 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:12,969 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5b90731c-5249-453a-9f10-ddd353e45997
scm3.org_1   | 2021-07-12 13:48:12,971 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: 3210368079521, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3.org_1   | 2021-07-12 13:48:12,971 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
datanode1_1  | 2021-07-12 14:03:20,922 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:23,992 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:27,065 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:30,136 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:33,209 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:36,281 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:39,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:42,424 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:48,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:51,641 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:54,713 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:03:57,785 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:00,856 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:03,929 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:07,000 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:10,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:13,145 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:16,216 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:19,288 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:22,360 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:25,432 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:28,504 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:31,576 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:37,721 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:40,792 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:43,864 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:46,936 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:50,008 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:53,080 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:56,152 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:04:59,224 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:02,297 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:05,369 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:08,442 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:11,512 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:14,584 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode1_1  | 2021-07-12 14:05:17,657 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2.org_1   | 2021-07-12 13:50:18,225 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36014
scm2.org_1   | 2021-07-12 13:50:18,243 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:50:18,827 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37688
scm2.org_1   | 2021-07-12 13:50:18,848 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:50:18,942 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48106
scm2.org_1   | 2021-07-12 13:50:18,981 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:50:48,190 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36166
scm2.org_1   | 2021-07-12 13:50:48,227 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:50:48,810 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37838
scm2.org_1   | 2021-07-12 13:50:48,850 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:50:48,960 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48262
scm2.org_1   | 2021-07-12 13:50:49,006 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:18,214 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36312
scm2.org_1   | 2021-07-12 13:51:18,234 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:18,816 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:37984
scm2.org_1   | 2021-07-12 13:51:18,837 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:18,934 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48406
scm2.org_1   | 2021-07-12 13:51:18,973 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:41,137 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 2 containers.
scm2.org_1   | 2021-07-12 13:51:44,400 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48538
scm2.org_1   | 2021-07-12 13:51:44,423 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38116
scm2.org_1   | 2021-07-12 13:51:44,453 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:44,517 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36464
scm2.org_1   | 2021-07-12 13:51:44,532 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:51:44,609 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:14,410 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48682
scm2.org_1   | 2021-07-12 13:52:14,424 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:14,430 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38258
scm2.org_1   | 2021-07-12 13:52:14,440 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:14,492 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36604
scm2.org_1   | 2021-07-12 13:52:14,522 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:44,463 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48834
scm2.org_1   | 2021-07-12 13:52:44,474 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38412
scm2.org_1   | 2021-07-12 13:52:44,503 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:44,503 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:52:44,525 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36758
scm2.org_1   | 2021-07-12 13:52:44,528 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:14,423 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:48978
scm2.org_1   | 2021-07-12 13:53:14,432 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38556
datanode3_1  | 2021-07-12 14:03:36,985 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:43,129 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:46,201 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:49,272 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:52,345 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:55,417 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:03:58,490 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:01,560 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:04,632 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:07,704 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:10,776 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:13,849 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:16,921 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:19,993 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:23,064 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:26,136 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:32,284 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:35,353 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:38,428 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:41,497 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:44,569 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:47,640 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:50,712 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:53,784 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:56,857 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:04:59,929 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:05:03,008 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:05:06,073 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:05:09,144 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:05:12,216 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode3_1  | 2021-07-12 14:05:15,288 [EndpointStateMachine task thread for recon/172.25.0.115:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.25.0.115:9891. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2.org_1   | 2021-07-12 13:53:14,463 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:14,489 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:36904
scm2.org_1   | 2021-07-12 13:53:14,503 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:14,510 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:44,412 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:49134
scm2.org_1   | 2021-07-12 13:53:44,415 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38712
scm2.org_1   | 2021-07-12 13:53:44,441 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:44,471 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:53:44,492 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37060
scm2.org_1   | 2021-07-12 13:53:44,499 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:14,417 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:38854
scm2.org_1   | 2021-07-12 13:54:14,424 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:49280
scm2.org_1   | 2021-07-12 13:54:14,440 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:14,444 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:14,500 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37206
scm2.org_1   | 2021-07-12 13:54:14,515 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:44,489 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:49432
scm2.org_1   | 2021-07-12 13:54:44,519 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39010
scm2.org_1   | 2021-07-12 13:54:44,546 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37354
scm2.org_1   | 2021-07-12 13:54:44,548 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:44,608 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:54:44,675 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:14,440 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39288
scm2.org_1   | 2021-07-12 13:55:14,490 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37640
scm2.org_1   | 2021-07-12 13:55:14,510 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:14,514 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:49714
scm2.org_1   | 2021-07-12 13:55:14,518 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:14,527 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:44,422 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39534
scm2.org_1   | 2021-07-12 13:55:44,441 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:49956
scm2.org_1   | 2021-07-12 13:55:44,505 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:37888
scm2.org_1   | 2021-07-12 13:55:44,509 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:44,583 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:55:44,609 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:56:14,405 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:50140
scm2.org_1   | 2021-07-12 13:56:14,436 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39716
scm2.org_1   | 2021-07-12 13:56:14,446 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
om1_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:42,938 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:42,938 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36664
om1_1        | 2021-07-12 13:56:42,948 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:42,980 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-16724/19004/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om1_1        | partName: "etag1"
om1_1        | , partNumber: 1
om1_1        | partName: "etag2"
om1_1        | ]
om1_1        | 2021-07-12 13:56:42,981 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om1_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:173)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:43,562 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:43,562 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36678
om1_1        | 2021-07-12 13:56:43,563 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:47,192 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:47,192 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36708
om1_1        | 2021-07-12 13:56:47,195 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:48,370 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:48,370 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36716
om1_1        | 2021-07-12 13:56:48,372 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:49,199 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:49,199 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36724
om1_1        | 2021-07-12 13:56:49,202 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:49,222 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om1_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567971869294625
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:49,754 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:49,755 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36738
om1_1        | 2021-07-12 13:56:49,756 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
scm3.org_1   | 2021-07-12 13:48:12,977 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3.org_1   | 2021-07-12 13:48:12,977 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3.org_1   | 2021-07-12 13:48:12,977 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3.org_1   | 2021-07-12 13:48:12,977 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3.org_1   | 2021-07-12 13:48:12,997 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3.org_1   | 2021-07-12 13:48:12,998 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3.org_1   | 2021-07-12 13:48:13,136 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]].
scm3.org_1   | 2021-07-12 13:48:13,137 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:13,281 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 7135b98c-5758-43ad-b6de-abe0feb7c4d8, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.198Z[UTC]].
scm3.org_1   | 2021-07-12 13:48:13,282 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:13,444 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolling segment log-1_36 to index:36
scm3.org_1   | 2021-07-12 13:48:13,446 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_1 to /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_1-36
scm3.org_1   | 2021-07-12 13:48:13,447 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/63d3b4db-26eb-4acd-b27c-ee404ddd1bcd/current/log_inprogress_37
scm3.org_1   | 2021-07-12 13:48:13,492 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 660497cf-d26f-4475-b297-562b256bcc94, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-07-12T13:48:13.306Z[UTC]].
scm3.org_1   | 2021-07-12 13:48:13,493 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:14,851 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7f7f3d83-0cbd-41c6-b795-89c5f88bc674, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:12.035Z[UTC]] moved to OPEN state
scm3.org_1   | 2021-07-12 13:48:15,109 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 255788.861us
scm1.org_1   | 2021-07-12 13:56:44,533 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:56:51,582 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:60596
scm1.org_1   | 2021-07-12 13:56:51,586 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:57:05,587 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:60720
scm1.org_1   | 2021-07-12 13:57:05,590 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:57:14,377 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52652
scm1.org_1   | 2021-07-12 13:57:14,403 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58024
scm1.org_1   | 2021-07-12 13:57:14,412 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:14,433 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:14,472 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40752
scm1.org_1   | 2021-07-12 13:57:14,503 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:20,545 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 7857.598us
scm1.org_1   | 2021-07-12 13:57:34,276 [IPC Server handler 7 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 76412.177us
scm1.org_1   | 2021-07-12 13:57:44,449 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58250
scm1.org_1   | 2021-07-12 13:57:44,461 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:52890
scm1.org_1   | 2021-07-12 13:57:44,507 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:44,517 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:44,528 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40980
scm1.org_1   | 2021-07-12 13:57:44,547 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:57:51,591 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58120
scm1.org_1   | 2021-07-12 13:57:51,594 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:57:52,796 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:32866
scm1.org_1   | 2021-07-12 13:57:52,798 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:57:56,661 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:60028
scm1.org_1   | 2021-07-12 13:57:56,707 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:57:56,721 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:55932
scm1.org_1   | 2021-07-12 13:57:56,751 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:58:09,009 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58246
scm1.org_1   | 2021-07-12 13:58:09,011 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:58:10,251 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:32992
scm1.org_1   | 2021-07-12 13:58:10,256 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:58:14,429 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:53120
scm1.org_1   | 2021-07-12 13:58:14,441 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58488
scm1.org_1   | 2021-07-12 13:58:14,449 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:58:14,496 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:58:14,523 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41218
scm1.org_1   | 2021-07-12 13:58:14,526 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:15,168 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:16,064 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:16,206 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 7ef1c9bf-43c8-43b8-b00a-7844fdf8175c, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5b90731c-5249-453a-9f10-ddd353e45997, CreationTimestamp2021-07-12T13:48:13.030Z[UTC]] moved to OPEN state
scm3.org_1   | 2021-07-12 13:48:16,207 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 1309.1us
scm3.org_1   | 2021-07-12 13:48:16,291 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:17,283 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:20,601 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:20,668 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:21,801 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:41,118 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:56962
scm3.org_1   | 2021-07-12 13:48:41,142 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:42,318 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: ff1246ee-6927-4ea3-b470-c4d54606d42f, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7, CreationTimestamp2021-07-12T13:48:09.774Z[UTC]] moved to OPEN state
scm3.org_1   | 2021-07-12 13:48:42,336 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 2646.401us
scm3.org_1   | 2021-07-12 13:48:42,361 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:42,494 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:43,465 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52224
scm3.org_1   | 2021-07-12 13:48:43,480 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3.org_1   | 2021-07-12 13:48:43,508 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:43,513 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:48:43,609 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 660497cf-d26f-4475-b297-562b256bcc94, Nodes: 5b90731c-5249-453a-9f10-ddd353e45997{ip: 172.25.0.102, host: ozonesecure-ha_datanode1_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:e7ed1272-c70a-4bce-ab84-6d03b85621dc, CreationTimestamp2021-07-12T13:48:13.306Z[UTC]] moved to OPEN state
scm3.org_1   | 2021-07-12 13:48:43,610 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 833.7us
scm3.org_1   | 2021-07-12 13:48:43,624 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:43,703 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3.org_1   | 2021-07-12 13:48:43,703 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3.org_1   | 2021-07-12 13:48:43,703 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3.org_1   | 2021-07-12 13:48:43,704 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3.org_1   | 2021-07-12 13:48:43,704 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3.org_1   | 2021-07-12 13:48:43,704 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3.org_1   | 2021-07-12 13:48:44,652 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39348
scm3.org_1   | 2021-07-12 13:48:44,654 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:01,320 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52312
scm3.org_1   | 2021-07-12 13:49:01,377 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:11,183 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57116
scm3.org_1   | 2021-07-12 13:49:11,197 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:13,266 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 107544261427200000.
scm3.org_1   | 2021-07-12 13:49:18,877 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39510
scm3.org_1   | 2021-07-12 13:49:18,956 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52402
scm3.org_1   | 2021-07-12 13:49:18,962 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:19,041 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:48,186 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57304
scm3.org_1   | 2021-07-12 13:49:48,221 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:48,831 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39666
scm3.org_1   | 2021-07-12 13:49:48,869 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:49:48,931 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52560
scm3.org_1   | 2021-07-12 13:49:48,949 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:18,250 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57440
scm3.org_1   | 2021-07-12 13:50:18,261 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:18,832 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39802
scm3.org_1   | 2021-07-12 13:50:18,854 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:18,936 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52698
scm3.org_1   | 2021-07-12 13:50:18,982 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:48,175 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57592
scm3.org_1   | 2021-07-12 13:50:48,226 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:48,811 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39958
scm3.org_1   | 2021-07-12 13:50:48,865 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:50:48,959 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52850
scm3.org_1   | 2021-07-12 13:50:49,002 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:18,215 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57742
scm3.org_1   | 2021-07-12 13:51:18,257 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:18,807 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40100
scm3.org_1   | 2021-07-12 13:51:18,829 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:18,976 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52994
scm3.org_1   | 2021-07-12 13:51:18,998 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:44,412 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53126
scm3.org_1   | 2021-07-12 13:51:44,454 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40232
scm3.org_1   | 2021-07-12 13:51:44,461 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:44,522 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:57890
scm3.org_1   | 2021-07-12 13:51:44,533 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:51:44,608 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:03,495 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 2 containers.
scm3.org_1   | 2021-07-12 13:52:14,411 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53270
scm3.org_1   | 2021-07-12 13:52:14,433 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:14,461 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40378
scm3.org_1   | 2021-07-12 13:52:14,477 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:14,496 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58034
scm3.org_1   | 2021-07-12 13:52:14,523 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:44,433 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53426
scm3.org_1   | 2021-07-12 13:52:44,441 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40532
scm3.org_1   | 2021-07-12 13:52:44,495 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:44,506 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:52:44,538 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58186
scm3.org_1   | 2021-07-12 13:52:44,544 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:14,396 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53570
scm3.org_1   | 2021-07-12 13:53:14,430 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40676
scm3.org_1   | 2021-07-12 13:53:14,443 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:14,503 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:14,516 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58330
scm3.org_1   | 2021-07-12 13:53:14,534 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:44,479 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53726
om1_1        | 2021-07-12 13:56:49,770 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om1_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567972107190306
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getMultipartDataSize(S3MultipartUploadCompleteRequest.java:445)
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:184)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:50,307 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:50,307 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36742
om1_1        | 2021-07-12 13:56:50,309 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:50,339 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-16724/19004/multipartKey3
om1_1        | 2021-07-12 13:56:50,345 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: 19004/multipartKey3 in Volume/Bucket s3v/bucket-16724
om1_1        | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3 because parts are in Invalid order.
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.getPartsListSize(S3MultipartUploadCompleteRequest.java:411)
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:180)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:50,903 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:50,903 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36748
om1_1        | 2021-07-12 13:56:50,915 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:51,489 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:51,490 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36754
om1_1        | 2021-07-12 13:56:51,499 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:52,431 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:52,431 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36764
om1_1        | 2021-07-12 13:56:52,433 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:53,033 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:53,033 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36770
om1_1        | 2021-07-12 13:56:53,044 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:53,663 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:53,664 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36774
om1_1        | 2021-07-12 13:56:53,665 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:53,688 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName 50306/multipartKey5 in VolumeName/Bucket s3v/bucket-16724
om1_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-16724key: 50306/multipartKey5
om1_1        | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:148)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm2.org_1   | 2021-07-12 13:56:14,482 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38066
scm2.org_1   | 2021-07-12 13:56:14,492 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:56:14,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:56:41,138 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 2 containers.
scm2.org_1   | 2021-07-12 13:56:44,428 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:50366
scm2.org_1   | 2021-07-12 13:56:44,438 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:39954
scm2.org_1   | 2021-07-12 13:56:44,489 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:56:44,497 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38296
scm2.org_1   | 2021-07-12 13:56:44,518 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:56:44,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:14,476 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40222
scm2.org_1   | 2021-07-12 13:57:14,529 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:14,532 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38570
scm2.org_1   | 2021-07-12 13:57:14,540 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:50644
scm2.org_1   | 2021-07-12 13:57:14,565 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:14,573 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:44,443 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40462
scm2.org_1   | 2021-07-12 13:57:44,458 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:50870
scm2.org_1   | 2021-07-12 13:57:44,467 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:44,506 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:38804
scm2.org_1   | 2021-07-12 13:57:44,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:57:44,542 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:14,430 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40692
scm2.org_1   | 2021-07-12 13:58:14,466 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:14,489 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51114
scm2.org_1   | 2021-07-12 13:58:14,509 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:14,533 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39042
scm2.org_1   | 2021-07-12 13:58:14,537 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:44,412 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40918
scm2.org_1   | 2021-07-12 13:58:44,424 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:44,452 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51330
scm2.org_1   | 2021-07-12 13:58:44,491 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39266
scm2.org_1   | 2021-07-12 13:58:44,502 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:58:44,522 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:14,423 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41144
scm2.org_1   | 2021-07-12 13:59:14,444 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51568
scm2.org_1   | 2021-07-12 13:59:14,447 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:14,479 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39492
om1_1        | 2021-07-12 13:56:54,231 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:54,232 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36780
om1_1        | 2021-07-12 13:56:54,233 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:54,249 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-16724, Key75644/multipartKey. Exception:{}
om1_1        | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om1_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartFileInfo(OMKeyRequest.java:708)
om1_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareFileInfo(OMKeyRequest.java:600)
om1_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:577)
om1_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:278)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:56:54,793 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:54,793 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36786
om1_1        | 2021-07-12 13:56:54,798 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:55,528 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:55,528 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36800
om1_1        | 2021-07-12 13:56:55,537 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:56,590 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:56,590 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36808
om1_1        | 2021-07-12 13:56:56,592 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:57,325 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:57,325 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36816
om1_1        | 2021-07-12 13:56:57,327 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:58,045 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:58,046 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36822
om1_1        | 2021-07-12 13:56:58,047 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:58,696 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:58,699 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36826
om1_1        | 2021-07-12 13:56:58,708 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:56:59,325 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:56:59,327 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36832
om1_1        | 2021-07-12 13:56:59,332 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:00,091 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:00,092 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36838
om1_1        | 2021-07-12 13:57:00,093 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:00,210 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:00,211 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36840
om1_1        | 2021-07-12 13:57:00,228 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:00,293 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:00,294 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36844
om1_1        | 2021-07-12 13:57:00,303 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
scm2.org_1   | 2021-07-12 13:59:14,506 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:14,520 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:24,783 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a6a86e95-d787-4a76-a3c8-90a7010e1026, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T13:59:24.764Z[UTC]].
scm2.org_1   | 2021-07-12 13:59:27,109 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51632
scm2.org_1   | 2021-07-12 13:59:27,122 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:44,428 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41310
scm2.org_1   | 2021-07-12 13:59:44,475 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39654
scm2.org_1   | 2021-07-12 13:59:44,490 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:44,507 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 13:59:57,072 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51794
scm2.org_1   | 2021-07-12 13:59:57,098 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:14,404 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41450
scm2.org_1   | 2021-07-12 14:00:14,424 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:14,487 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39792
scm2.org_1   | 2021-07-12 14:00:14,501 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:15,584 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
scm2.org_1   | 2021-07-12 14:00:15,587 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@47eaae7d, cost 1268.999us
scm2.org_1   | 2021-07-12 14:00:15,619 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
scm2.org_1   | org.apache.ratis.protocol.exceptions.NotLeaderException: Server de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD is not the leader 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:212)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
scm2.org_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:242)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:227)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:96)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:88)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:40)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm2.org_1   | 2021-07-12 14:00:15,716 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:51886
om1_1        | 2021-07-12 13:57:00,326 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36848
om1_1        | 2021-07-12 13:57:00,327 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:00,327 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:04,894 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:04,894 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36872
om1_1        | 2021-07-12 13:57:04,908 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:05,504 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:05,505 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36878
om1_1        | 2021-07-12 13:57:05,513 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:05,637 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:05,637 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36890
om1_1        | 2021-07-12 13:57:05,638 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:05,641 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36892
om1_1        | 2021-07-12 13:57:05,641 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:05,642 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36888
om1_1        | 2021-07-12 13:57:05,656 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:05,658 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:05,675 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:07,104 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:07,104 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36906
om1_1        | 2021-07-12 13:57:07,105 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:07,923 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:07,924 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36918
om1_1        | 2021-07-12 13:57:07,928 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:11,338 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:11,338 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36934
om1_1        | 2021-07-12 13:57:11,342 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:11,941 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:11,941 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36938
om1_1        | 2021-07-12 13:57:11,948 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:15,597 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:15,597 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36978
om1_1        | 2021-07-12 13:57:15,607 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:16,168 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:16,169 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36984
om1_1        | 2021-07-12 13:57:16,170 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:17,034 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:17,034 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36990
om1_1        | 2021-07-12 13:57:17,035 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:18,302 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
om1_1        | 2021-07-12 13:57:18,302 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:36998
om1_1        | 2021-07-12 13:57:18,307 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:18,900 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:18,900 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37004
om1_1        | 2021-07-12 13:57:18,905 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:23,051 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:23,051 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37028
om1_1        | 2021-07-12 13:57:23,064 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:26,370 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:26,373 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37050
om1_1        | 2021-07-12 13:57:26,374 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:26,920 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:26,920 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37056
om1_1        | 2021-07-12 13:57:26,924 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:28,028 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:28,029 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37066
om1_1        | 2021-07-12 13:57:28,036 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:31,941 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:31,942 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37098
om1_1        | 2021-07-12 13:57:31,948 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:32,550 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:32,550 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37102
om1_1        | 2021-07-12 13:57:32,556 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:33,126 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:33,127 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37108
om1_1        | 2021-07-12 13:57:33,132 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:33,713 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:33,713 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37112
om1_1        | 2021-07-12 13:57:33,715 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:37,503 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:37,504 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37128
om1_1        | 2021-07-12 13:57:37,520 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:40,777 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:40,778 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37152
om1_1        | 2021-07-12 13:57:40,781 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:41,315 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:41,316 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37156
om1_1        | 2021-07-12 13:57:41,317 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:42,236 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:42,236 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37166
om1_1        | 2021-07-12 13:57:42,243 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:42,825 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:42,826 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37170
om1_1        | 2021-07-12 13:57:42,829 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:43,415 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:43,416 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37176
om1_1        | 2021-07-12 13:57:43,417 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:47,840 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:50474
om1_1        | 2021-07-12 13:57:47,860 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:50,462 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:50,462 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37228
om1_1        | 2021-07-12 13:57:50,463 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:50,997 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:50,998 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37236
om1_1        | 2021-07-12 13:57:50,999 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:51,569 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:51,569 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37242
om1_1        | 2021-07-12 13:57:51,576 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:52,236 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:42,398 [qtp1771421544-23] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 19004/multipartKey3
s3g_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
om1_1        | 2021-07-12 13:57:52,237 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37252
om1_1        | 2021-07-12 13:57:52,240 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:52,769 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:52,770 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37256
om1_1        | 2021-07-12 13:57:52,773 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:55,976 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:55,977 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37274
om1_1        | 2021-07-12 13:57:55,984 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:56,512 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:56,512 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37278
om1_1        | 2021-07-12 13:57:56,513 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:57:59,740 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:57:59,740 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37308
om1_1        | 2021-07-12 13:57:59,749 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:00,262 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:00,262 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37312
om1_1        | 2021-07-12 13:58:00,266 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:00,784 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:00,784 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37318
om1_1        | 2021-07-12 13:58:00,786 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:01,310 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:01,311 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37322
om1_1        | 2021-07-12 13:58:01,317 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:01,827 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:01,827 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37326
om1_1        | 2021-07-12 13:58:01,833 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:05,784 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:50606
om1_1        | 2021-07-12 13:58:05,803 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:08,426 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:08,427 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37356
om1_1        | 2021-07-12 13:58:08,431 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:08,981 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:08,982 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37368
om1_1        | 2021-07-12 13:58:08,983 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:09,691 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:09,691 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37378
om1_1        | 2021-07-12 13:58:09,694 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:10,222 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:10,222 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37382
om1_1        | 2021-07-12 13:58:10,223 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:10,780 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:10,780 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37390
om1_1        | 2021-07-12 13:58:10,782 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:11,271 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:11,271 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37394
om1_1        | 2021-07-12 13:58:11,273 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:11,767 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:11,767 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37400
om1_1        | 2021-07-12 13:58:11,768 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:12,274 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:12,275 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37404
om1_1        | 2021-07-12 13:58:12,279 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:12,789 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:12,790 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37410
om1_1        | 2021-07-12 13:58:12,791 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:13,426 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:13,427 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37416
om1_1        | 2021-07-12 13:58:13,429 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:13,950 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:13,950 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37422
om1_1        | 2021-07-12 13:58:13,951 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm3.org_1   | 2021-07-12 13:53:44,504 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40832
scm3.org_1   | 2021-07-12 13:53:44,514 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58490
scm3.org_1   | 2021-07-12 13:53:44,534 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:44,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:53:44,543 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:54:14,408 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:40974
scm3.org_1   | 2021-07-12 13:54:14,434 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53868
scm3.org_1   | 2021-07-12 13:54:14,443 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:54:14,473 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:54:14,491 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58632
scm3.org_1   | 2021-07-12 13:54:14,505 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:15,748 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:45,594 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:39944
scm2.org_1   | 2021-07-12 14:00:45,649 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:45,665 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52026
scm2.org_1   | 2021-07-12 14:00:45,681 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:45,736 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41620
scm2.org_1   | 2021-07-12 14:00:45,743 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:00:53,664 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 910d9890-e228-42df-8573-868a0d6a1cfb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:00:53.652Z[UTC]].
scm2.org_1   | 2021-07-12 14:01:15,593 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40092
scm2.org_1   | 2021-07-12 14:01:15,639 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:15,662 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52176
scm2.org_1   | 2021-07-12 14:01:15,670 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:15,742 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41770
scm2.org_1   | 2021-07-12 14:01:15,754 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:41,053 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:46326
scm2.org_1   | 2021-07-12 14:01:41,083 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm2.org_1   | 2021-07-12 14:01:41,138 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 3 containers.
scm2.org_1   | 2021-07-12 14:01:41,558 [IPC Server handler 75 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
scm2.org_1   | 2021-07-12 14:01:45,626 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52332
scm2.org_1   | 2021-07-12 14:01:45,634 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:45,649 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40246
scm2.org_1   | 2021-07-12 14:01:45,658 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:45,739 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41926
scm2.org_1   | 2021-07-12 14:01:45,779 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:01:50,142 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:46382
scm2.org_1   | 2021-07-12 14:01:50,165 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm2.org_1   | 2021-07-12 14:01:50,167 [IPC Server handler 75 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2021-07-12 14:01:50,168 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 3 containers.
scm2.org_1   | 2021-07-12 14:02:15,597 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40398
scm2.org_1   | 2021-07-12 14:02:15,617 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52484
scm2.org_1   | 2021-07-12 14:02:15,636 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:02:15,638 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:02:15,738 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42074
scm2.org_1   | 2021-07-12 14:02:15,783 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:02:41,923 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52624
scm2.org_1   | 2021-07-12 14:02:41,938 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:42,403 [qtp1771421544-23] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidPart</Code>
s3g_1        |   <Message>One or more of the specified parts could not be found. The part might not have been uploaded, or the specified entity tag might not have matched the part's entity tag.</Message>
s3g_1        |   <Resource>19004/multipartKey3</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:92)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
scm1.org_1   | 2021-07-12 13:58:20,559 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 21342.793us
scm1.org_1   | 2021-07-12 13:58:28,075 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58416
scm1.org_1   | 2021-07-12 13:58:28,084 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:58:34,216 [IPC Server handler 51 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 16311.195us
scm1.org_1   | 2021-07-12 13:58:44,424 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:53352
scm1.org_1   | 2021-07-12 13:58:44,429 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:58:44,451 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58716
scm1.org_1   | 2021-07-12 13:58:44,488 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41442
scm1.org_1   | 2021-07-12 13:58:44,508 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:58:44,517 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:58:45,454 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:33292
scm1.org_1   | 2021-07-12 13:58:45,456 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:14,431 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:53574
scm1.org_1   | 2021-07-12 13:59:14,462 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:58942
scm1.org_1   | 2021-07-12 13:59:14,475 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:14,519 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41670
scm1.org_1   | 2021-07-12 13:59:14,527 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:14,531 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:15,103 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58772
scm1.org_1   | 2021-07-12 13:59:15,109 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:59:20,547 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9458.796us
scm1.org_1   | 2021-07-12 13:59:24,386 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:42762
scm1.org_1   | 2021-07-12 13:59:24,404 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 13:59:24,742 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43872
scm1.org_1   | 2021-07-12 13:59:24,759 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:24,773 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a6a86e95-d787-4a76-a3c8-90a7010e1026, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T13:59:24.764Z[UTC]].
scm1.org_1   | 2021-07-12 13:59:24,774 [IPC Server handler 22 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9750.397us
scm1.org_1   | 2021-07-12 13:59:24,796 [IPC Server handler 22 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 21106.793us
scm1.org_1   | 2021-07-12 13:59:27,076 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59006
scm1.org_1   | 2021-07-12 13:59:27,111 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:31,169 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43914
scm1.org_1   | 2021-07-12 13:59:31,192 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
om1_1        | 2021-07-12 13:58:14,565 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:14,565 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37446
scm2.org_1   | 2021-07-12 14:02:45,618 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40558
scm2.org_1   | 2021-07-12 14:02:45,621 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:02:45,731 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42230
scm2.org_1   | 2021-07-12 14:02:45,738 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:11,920 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52774
scm2.org_1   | 2021-07-12 14:03:11,930 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:12,987 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
scm2.org_1   | 2021-07-12 14:03:12,993 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@47eaae7d, cost 2488.599us
scm2.org_1   | 2021-07-12 14:03:12,996 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
scm2.org_1   | org.apache.ratis.protocol.exceptions.NotLeaderException: Server de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD is not the leader 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm2.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm2.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
scm2.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:212)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
scm2.org_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:242)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:227)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:96)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:88)
scm2.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:40)
scm2.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm2.org_1   | 2021-07-12 14:03:12,999 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40706
scm2.org_1   | 2021-07-12 14:03:13,034 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42372
scm2.org_1   | 2021-07-12 14:03:13,037 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:13,047 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:42,968 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:52934
scm2.org_1   | 2021-07-12 14:03:43,010 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:43,047 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42528
scm2.org_1   | 2021-07-12 14:03:43,060 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:03:43,065 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:40860
scm2.org_1   | 2021-07-12 14:03:43,078 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:34,212 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:58880
scm1.org_1   | 2021-07-12 13:59:34,214 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 13:59:34,225 [IPC Server handler 75 on default port 9863] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.addTransactionsToDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 10584.597us
scm1.org_1   | 2021-07-12 13:59:35,566 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43938
scm1.org_1   | 2021-07-12 13:59:35,586 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:39,452 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43952
scm1.org_1   | 2021-07-12 13:59:39,469 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:43,363 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43970
scm1.org_1   | 2021-07-12 13:59:43,391 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:44,434 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:53744
scm1.org_1   | 2021-07-12 13:59:44,484 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41830
scm1.org_1   | 2021-07-12 13:59:44,496 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:44,516 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 13:59:47,630 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44002
scm1.org_1   | 2021-07-12 13:59:47,652 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:51,627 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44014
scm1.org_1   | 2021-07-12 13:59:51,650 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:55,855 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44034
scm1.org_1   | 2021-07-12 13:59:55,874 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 13:59:56,118 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1
scm1.org_1   | 2021-07-12 13:59:56,147 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 22844.592us
scm1.org_1   | 2021-07-12 13:59:57,101 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59168
scm1.org_1   | 2021-07-12 13:59:57,122 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:00,188 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44062
scm1.org_1   | 2021-07-12 14:00:00,221 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:04,420 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44082
scm1.org_1   | 2021-07-12 14:00:04,437 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:14,399 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:53880
scm1.org_1   | 2021-07-12 14:00:14,411 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:14,469 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41968
scm1.org_1   | 2021-07-12 14:00:14,481 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:15,600 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
om1_1        | 2021-07-12 13:58:14,566 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:15,072 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:15,075 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37458
om1_1        | 2021-07-12 13:58:15,083 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:15,675 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:15,676 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37464
om1_1        | 2021-07-12 13:58:15,677 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:18,810 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:18,811 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37476
om1_1        | 2021-07-12 13:58:18,813 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:19,329 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:19,330 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37480
om1_1        | 2021-07-12 13:58:19,331 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:19,818 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:19,818 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37484
om1_1        | 2021-07-12 13:58:19,821 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:20,329 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:20,329 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37490
om1_1        | 2021-07-12 13:58:20,330 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:20,841 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:20,842 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37496
om1_1        | 2021-07-12 13:58:20,844 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:24,773 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:50774
om1_1        | 2021-07-12 13:58:24,791 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:27,493 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:27,496 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37532
om1_1        | 2021-07-12 13:58:27,504 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:28,035 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:28,036 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37538
om1_1        | 2021-07-12 13:58:28,043 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:31,215 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:31,216 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37562
om1_1        | 2021-07-12 13:58:31,217 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:34,363 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:34,363 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37582
om1_1        | 2021-07-12 13:58:34,364 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:35,013 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:35,013 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37588
om1_1        | 2021-07-12 13:58:35,015 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:35,532 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:35,532 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37594
scm2.org_1   | 2021-07-12 14:03:51,766 [de93e951-704c-492b-918a-7b9e03404729@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: de4c8d2c-187a-46f2-8f2d-2116a6d654eb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:03:51.730Z[UTC]].
scm2.org_1   | 2021-07-12 14:04:12,966 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53078
scm2.org_1   | 2021-07-12 14:04:13,001 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41006
scm2.org_1   | 2021-07-12 14:04:13,018 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:13,038 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42676
scm2.org_1   | 2021-07-12 14:04:13,045 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:13,060 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:38,732 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:52720
scm2.org_1   | 2021-07-12 14:04:38,751 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm2.org_1   | 2021-07-12 14:04:38,753 [IPC Server handler 92 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
scm2.org_1   | 2021-07-12 14:04:42,936 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53244
scm2.org_1   | 2021-07-12 14:04:42,959 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:43,011 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42840
scm2.org_1   | 2021-07-12 14:04:43,032 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41170
scm2.org_1   | 2021-07-12 14:04:43,036 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:43,066 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:04:47,020 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:52772
scm2.org_1   | 2021-07-12 14:04:47,046 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm2.org_1   | 2021-07-12 14:04:47,047 [IPC Server handler 75 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm2.org_1   | 2021-07-12 14:04:47,060 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 4 containers.
scm2.org_1   | 2021-07-12 14:05:12,944 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:53392
scm2.org_1   | 2021-07-12 14:05:13,038 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:05:13,057 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:41316
scm2.org_1   | 2021-07-12 14:05:13,105 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42990
scm2.org_1   | 2021-07-12 14:05:13,116 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm2.org_1   | 2021-07-12 14:05:13,145 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:42,984 [qtp1771421544-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 19004/multipartKey3
s3g_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
scm1.org_1   | 2021-07-12 14:00:15,656 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 55060.28us
scm1.org_1   | 2021-07-12 14:00:15,685 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59258
scm1.org_1   | 2021-07-12 14:00:15,754 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:18,962 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44150
scm1.org_1   | 2021-07-12 14:00:18,985 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:20,555 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 16887.894us
scm1.org_1   | 2021-07-12 14:00:25,887 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44176
scm1.org_1   | 2021-07-12 14:00:25,906 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:25,908 [IPC Server handler 81 on default port 9860] INFO ipc.Server: IPC Server handler 81 on default port 9860, call Call#0 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.116:44176
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainer(SCMClientProtocolServer.java:214)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:425)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:190)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm1.org_1   | 2021-07-12 14:00:29,961 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:43092
scm1.org_1   | 2021-07-12 14:00:29,994 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 14:00:30,339 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44210
scm1.org_1   | 2021-07-12 14:00:30,344 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:30,345 [IPC Server handler 2 on default port 9860] INFO ipc.Server: IPC Server handler 2 on default port 9860, call Call#1 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.116:44210
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.allocateContainer(SCMClientProtocolServer.java:198)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.allocateContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:414)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:182)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm1.org_1   | 2021-07-12 14:00:34,265 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44230
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm3.org_1   | 2021-07-12 13:54:44,434 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41126
scm3.org_1   | 2021-07-12 13:54:44,495 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54020
scm3.org_1   | 2021-07-12 13:54:44,515 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:54:44,571 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:58786
scm3.org_1   | 2021-07-12 13:54:44,621 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:54:44,670 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:14,420 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54302
scm3.org_1   | 2021-07-12 13:55:14,433 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41408
scm3.org_1   | 2021-07-12 13:55:14,434 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:14,499 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59066
scm3.org_1   | 2021-07-12 13:55:14,509 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:14,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:44,418 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41654
scm3.org_1   | 2021-07-12 13:55:44,460 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:44,481 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54548
scm3.org_1   | 2021-07-12 13:55:44,495 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59314
scm3.org_1   | 2021-07-12 13:55:44,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:55:44,604 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:14,415 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54728
scm3.org_1   | 2021-07-12 13:56:14,432 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:41836
scm3.org_1   | 2021-07-12 13:56:14,446 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:14,469 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59492
scm3.org_1   | 2021-07-12 13:56:14,493 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:14,505 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:44,465 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:54954
scm3.org_1   | 2021-07-12 13:56:44,468 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42074
scm3.org_1   | 2021-07-12 13:56:44,500 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:44,517 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:56:44,537 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59726
scm3.org_1   | 2021-07-12 13:56:44,541 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:03,495 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 2 containers.
scm3.org_1   | 2021-07-12 13:57:14,417 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42342
scm3.org_1   | 2021-07-12 13:57:14,434 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:14,552 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:59996
scm3.org_1   | 2021-07-12 13:57:14,554 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55232
scm3.org_1   | 2021-07-12 13:57:14,569 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:14,588 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:44,416 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42578
scm1.org_1   | 2021-07-12 14:00:34,282 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:34,283 [IPC Server handler 84 on default port 9860] INFO ipc.Server: IPC Server handler 84 on default port 9860, call Call#0 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.116:44230
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainer(SCMClientProtocolServer.java:214)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:425)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:190)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm1.org_1   | 2021-07-12 14:00:38,186 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44240
scm1.org_1   | 2021-07-12 14:00:38,210 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:42,609 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44262
scm1.org_1   | 2021-07-12 14:00:42,630 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:45,637 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59406
scm1.org_1   | 2021-07-12 14:00:45,652 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42120
scm1.org_1   | 2021-07-12 14:00:45,673 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:45,684 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:45,769 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54048
scm1.org_1   | 2021-07-12 14:00:45,784 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:00:46,964 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44300
scm1.org_1   | 2021-07-12 14:00:46,987 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:53,634 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44326
scm1.org_1   | 2021-07-12 14:00:53,650 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:00:53,661 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 910d9890-e228-42df-8573-868a0d6a1cfb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:00:53.652Z[UTC]].
scm1.org_1   | 2021-07-12 14:00:53,666 [IPC Server handler 27 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 14484.095us
scm1.org_1   | 2021-07-12 14:00:57,625 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44340
scm1.org_1   | 2021-07-12 14:00:57,647 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:01,530 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44360
scm1.org_1   | 2021-07-12 14:01:01,549 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:05,835 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44380
scm1.org_1   | 2021-07-12 14:01:05,863 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
om1_1        | 2021-07-12 13:58:35,534 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:35,574 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-97294, Key:27526/multidelete/f4.
om1_1        | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1        | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:133)
om1_1        | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:246)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:472)
om1_1        | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$2(OzoneManagerStateMachine.java:291)
om1_1        | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1        | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1        | 2021-07-12 13:58:36,085 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:36,087 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37600
om1_1        | 2021-07-12 13:58:36,090 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:39,785 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:50878
om1_1        | 2021-07-12 13:58:39,809 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:42,470 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:42,470 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37628
om1_1        | 2021-07-12 13:58:42,483 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:43,009 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:43,009 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37632
om1_1        | 2021-07-12 13:58:43,010 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:43,713 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:43,713 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37640
om1_1        | 2021-07-12 13:58:43,715 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:44,245 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:44,245 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37646
om1_1        | 2021-07-12 13:58:44,246 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:44,903 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:44,903 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37670
om1_1        | 2021-07-12 13:58:44,905 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:45,434 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:45,435 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37682
om1_1        | 2021-07-12 13:58:45,436 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:46,042 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:46,043 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37690
om1_1        | 2021-07-12 13:58:46,050 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:46,628 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:46,629 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37698
om1_1        | 2021-07-12 13:58:46,631 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:47,250 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:47,251 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37704
om1_1        | 2021-07-12 13:58:47,252 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:47,838 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:47,839 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37712
om1_1        | 2021-07-12 13:58:47,840 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:48,372 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:48,372 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37718
om1_1        | 2021-07-12 13:58:48,379 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:48,978 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:48,978 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37726
om1_1        | 2021-07-12 13:58:48,980 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:49,618 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:49,619 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37734
om1_1        | 2021-07-12 13:58:49,621 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:50,206 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:50,206 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37740
om1_1        | 2021-07-12 13:58:50,207 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:50,779 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:50,779 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37746
om1_1        | 2021-07-12 13:58:50,780 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:51,366 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:51,366 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37762
om1_1        | 2021-07-12 13:58:51,375 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:51,925 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:51,926 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37768
om1_1        | 2021-07-12 13:58:51,930 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:52,479 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:52,479 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37776
om1_1        | 2021-07-12 13:58:52,500 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:53,074 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:53,074 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37782
om1_1        | 2021-07-12 13:58:53,075 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:42,988 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidPart</Code>
s3g_1        |   <Message>One or more of the specified parts could not be found. The part might not have been uploaded, or the specified entity tag might not have matched the part's entity tag.</Message>
s3g_1        |   <Resource>19004/multipartKey3</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:92)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
om1_1        | 2021-07-12 13:58:53,572 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:53,572 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37786
om1_1        | 2021-07-12 13:58:53,573 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:54,075 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:58:54,075 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37792
om1_1        | 2021-07-12 13:58:54,078 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:58:57,798 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:51074
om1_1        | 2021-07-12 13:58:57,817 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:59:00,483 [Socket Reader #1 for port 9862] INFO security.AWSV4AuthValidator: 19b6393d405ba16619004277413d827319b966c8a01e2e0cdb6c21335d3563d6
om1_1        | 2021-07-12 13:59:00,483 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) from 172.25.0.114:37822
om1_1        | 2021-07-12 13:59:00,488 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 13:59:14,275 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:51136
scm1.org_1   | 2021-07-12 14:01:05,880 [IPC Server handler 21 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 15043.094us
scm1.org_1   | 2021-07-12 14:01:09,985 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44394
scm1.org_1   | 2021-07-12 14:01:10,002 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:14,280 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44414
scm1.org_1   | 2021-07-12 14:01:14,307 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:14,317 [IPC Server handler 52 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 8859.696us
scm1.org_1   | 2021-07-12 14:01:15,626 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59552
scm1.org_1   | 2021-07-12 14:01:15,635 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42268
scm1.org_1   | 2021-07-12 14:01:15,644 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:15,657 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:15,756 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54198
scm1.org_1   | 2021-07-12 14:01:15,778 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:18,172 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44452
scm1.org_1   | 2021-07-12 14:01:18,193 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:18,573 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 3 containers.
scm1.org_1   | 2021-07-12 14:01:20,547 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 8942.596us
scm1.org_1   | 2021-07-12 14:01:22,393 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44472
scm1.org_1   | 2021-07-12 14:01:22,413 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:22,423 [IPC Server handler 72 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9305.696us
scm1.org_1   | 2021-07-12 14:01:22,425 [IPC Server handler 72 on default port 9860] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 910d9890-e228-42df-8573-868a0d6a1cfb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:00:53.652Z[UTC]] moved to CLOSED state
scm1.org_1   | 2021-07-12 14:01:26,293 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44484
scm1.org_1   | 2021-07-12 14:01:26,320 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:32,856 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44520
scm1.org_1   | 2021-07-12 14:01:32,874 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:37,005 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44540
scm1.org_1   | 2021-07-12 14:01:37,022 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:41,869 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44564
scm1.org_1   | 2021-07-12 14:01:41,876 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:41,877 [IPC Server handler 21 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
scm1.org_1   | 2021-07-12 14:01:45,633 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42424
scm1.org_1   | 2021-07-12 14:01:45,641 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59708
scm1.org_1   | 2021-07-12 14:01:45,658 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:45,661 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:45,750 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54354
scm1.org_1   | 2021-07-12 14:01:45,786 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:01:46,021 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44596
scm1.org_1   | 2021-07-12 14:01:46,038 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:50,378 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44618
scm1.org_1   | 2021-07-12 14:01:50,381 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:01:50,391 [IPC Server handler 61 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 2021-07-12 14:01:50,406 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 3 containers.
scm1.org_1   | 2021-07-12 14:01:54,279 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44640
scm1.org_1   | 2021-07-12 14:01:54,302 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:01,055 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44670
scm1.org_1   | 2021-07-12 14:02:01,072 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:05,183 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44692
scm1.org_1   | 2021-07-12 14:02:05,205 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:09,107 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44700
om1_1        | 2021-07-12 13:59:14,293 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om1_1        | 2021-07-12 14:02:29,854 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:51030
om1_1        | 2021-07-12 14:02:29,885 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
scm3.org_1   | 2021-07-12 13:57:44,434 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55458
scm3.org_1   | 2021-07-12 13:57:44,437 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:44,504 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:60230
scm3.org_1   | 2021-07-12 13:57:44,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:57:44,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:14,406 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:42808
scm3.org_1   | 2021-07-12 13:58:14,420 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:14,424 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55702
scm3.org_1   | 2021-07-12 13:58:14,467 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:60468
scm3.org_1   | 2021-07-12 13:58:14,470 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:14,486 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:44,435 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43034
scm3.org_1   | 2021-07-12 13:58:44,439 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:55918
scm3.org_1   | 2021-07-12 13:58:44,452 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:44,485 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:60692
scm3.org_1   | 2021-07-12 13:58:44,496 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:58:44,524 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:14,413 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43264
scm3.org_1   | 2021-07-12 13:59:14,439 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:14,481 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:60922
scm3.org_1   | 2021-07-12 13:59:14,503 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56156
scm3.org_1   | 2021-07-12 13:59:14,514 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:14,523 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:24,790 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a6a86e95-d787-4a76-a3c8-90a7010e1026, Nodes: e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T13:59:24.764Z[UTC]].
scm3.org_1   | 2021-07-12 13:59:27,079 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56220
scm3.org_1   | 2021-07-12 13:59:27,121 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:44,427 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43426
scm3.org_1   | 2021-07-12 13:59:44,492 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:32848
scm3.org_1   | 2021-07-12 13:59:44,506 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:44,516 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 13:59:57,107 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56382
scm3.org_1   | 2021-07-12 13:59:57,130 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:14,413 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43564
scm3.org_1   | 2021-07-12 14:00:14,424 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:14,488 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:32986
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
scm3.org_1   | 2021-07-12 14:00:14,508 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:15,571 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
scm3.org_1   | 2021-07-12 14:00:15,574 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 844.5us
scm3.org_1   | 2021-07-12 14:00:15,583 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
scm3.org_1   | org.apache.ratis.protocol.exceptions.NotLeaderException: Server cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD is not the leader 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:212)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
scm3.org_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:242)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:227)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:96)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:88)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:40)
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm3.org_1   | 2021-07-12 14:00:15,630 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56468
scm3.org_1   | 2021-07-12 14:00:15,679 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:45,622 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56620
scm3.org_1   | 2021-07-12 14:00:45,628 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33138
scm3.org_1   | 2021-07-12 14:00:45,643 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:45,663 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:45,762 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43736
scm3.org_1   | 2021-07-12 14:00:45,771 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:00:53,668 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 910d9890-e228-42df-8573-868a0d6a1cfb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:00:53.652Z[UTC]].
scm3.org_1   | 2021-07-12 14:01:15,599 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33286
scm3.org_1   | 2021-07-12 14:01:15,632 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56768
scm3.org_1   | 2021-07-12 14:01:15,633 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:01:15,665 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:01:15,756 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:43886
scm3.org_1   | 2021-07-12 14:01:15,770 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:02:09,127 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:15,652 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42576
scm1.org_1   | 2021-07-12 14:02:15,661 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:02:15,671 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:59864
scm1.org_1   | 2021-07-12 14:02:15,683 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:02:15,789 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54510
scm1.org_1   | 2021-07-12 14:02:15,796 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm1.org_1   | 2021-07-12 14:02:15,948 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:44740
scm1.org_1   | 2021-07-12 14:02:15,966 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:20,552 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 13298.294us
scm1.org_1   | 2021-07-12 14:02:30,547 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:59760
scm1.org_1   | 2021-07-12 14:02:30,553 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm1.org_1   | 2021-07-12 14:02:30,614 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.112:33402
scm1.org_1   | 2021-07-12 14:02:30,615 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.111:34496
scm1.org_1   | 2021-07-12 14:02:30,618 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:30,623 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.113:57542
scm1.org_1   | 2021-07-12 14:02:30,628 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:30,639 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:02:39,403 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:54866
scm1.org_1   | 2021-07-12 14:02:39,422 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm1.org_1   | 2021-07-12 14:02:39,772 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38122
scm3.org_1   | 2021-07-12 14:01:41,908 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:51208
scm3.org_1   | 2021-07-12 14:01:41,912 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 2021-07-12 14:01:42,253 [IPC Server handler 10 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
scm3.org_1   | 2021-07-12 14:01:45,621 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33444
scm3.org_1   | 2021-07-12 14:01:45,625 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:56924
scm3.org_1   | 2021-07-12 14:01:45,652 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:01:45,655 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:01:45,741 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44042
scm3.org_1   | 2021-07-12 14:01:45,787 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:01:50,424 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.116:51262
scm3.org_1   | 2021-07-12 14:01:50,429 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 2021-07-12 14:01:50,431 [IPC Server handler 62 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm3.org_1   | 2021-07-12 14:01:50,434 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 3 containers.
scm3.org_1   | 2021-07-12 14:02:15,614 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33596
scm3.org_1   | 2021-07-12 14:02:15,645 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:02:15,655 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57072
scm3.org_1   | 2021-07-12 14:02:15,662 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:02:15,765 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44194
scm3.org_1   | 2021-07-12 14:02:15,787 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:02:41,922 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57212
scm3.org_1   | 2021-07-12 14:02:41,943 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:02:45,599 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33756
scm3.org_1   | 2021-07-12 14:02:45,614 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:02:45,731 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44346
scm3.org_1   | 2021-07-12 14:02:45,741 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:03:11,883 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57362
scm3.org_1   | 2021-07-12 14:03:11,912 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:03:12,933 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
scm3.org_1   | 2021-07-12 14:03:12,934 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@700873df, cost 761.9us
scm3.org_1   | 2021-07-12 14:03:12,935 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
scm3.org_1   | org.apache.ratis.protocol.exceptions.NotLeaderException: Server cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD is not the leader 1e6f42a1-e2d2-4f78-bf5d-162da394b7e9|rpc:scm1.org:9894|admin:|client:|dataStream:|priority:0
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)
scm1.org_1   | 2021-07-12 14:02:39,774 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)
scm1.org_1   | 2021-07-12 14:02:39,785 [IPC Server handler 25 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9835.396us
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)
scm1.org_1   | 2021-07-12 14:02:41,919 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60004
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)
scm1.org_1   | 2021-07-12 14:02:41,935 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)
scm1.org_1   | 2021-07-12 14:02:45,597 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42736
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
scm3.org_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)
scm1.org_1   | 2021-07-12 14:02:45,609 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)
scm1.org_1   | 2021-07-12 14:02:45,745 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54658
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)
scm1.org_1   | 2021-07-12 14:02:45,749 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
scm3.org_1   | 	at java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)
scm1.org_1   | 2021-07-12 14:02:46,128 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38168
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
scm1.org_1   | 2021-07-12 14:02:46,152 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
scm1.org_1   | 2021-07-12 14:02:50,104 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38188
scm3.org_1   | 	at org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
scm1.org_1   | 2021-07-12 14:02:50,125 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:212)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
scm1.org_1   | 2021-07-12 14:02:54,114 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38208
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm1.org_1   | 2021-07-12 14:02:54,141 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
scm1.org_1   | 2021-07-12 14:02:58,202 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38218
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
scm3.org_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm1.org_1   | 2021-07-12 14:02:58,225 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
scm1.org_1   | 2021-07-12 14:03:02,111 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38240
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:242)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm1.org_1   | 2021-07-12 14:03:02,141 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:227)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
scm1.org_1   | 2021-07-12 14:03:06,122 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38260
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:96)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
scm1.org_1   | 2021-07-12 14:03:06,137 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:88)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
scm3.org_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:40)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
scm1.org_1   | 2021-07-12 14:03:10,257 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38272
scm3.org_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm1.org_1   | 2021-07-12 14:03:10,274 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
scm1.org_1   | 2021-07-12 14:03:10,480 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2
scm3.org_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
scm1.org_1   | 2021-07-12 14:03:10,488 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 7354.997us
scm3.org_1   | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
scm1.org_1   | 2021-07-12 14:03:11,889 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60148
scm3.org_1   | 2021-07-12 14:03:13,004 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:33900
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
scm1.org_1   | 2021-07-12 14:03:11,915 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:03:13,005 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44482
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
scm1.org_1   | 2021-07-12 14:03:12,939 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode e7ed1272-c70a-4bce-ab84-6d03b85621dc{ip: 172.25.0.103, host: ozonesecure-ha_datanode2_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} reported CLOSED replica.
scm3.org_1   | 2021-07-12 14:03:13,015 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
scm1.org_1   | 2021-07-12 14:03:12,946 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:42878
scm3.org_1   | 2021-07-12 14:03:13,024 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
scm1.org_1   | 2021-07-12 14:03:12,958 [EventQueue-IncrementalContainerReportForIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.updateContainerState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$LifeCycleEvent) throws java.io.IOException,org.apache.hadoop.ozone.common.statemachine.InvalidStateTransitionException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 11691.295us
scm3.org_1   | 2021-07-12 14:03:42,948 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57522
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
scm1.org_1   | 2021-07-12 14:03:12,994 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:03:42,973 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
scm1.org_1   | 2021-07-12 14:03:13,001 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54798
scm3.org_1   | 2021-07-12 14:03:43,007 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44648
scm3.org_1   | 2021-07-12 14:03:43,014 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34054
scm3.org_1   | 2021-07-12 14:03:43,039 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm1.org_1   | 2021-07-12 14:03:13,024 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm3.org_1   | 2021-07-12 14:03:43,073 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 2021-07-12 13:56:43,658 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-52C13FAB9C5E->e7ed1272-c70a-4bce-ab84-6d03b85621dc
scm1.org_1   | 2021-07-12 14:03:14,648 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38310
scm3.org_1   | 2021-07-12 14:03:51,745 [cae79eb6-d521-4d9d-a048-244f8c0679d9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: de4c8d2c-187a-46f2-8f2d-2116a6d654eb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:03:51.730Z[UTC]].
s3g_1        | 2021-07-12 13:56:43,659 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 14:03:14,669 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 2021-07-12 14:04:12,933 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57666
s3g_1        | 2021-07-12 13:56:47,278 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-812618B2804A->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 14:03:18,763 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38332
scm3.org_1   | 2021-07-12 14:04:12,952 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 2021-07-12 13:56:47,279 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 14:03:18,780 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 2021-07-12 14:04:12,998 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34200
s3g_1        | 2021-07-12 13:56:48,432 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D96B785FA3DF->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
scm1.org_1   | 2021-07-12 14:03:20,546 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 8088.496us
scm3.org_1   | 2021-07-12 14:04:13,021 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44792
s3g_1        | 2021-07-12 13:56:48,432 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
scm1.org_1   | 2021-07-12 14:03:25,575 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38358
scm3.org_1   | 2021-07-12 14:04:13,027 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 2021-07-12 13:56:49,233 [qtp1771421544-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 19004/multipartKey3
scm1.org_1   | 2021-07-12 14:03:25,605 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm3.org_1   | 2021-07-12 14:04:13,031 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag1, 1}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567971869294625
scm1.org_1   | 2021-07-12 14:03:25,606 [IPC Server handler 27 on default port 9860] INFO ipc.Server: IPC Server handler 27 on default port 9860, call Call#0 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.117:38358
scm3.org_1   | 2021-07-12 14:04:39,006 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:34630
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
scm3.org_1   | 2021-07-12 14:04:39,016 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
scm3.org_1   | 2021-07-12 14:04:39,018 [IPC Server handler 22 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
scm3.org_1   | 2021-07-12 14:04:42,939 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57832
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainer(SCMClientProtocolServer.java:214)
scm3.org_1   | 2021-07-12 14:04:42,952 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:425)
scm3.org_1   | 2021-07-12 14:04:43,013 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:44956
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:190)
scm3.org_1   | 2021-07-12 14:04:43,031 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34364
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
scm3.org_1   | 2021-07-12 14:04:43,037 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
scm3.org_1   | 2021-07-12 14:04:43,066 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
scm3.org_1   | 2021-07-12 14:04:47,320 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:34684
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
scm3.org_1   | 2021-07-12 14:04:47,332 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
scm3.org_1   | 2021-07-12 14:04:47,335 [IPC Server handler 61 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
scm3.org_1   | 2021-07-12 14:04:47,341 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 0 milliseconds for processing 4 containers.
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
scm3.org_1   | 2021-07-12 14:05:12,945 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:57984
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3.org_1   | 2021-07-12 14:05:13,032 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:34516
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3.org_1   | 2021-07-12 14:05:13,061 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:45106
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm3.org_1   | 2021-07-12 14:05:13,065 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
scm1.org_1   | 2021-07-12 14:03:29,397 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:55124
scm3.org_1   | 2021-07-12 14:05:13,126 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
scm1.org_1   | 2021-07-12 14:03:29,414 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm3.org_1   | 2021-07-12 14:05:13,144 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
scm1.org_1   | 2021-07-12 14:03:29,709 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38380
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
scm1.org_1   | 2021-07-12 14:03:29,716 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
scm1.org_1   | 2021-07-12 14:03:29,717 [IPC Server handler 22 on default port 9860] INFO ipc.Server: IPC Server handler 22 on default port 9860, call Call#1 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.117:38380
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.allocateContainer(SCMClientProtocolServer.java:198)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.allocateContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:414)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:182)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
scm1.org_1   | 2021-07-12 14:03:33,434 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38402
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
scm1.org_1   | 2021-07-12 14:03:33,450 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
scm1.org_1   | 2021-07-12 14:03:33,450 [IPC Server handler 72 on default port 9860] INFO ipc.Server: IPC Server handler 72 on default port 9860, call Call#0 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.25.0.117:38402
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm1.org_1   | org.apache.hadoop.security.AccessControlException: Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1579)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getContainer(SCMClientProtocolServer.java:214)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:425)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:190)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
scm1.org_1   | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
scm1.org_1   | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:169)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
scm1.org_1   | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:55800)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:466)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:574)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
scm1.org_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:552)
scm1.org_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1035)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
scm1.org_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1.org_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1.org_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1.org_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
scm1.org_1   | 2021-07-12 14:03:37,108 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38420
scm1.org_1   | 2021-07-12 14:03:37,134 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
scm1.org_1   | 2021-07-12 14:03:41,043 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38432
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
scm1.org_1   | 2021-07-12 14:03:41,060 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
scm1.org_1   | 2021-07-12 14:03:42,943 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60308
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
scm1.org_1   | 2021-07-12 14:03:42,973 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
scm1.org_1   | 2021-07-12 14:03:43,004 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43036
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
scm1.org_1   | 2021-07-12 14:03:43,005 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:54958
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
scm1.org_1   | 2021-07-12 14:03:43,040 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
scm1.org_1   | 2021-07-12 14:03:43,057 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
scm1.org_1   | 2021-07-12 14:03:44,963 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38472
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
scm1.org_1   | 2021-07-12 14:03:44,984 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser2/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
scm1.org_1   | 2021-07-12 14:03:51,710 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38496
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
scm1.org_1   | 2021-07-12 14:03:51,729 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
scm1.org_1   | 2021-07-12 14:03:51,746 [1e6f42a1-e2d2-4f78-bf5d-162da394b7e9@group-EE404DDD1BCD-StateMachineUpdater] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: de4c8d2c-187a-46f2-8f2d-2116a6d654eb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:03:51.730Z[UTC]].
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
scm1.org_1   | 2021-07-12 14:03:51,747 [IPC Server handler 21 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 16779.393us
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
scm1.org_1   | 2021-07-12 14:03:55,904 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38518
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
scm1.org_1   | 2021-07-12 14:03:55,927 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:04:00,264 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38540
scm1.org_1   | 2021-07-12 14:04:00,289 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:04:04,393 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38552
scm1.org_1   | 2021-07-12 14:04:04,414 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm1.org_1   | 2021-07-12 14:04:04,423 [IPC Server handler 61 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 9219.296us
s3g_1        | 2021-07-12 13:56:49,234 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
scm1.org_1   | 2021-07-12 14:04:08,325 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38572
s3g_1        | <Error>
scm1.org_1   | 2021-07-12 14:04:08,345 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        |   <Code>InvalidPart</Code>
scm1.org_1   | 2021-07-12 14:04:12,539 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38592
s3g_1        |   <Message>One or more of the specified parts could not be found. The part might not have been uploaded, or the specified entity tag might not have matched the part's entity tag.</Message>
scm1.org_1   | 2021-07-12 14:04:12,559 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        |   <Resource>19004/multipartKey3</Resource>
scm1.org_1   | 2021-07-12 14:04:12,567 [IPC Server handler 70 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 7399.197us
s3g_1        |   <RequestId/>
scm1.org_1   | 2021-07-12 14:04:12,934 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60458
s3g_1        | </Error>
scm1.org_1   | 2021-07-12 14:04:12,966 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 
scm1.org_1   | 2021-07-12 14:04:13,010 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43182
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
scm1.org_1   | 2021-07-12 14:04:13,036 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:55104
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:92)
scm1.org_1   | 2021-07-12 14:04:13,046 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
scm1.org_1   | 2021-07-12 14:04:13,061 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm1.org_1   | 2021-07-12 14:04:16,458 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38622
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
scm1.org_1   | 2021-07-12 14:04:16,482 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
scm1.org_1   | 2021-07-12 14:04:20,523 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38642
scm1.org_1   | 2021-07-12 14:04:20,549 [SCMBlockDeletingService#0] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.block.DeletedBlockLogStateManager.removeTransactionsFromDB(java.util.ArrayList) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 11150.595us
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
scm1.org_1   | 2021-07-12 14:04:20,562 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
scm1.org_1   | 2021-07-12 14:04:20,573 [IPC Server handler 70 on default port 9860] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@fe1c5d4, cost 10446.896us
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
scm1.org_1   | 2021-07-12 14:04:20,573 [IPC Server handler 70 on default port 9860] INFO pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: de4c8d2c-187a-46f2-8f2d-2116a6d654eb, Nodes: 8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7{ip: 172.25.0.104, host: ozonesecure-ha_datanode3_1.ozonesecure-ha_ozone_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: STANDALONE/ONE, State:OPEN, leaderId:, CreationTimestamp2021-07-12T14:03:51.730Z[UTC]] moved to CLOSED state
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
scm1.org_1   | 2021-07-12 14:04:24,415 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38662
scm1.org_1   | 2021-07-12 14:04:24,433 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
scm1.org_1   | 2021-07-12 14:04:30,833 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38698
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
scm1.org_1   | 2021-07-12 14:04:30,853 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
scm1.org_1   | 2021-07-12 14:04:34,924 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38712
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
scm1.org_1   | 2021-07-12 14:04:34,960 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
scm1.org_1   | 2021-07-12 14:04:38,971 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38734
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
scm1.org_1   | 2021-07-12 14:04:38,980 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
scm1.org_1   | 2021-07-12 14:04:38,983 [IPC Server handler 80 on default port 9860] INFO container.ReplicationManager: Stopping Replication Monitor Thread.
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
scm1.org_1   | 2021-07-12 14:04:42,870 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38756
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
scm1.org_1   | 2021-07-12 14:04:42,890 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
scm1.org_1   | 2021-07-12 14:04:42,937 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60618
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
scm1.org_1   | 2021-07-12 14:04:42,951 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
scm1.org_1   | 2021-07-12 14:04:43,015 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:55268
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
scm1.org_1   | 2021-07-12 14:04:43,030 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43346
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
scm1.org_1   | 2021-07-12 14:04:43,037 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
scm1.org_1   | 2021-07-12 14:04:43,046 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
scm1.org_1   | 2021-07-12 14:04:47,278 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38788
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
scm1.org_1   | 2021-07-12 14:04:47,296 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
scm1.org_1   | 2021-07-12 14:04:47,299 [IPC Server handler 41 on default port 9860] INFO container.ReplicationManager: Starting Replication Monitor Thread.
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
scm1.org_1   | 2021-07-12 14:04:47,306 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 4 containers.
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
scm1.org_1   | 2021-07-12 14:04:51,224 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38810
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 2021-07-12 14:04:51,243 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
scm1.org_1   | 2021-07-12 14:04:57,810 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38836
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
scm1.org_1   | 2021-07-12 14:04:57,833 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 2021-07-12 14:05:01,798 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38858
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
scm1.org_1   | 2021-07-12 14:05:01,817 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1.org_1   | 2021-07-12 14:05:05,765 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38870
scm1.org_1   | 2021-07-12 14:05:05,781 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm1.org_1   | 2021-07-12 14:05:12,063 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.117:38904
scm1.org_1   | 2021-07-12 14:05:12,077 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 2021-07-12 14:05:12,950 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.103:60768
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
scm1.org_1   | 2021-07-12 14:05:13,045 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
scm1.org_1   | 2021-07-12 14:05:13,074 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.102:55418
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
scm1.org_1   | 2021-07-12 14:05:13,087 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) from 172.25.0.104:43498
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
scm1.org_1   | 2021-07-12 14:05:13,106 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
scm1.org_1   | 2021-07-12 14:05:13,145 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/dn@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:49,783 [qtp1771421544-23] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 19004/multipartKey3
s3g_1        | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3. Provided Part info is { etag2, 2}, whereas OM has partName /s3v/bucket-16724/19004/multipartKey3106567972107190306
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:49,784 [qtp1771421544-23] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidPart</Code>
s3g_1        |   <Message>One or more of the specified parts could not be found. The part might not have been uploaded, or the specified entity tag might not have matched the part's entity tag.</Message>
s3g_1        |   <Resource>19004/multipartKey3</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:92)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:50,354 [qtp1771421544-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-16724, , key: 19004/multipartKey3
s3g_1        | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: bucket-16724 key: 19004/multipartKey3 because parts are in Invalid order.
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:613)
s3g_1        | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:985)
s3g_1        | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:1075)
s3g_1        | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:691)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:531)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:50,356 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidPartOrder</Code>
s3g_1        |   <Message>The list of parts was not in ascending order. The parts list must be specified in order by part number.</Message>
s3g_1        |   <Resource>19004/multipartKey3</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:97)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:53,698 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchUpload</Code>
s3g_1        |   <Message>The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed.</Message>
s3g_1        |   <Resource>random</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:83)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:54,258 [qtp1771421544-23] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchUpload</Code>
s3g_1        |   <Message>The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed.</Message>
s3g_1        |   <Resource>random</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:83)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:56:55,619 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2BEE0E827F92->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:55,623 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:56:56,654 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-BFBA019B713D->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:56:56,654 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:00,447 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-2E6E85E8F035->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:00,449 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:00,576 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-804833BF63C5->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:00,576 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:00,597 [qtp1771421544-23] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4BDF0BFE35B2->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:57:00,597 [qtp1771421544-23] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:07,972 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-5EE1142E0689->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:07,973 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:12,140 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-54FDF540DBBA->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:12,140 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:17,088 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DEA7A686EAE1->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:57:17,088 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:19,228 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DCEDFAC4B30F->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:19,228 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:23,204 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C674A0D0B8F8->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:57:23,204 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:28,123 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-937CA1F2ABDD->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:28,123 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:32,589 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>PreconditionFailed</Code>
s3g_1        |   <Message>At least one of the pre-conditions you specified did not hold</Message>
s3g_1        |   <Resource>bucket-16724/58905/copyrange/source</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:115)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:57:33,186 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>PreconditionFailed</Code>
s3g_1        |   <Message>At least one of the pre-conditions you specified did not hold</Message>
s3g_1        |   <Resource>bucket-16724/58905/copyrange/source</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:115)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:57:33,895 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7C9062B74AA0->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:33,896 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:37,635 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-DDC2FCFE88BA->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:37,636 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:50,470 [qtp1771421544-22] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-01782, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:57:50,489 [qtp1771421544-22] INFO endpoint.BucketEndpoint: Location is /bucket-01782
s3g_1        | 2021-07-12 13:57:51,010 [qtp1771421544-17] INFO rpc.RpcClient: Creating Bucket: s3v/destbucket-29904, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:57:51,029 [qtp1771421544-17] INFO endpoint.BucketEndpoint: Location is /destbucket-29904
s3g_1        | 2021-07-12 13:57:51,623 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A8D9EF309EBC->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:57:51,624 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:52,861 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-79F5D2037F0A->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:57:52,861 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:57:56,625 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D4297BC1C330->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:57:56,625 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:00,273 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchBucket</Code>
s3g_1        |   <Message>The specified bucket does not exist</Message>
s3g_1        |   <Resource>dfdfdfdfdfnonexistent</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:00,790 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchBucket</Code>
s3g_1        |   <Message>The specified bucket does not exist</Message>
s3g_1        |   <Resource>dfdfdfdfdfnonexistent</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:01,844 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchKey</Code>
s3g_1        |   <Message>The specified key does not exist</Message>
s3g_1        |   <Resource>nonnonexistentkey</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:70)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:08,435 [qtp1771421544-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-99740, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:58:08,451 [qtp1771421544-17] INFO endpoint.BucketEndpoint: Location is /bucket-99740
s3g_1        | 2021-07-12 13:58:09,039 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C04EF3028629->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:58:09,039 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:12,816 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-7A8651672D0E->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:58:12,816 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:15,711 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-926C7B80E2AB->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:58:15,711 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:20,848 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>NoSuchBucket</Code>
s3g_1        |   <Message>The specified bucket does not exist</Message>
s3g_1        |   <Resource>bucket-99740-nosuchbucket</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:51)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:27,511 [qtp1771421544-22] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-97294, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:58:27,521 [qtp1771421544-22] INFO endpoint.BucketEndpoint: Location is /bucket-97294
s3g_1        | 2021-07-12 13:58:28,119 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-E4FE6378B85E->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:58:28,119 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:31,246 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A866503C9BEA->e7ed1272-c70a-4bce-ab84-6d03b85621dc
s3g_1        | 2021-07-12 13:58:31,247 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:34,396 [qtp1771421544-17] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FA4F30C2A13D->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:58:34,396 [qtp1771421544-17] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:42,488 [qtp1771421544-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-76259, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:58:42,508 [qtp1771421544-17] INFO endpoint.BucketEndpoint: Location is /bucket-76259
s3g_1        | 2021-07-12 13:58:43,054 [qtp1771421544-22] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C5129AA9B387->8d7bb2a9-2192-43c8-b506-5e7fa5cd9bf7
s3g_1        | 2021-07-12 13:58:43,054 [qtp1771421544-22] WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.
s3g_1        | 2021-07-12 13:58:47,857 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidRange</Code>
s3g_1        |   <Message>The requested range is not satisfiable</Message>
s3g_1        |   <Resource>bytes=10000-10000</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:79)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:53,081 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidRange</Code>
s3g_1        |   <Message>The requested range is not satisfiable</Message>
s3g_1        |   <Resource>bytes=0-0</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:79)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:53,588 [qtp1771421544-22] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidRange</Code>
s3g_1        |   <Message>The requested range is not satisfiable</Message>
s3g_1        |   <Resource>bytes=0-1</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:79)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:58:54,088 [qtp1771421544-17] ERROR exception.S3ErrorTable: <?xml version="1.0" encoding="UTF-8"?>
s3g_1        | <Error>
s3g_1        |   <Code>InvalidRange</Code>
s3g_1        |   <Message>The requested range is not satisfiable</Message>
s3g_1        |   <Resource>bytes=0-10000</Resource>
s3g_1        |   <RequestId/>
s3g_1        | </Error>
s3g_1        | 
s3g_1        | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1        | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.<clinit>(S3ErrorTable.java:79)
s3g_1        | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:247)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1        | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1        | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1        | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
s3g_1        | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
s3g_1        | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
s3g_1        | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
s3g_1        | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
s3g_1        | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
s3g_1        | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
s3g_1        | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
s3g_1        | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
s3g_1        | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1452)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
s3g_1        | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.ozone.s3.EmptyContentTypeFilter.doFilter(EmptyContentTypeFilter.java:76)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:201)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1667)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1        | 	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1        | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1        | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
s3g_1        | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1        | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)
s3g_1        | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1        | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1        | 	at org.eclipse.jetty.server.Server.handle(Server.java:516)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
s3g_1        | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
s3g_1        | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
s3g_1        | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1        | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
s3g_1        | 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1        | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1        | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:773)
s3g_1        | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:905)
s3g_1        | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1        | 2021-07-12 13:59:00,491 [qtp1771421544-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-85854, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1        | 2021-07-12 13:59:00,500 [qtp1771421544-17] INFO endpoint.BucketEndpoint: Location is /bucket-85854
