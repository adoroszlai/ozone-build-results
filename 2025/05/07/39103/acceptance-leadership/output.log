Using Docker Compose v2
Executing test ozonesecure-ha/test-leadership.sh
Using Docker Compose v2
fatal error: concurrent map writes

goroutine 28 [running]:
github.com/docker/compose/v2/pkg/compose.(*composeService).pullRequiredImages.func1.1()
	github.com/docker/compose/v2/pkg/compose/pull.go:328 +0x236
golang.org/x/sync/errgroup.(*Group).Go.func1()
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:79 +0x50
created by golang.org/x/sync/errgroup.(*Group).Go in goroutine 23
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:76 +0x96

goroutine 1 [semacquire]:
sync.runtime_Semacquire(0xc000889c38?)
	runtime/sema.go:71 +0x25
sync.(*WaitGroup).Wait(0x25c5480?)
	sync/waitgroup.go:118 +0x48
golang.org/x/sync/errgroup.(*Group).Wait(0xc000a09000)
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:56 +0x25
github.com/docker/compose/v2/pkg/progress.RunWithStatus({0x2ba9388, 0xc00071abd0}, 0xc00068cd80, 0xc0005c3200, {0x27c8359, 0x7})
	github.com/docker/compose/v2/pkg/progress/writer.go:97 +0x225
github.com/docker/compose/v2/pkg/progress.Run({0x2ba9388, 0xc00071abd0}, 0xc000a08fc0, 0xc0005c3200)
	github.com/docker/compose/v2/pkg/progress/writer.go:61 +0x85
github.com/docker/compose/v2/pkg/compose.(*composeService).Up(0xc000282b40, {0x2ba9388, 0xc00071abd0}, _, {{0xc0003f8880, {0xc0003537c0, 0x0, 0x1}, 0x0, 0x0, ...}, ...})
	github.com/docker/compose/v2/pkg/compose/up.go:40 +0x213
github.com/docker/compose/v2/cmd/compose.runUp({_, _}, {_, _}, {_, _}, {0x0, 0x0, {0x27c670c, 0x6}, ...}, ...)
	github.com/docker/compose/v2/cmd/compose/up.go:319 +0xb54
github.com/docker/compose/v2/cmd/compose.upCommand.func2({0x2ba9388, 0xc00071abd0}, 0xc00085f950, {0xc0003537c0, 0x0, 0x1})
	github.com/docker/compose/v2/cmd/compose/up.go:143 +0x29f
github.com/docker/compose/v2/cmd/compose.upCommand.(*ProjectOptions).WithServices.func5({0x2ba93c0, 0xc0003b0280}, {0xc0003537c0, 0x0, 0x1})
	github.com/docker/compose/v2/cmd/compose/compose.go:187 +0x22d
github.com/docker/compose/v2/cmd/compose.upCommand.(*ProjectOptions).WithServices.Adapt.func7({0x2ba93c0?, 0xc0003b0280?}, 0x2?, {0xc0003537c0?, 0x2b8dfe8?, 0x1db4493?})
	github.com/docker/compose/v2/cmd/compose/compose.go:137 +0x30
github.com/docker/compose/v2/cmd/compose.upCommand.(*ProjectOptions).WithServices.Adapt.AdaptCmd.func8(0xc0001ecf08, {0xc0003537c0, 0x0, 0x1})
	github.com/docker/compose/v2/cmd/compose/compose.go:121 +0x143
github.com/docker/cli/cli-plugins/plugin.RunPlugin.func1.1.2(0xc0001ecf08, {0xc0003537c0, 0x0, 0x1})
	github.com/docker/cli@v28.1.0+incompatible/cli-plugins/plugin/plugin.go:65 +0x6c
github.com/docker/compose/v2/cmd/cmdtrace.Setup.wrapRunE.func2(0xc0001ecf08?, {0xc0003537c0?, 0x0?, 0x1?})
	github.com/docker/compose/v2/cmd/cmdtrace/cmd_span.go:85 +0x63
github.com/spf13/cobra.(*Command).execute(0xc0001ecf08, {0xc000277f00, 0x1, 0x1})
	github.com/spf13/cobra@v1.9.1/command.go:1015 +0xa94
github.com/spf13/cobra.(*Command).ExecuteC(0xc000176f08)
	github.com/spf13/cobra@v1.9.1/command.go:1148 +0x40c
github.com/spf13/cobra.(*Command).Execute(...)
	github.com/spf13/cobra@v1.9.1/command.go:1071
github.com/docker/cli/cli-plugins/plugin.RunPlugin(0xc0003cc780, 0xc0001ec908, {{0x27c5150, 0x5}, {0x27cf09e, 0xb}, {0x2b790c8, 0x7}, {0x0, 0x0}, ...})
	github.com/docker/cli@v28.1.0+incompatible/cli-plugins/plugin/plugin.go:80 +0x145
github.com/docker/cli/cli-plugins/plugin.Run(0x29213b0, {{0x27c5150, 0x5}, {0x27cf09e, 0xb}, {0x2b790c8, 0x7}, {0x0, 0x0}, {0x0, ...}})
	github.com/docker/cli@v28.1.0+incompatible/cli-plugins/plugin/plugin.go:95 +0x105
main.pluginMain()
	github.com/docker/compose/v2/cmd/main.go:38 +0xa5
main.main()
	github.com/docker/compose/v2/cmd/main.go:98 +0x19c

goroutine 10 [IO wait]:
internal/poll.runtime_pollWait(0x7f8b992c66b0, 0x72)
	runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000051680?, 0xc000088fbf?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000051680, {0xc000088fbf, 0x1, 0x1})
	internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000051680, {0xc000088fbf?, 0x0?, 0x0?})
	net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0005b8270, {0xc000088fbf?, 0x0?, 0x0?})
	net/net.go:189 +0x45
github.com/docker/cli/cli-plugins/socket.ConnectAndWait.func1()
	github.com/docker/cli@v28.1.0+incompatible/cli-plugins/socket/socket.go:162 +0x45
created by github.com/docker/cli/cli-plugins/socket.ConnectAndWait in goroutine 1
	github.com/docker/cli@v28.1.0+incompatible/cli-plugins/socket/socket.go:159 +0x118

goroutine 12 [IO wait]:
internal/poll.runtime_pollWait(0x7f8b992c6598, 0x72)
	runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000051c00?, 0xc00017b000?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000051c00, {0xc00017b000, 0x1000, 0x1000})
	internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000051c00, {0xc00017b000?, 0x0?, 0x2b805a0?})
	net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0005b8350, {0xc00017b000?, 0x0?, 0x0?})
	net/net.go:189 +0x45
net/http.(*persistConn).Read(0xc0005c1680, {0xc00017b000?, 0x777f25?, 0x2379d40?})
	net/http/transport.go:2052 +0x4a
bufio.(*Reader).fill(0xc0004018c0)
	bufio/bufio.go:110 +0x103
bufio.(*Reader).Peek(0xc0004018c0, 0x1)
	bufio/bufio.go:148 +0x53
net/http.(*persistConn).readLoop(0xc0005c1680)
	net/http/transport.go:2205 +0x185
created by net/http.(*Transport).dialConn in goroutine 11
	net/http/transport.go:1874 +0x154f

goroutine 13 [select]:
net/http.(*persistConn).writeLoop(0xc0005c1680)
	net/http/transport.go:2519 +0xe7
created by net/http.(*Transport).dialConn in goroutine 11
	net/http/transport.go:1875 +0x15a5

goroutine 49 [select]:
go.opentelemetry.io/otel/sdk/trace.(*batchSpanProcessor).processQueue(0xc00047c0a0)
	go.opentelemetry.io/otel/sdk@v1.34.0/trace/batch_span_processor.go:302 +0x114
go.opentelemetry.io/otel/sdk/trace.NewBatchSpanProcessor.func1()
	go.opentelemetry.io/otel/sdk@v1.34.0/trace/batch_span_processor.go:117 +0x4e
created by go.opentelemetry.io/otel/sdk/trace.NewBatchSpanProcessor in goroutine 1
	go.opentelemetry.io/otel/sdk@v1.34.0/trace/batch_span_processor.go:115 +0x2e5

goroutine 34 [syscall]:
os/signal.signal_recv()
	runtime/sigqueue.go:152 +0x29
os/signal.loop()
	os/signal/signal_unix.go:23 +0x13
created by os/signal.Notify.func1.1 in goroutine 1
	os/signal/signal.go:151 +0x1f

goroutine 51 [chan receive]:
github.com/docker/compose/v2/cmd/compose.upCommand.AdaptCmd.func4.1()
	github.com/docker/compose/v2/cmd/compose/compose.go:115 +0x27
created by github.com/docker/compose/v2/cmd/compose.upCommand.AdaptCmd.func4 in goroutine 1
	github.com/docker/compose/v2/cmd/compose/compose.go:114 +0x10a

goroutine 52 [chan receive]:
github.com/docker/compose/v2/cmd/compose.upCommand.(*ProjectOptions).WithServices.Adapt.AdaptCmd.func8.1()
	github.com/docker/compose/v2/cmd/compose/compose.go:115 +0x27
created by github.com/docker/compose/v2/cmd/compose.upCommand.(*ProjectOptions).WithServices.Adapt.AdaptCmd.func8 in goroutine 1
	github.com/docker/compose/v2/cmd/compose/compose.go:114 +0x10a

goroutine 15 [semacquire]:
sync.runtime_Semacquire(0xc000744558?)
	runtime/sema.go:71 +0x25
sync.(*WaitGroup).Wait(0x25c5480?)
	sync/waitgroup.go:118 +0x48
golang.org/x/sync/errgroup.(*Group).Wait(0xc0009839c0)
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:56 +0x25
github.com/docker/compose/v2/pkg/progress.RunWithStatus({0x2ba9388, 0xc000a0fdd0}, 0xc0006d5660, 0xc0005c3200, {0x27c8359, 0x7})
	github.com/docker/compose/v2/pkg/progress/writer.go:97 +0x225
github.com/docker/compose/v2/pkg/progress.Run({0x2ba9388, 0xc000a0fdd0}, 0xc000a0fe30, 0xc0005c3200)
	github.com/docker/compose/v2/pkg/progress/writer.go:61 +0x85
github.com/docker/compose/v2/pkg/compose.(*composeService).pullRequiredImages(0xc000282b40, {0x2ba9388, 0xc000a0fdd0}, 0xc00085f950, 0xc00071bb90, 0x0)
	github.com/docker/compose/v2/pkg/compose/pull.go:320 +0x2ef
github.com/docker/compose/v2/pkg/compose.(*composeService).ensureImagesExists.func1({0x2ba9388?, 0xc000a0fdd0?})
	github.com/docker/compose/v2/pkg/compose/build.go:278 +0x37
github.com/docker/compose/v2/pkg/compose.(*composeService).ensureImagesExists.SpanWrapFunc.func3({0x2ba9388, 0xc00071bad0})
	github.com/docker/compose/v2/internal/tracing/wrap.go:43 +0x13d
github.com/docker/compose/v2/pkg/compose.(*composeService).ensureImagesExists(0xc000282b40, {0x2ba9388, 0xc00071bad0}, 0xc00085f950, 0xc0003f8880, 0x0)
	github.com/docker/compose/v2/pkg/compose/build.go:280 +0x29e
github.com/docker/compose/v2/pkg/compose.(*composeService).create(0xc000282b40, {0x2ba9388, 0xc00071bad0}, 0xc00085f950, {0xc0003f8880, {0xc0001cd900, 0xe, 0x10}, 0x0, 0x0, ...})
	github.com/docker/compose/v2/pkg/compose/create.go:83 +0xdf
github.com/docker/compose/v2/pkg/compose.(*composeService).Up.func1({0x2ba9388, 0xc00071bad0})
	github.com/docker/compose/v2/pkg/compose/up.go:41 +0x85
github.com/docker/compose/v2/pkg/compose.(*composeService).Up.SpanWrapFunc.func5({0x2ba9388, 0xc00071ba70})
	github.com/docker/compose/v2/internal/tracing/wrap.go:43 +0x13d
github.com/docker/compose/v2/pkg/progress.Run.func1({0x2ba9388?, 0xc00071ba70?})
	github.com/docker/compose/v2/pkg/progress/writer.go:62 +0x22
github.com/docker/compose/v2/pkg/progress.RunWithStatus.func2()
	github.com/docker/compose/v2/pkg/progress/writer.go:90 +0x70
golang.org/x/sync/errgroup.(*Group).Go.func1()
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:79 +0x50
created by golang.org/x/sync/errgroup.(*Group).Go in goroutine 1
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:76 +0x96

goroutine 23 [semacquire]:
sync.runtime_Semacquire(0x3?)
	runtime/sema.go:71 +0x25
sync.(*WaitGroup).Wait(0xc000a0fe00?)
	sync/waitgroup.go:118 +0x48
golang.org/x/sync/errgroup.(*Group).Wait(0xc000983a40)
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:56 +0x25
github.com/docker/compose/v2/pkg/compose.(*composeService).pullRequiredImages.func1({0x2ba9388, 0xc000a0fe60})
	github.com/docker/compose/v2/pkg/compose/pull.go:340 +0x39b
github.com/docker/compose/v2/pkg/progress.Run.func1({0x2ba9388?, 0xc000a0fe60?})
	github.com/docker/compose/v2/pkg/progress/writer.go:62 +0x22
github.com/docker/compose/v2/pkg/progress.RunWithStatus.func2()
	github.com/docker/compose/v2/pkg/progress/writer.go:90 +0x70
golang.org/x/sync/errgroup.(*Group).Go.func1()
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:79 +0x50
created by golang.org/x/sync/errgroup.(*Group).Go in goroutine 15
	golang.org/x/sync@v0.13.0/errgroup/errgroup.go:76 +0x96

goroutine 68 [IO wait]:
internal/poll.runtime_pollWait(0x7f8b992c6480, 0x72)
	runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00028a000?, 0xc000173000?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00028a000, {0xc000173000, 0x1000, 0x1000})
	internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00028a000, {0xc000173000?, 0x0?, 0x2b805a0?})
	net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0005b8020, {0xc000173000?, 0x0?, 0x0?})
	net/net.go:189 +0x45
net/http.(*persistConn).Read(0xc0000246c0, {0xc000173000?, 0x777f25?, 0x2379d40?})
	net/http/transport.go:2052 +0x4a
bufio.(*Reader).fill(0xc000942060)
	bufio/bufio.go:110 +0x103
bufio.(*Reader).Peek(0xc000942060, 0x1)
	bufio/bufio.go:148 +0x53
net/http.(*persistConn).readLoop(0xc0000246c0)
	net/http/transport.go:2205 +0x185
created by net/http.(*Transport).dialConn in goroutine 67
	net/http/transport.go:1874 +0x154f

goroutine 69 [select]:
net/http.(*persistConn).writeLoop(0xc0000246c0)
	net/http/transport.go:2519 +0xe7
created by net/http.(*Transport).dialConn in goroutine 67
	net/http/transport.go:1875 +0x15a5
ERROR: Test execution of ozonesecure-ha/test-leadership.sh is FAILED!!!!
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha'
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/leadership'
mv: cannot stat 'ozonesecure-ha/result/*': No such file or directory
Executing test ozonesecure-ha/test-scm-decommission.sh
Using Docker Compose v2

Port 88 is not available on kdc yet
Port 88 is available on kdc

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet

Port 9860 is not available on scm1.org yet
Port 9860 is available on scm1.org
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 233
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 232
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 231
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 230
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 229
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 228
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 227
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 226
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 225
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 224
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 223
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 222
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 221
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 220
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 219
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 218
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 217
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 216
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 215
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 214
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 213
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 212
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 211
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 210
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 209
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 208
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 207
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 206
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 205
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 204
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 203
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 202
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 201
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 200
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 199
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 198
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 197
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 196
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 195
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 194
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 193
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 192
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 191
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 190
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 189
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 188
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 187
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 186
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 185
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 184
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 183
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 182
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 181
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 180
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 179
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 178
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 177
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 176
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 175
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 174
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 173
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 172
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 171
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 170
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 169
SCM is out of safe mode.
Found OM leader for service omservice: om2 : LEADER (om2)
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode1-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode2-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode3-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-httpfs-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om1-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om2-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om3-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-recon-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-s3g-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm1.org-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm2.org-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm3.org-1

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet

Port 9894 is not available on scm4.org yet
Port 9894 is available on scm4.org
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-001.xml

ozone admin scm roles | grep scm4.org hasn't succeed yet
scm4.org:9894:FOLLOWER:65ea7a57-4ee2-40cd-8b80-11dea46de6da:172.25.0.220
ozone admin scm roles | grep scm4.org succeed
==============================================================================
Primordial-Scm :: Smoketest ozone cluster startup                             
==============================================================================
Verify SCM Count                                                      | PASS |
------------------------------------------------------------------------------
Transfer Leader to SCM4                                               | PASS |
------------------------------------------------------------------------------
Verify SCM4 Certificate                                               | PASS |
------------------------------------------------------------------------------
Primordial-Scm :: Smoketest ozone cluster startup                     | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-002.xml

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet

Port 9856 is not available on datanode4 yet
Port 9856 is available on datanode4
Datanode: 26397f44-5d16-4a9b-b552-ad1eb606fc99 (/default-rack/172.25.0.221/ozonesecure-ha-datanode4-1.ozonesecure-ha_ozone_net/1 pipelines)
ozone admin datanode list | grep datanode4 succeed
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-003.xml
Decommissioned Scm 65ea7a57-4ee2-40cd-8b80-11dea46de6da
ozone admin scm decommission --nodeid=65ea7a57-4ee2-40cd-8b80-11dea46de6da | grep Decommissioned succeed
==============================================================================
Scm-Decommission :: Test Ozone SCM Decommissioning                            
==============================================================================
Decommission SCM Primordial Node                                      | PASS |
------------------------------------------------------------------------------
Scm-Decommission :: Test Ozone SCM Decommissioning                    | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-004.xml
To use Ozone please mount ozone folder to /opt/hadoop
Output:  /rebot-output/ozonesecure-ha-scm-decommission.xml
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/rebot-wLZbDM/ozonesecure-ha-scm-decommission.xml' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha-scm-decommission.xml'
removed 'ozonesecure-ha/result/robot-001.xml'
removed 'ozonesecure-ha/result/robot-002.xml'
removed 'ozonesecure-ha/result/robot-003.xml'
removed 'ozonesecure-ha/result/robot-004.xml'
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission'
renamed 'ozonesecure-ha/result/dn-audit-136f6cd9bb14.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/dn-audit-136f6cd9bb14.log'
renamed 'ozonesecure-ha/result/dn-audit-2f254468dfcf.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/dn-audit-2f254468dfcf.log'
renamed 'ozonesecure-ha/result/dn-audit-4dc396848d1b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/dn-audit-4dc396848d1b.log'
renamed 'ozonesecure-ha/result/dn-audit-d8c8182ae289.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/dn-audit-d8c8182ae289.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-datanode1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-datanode2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-datanode3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode4-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-datanode4-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-httpfs-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-httpfs-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kdc-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-kdc-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kms-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-kms-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-om1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-om2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-om3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-recon-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-recon-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-s3g-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-s3g-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm1.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-scm1.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm2.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-scm2.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm3.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-scm3.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm4.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/docker-ozonesecure-ha-scm4.org-1.log'
renamed 'ozonesecure-ha/result/kms-audit.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/kms-audit.log'
renamed 'ozonesecure-ha/result/om-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-audit-om1.log'
renamed 'ozonesecure-ha/result/om-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-audit-om2.log'
renamed 'ozonesecure-ha/result/om-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-audit-om3.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-sys-audit-om1.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-sys-audit-om2.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/om-sys-audit-om3.log'
renamed 'ozonesecure-ha/result/s3g-audit-s3g.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/s3g-audit-s3g.log'
renamed 'ozonesecure-ha/result/scm-audit-scm1.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/scm-audit-scm1.org.log'
renamed 'ozonesecure-ha/result/scm-audit-scm2.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/ozonesecure-ha/scm-decommission/scm-audit-scm2.org.log'
To use Ozone please mount ozone folder to /opt/hadoop
Log:     /rebot-output/log.html
Report:  /rebot-output/report.html
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/rebot-XkYdFN/log.html' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/log.html'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/rebot-XkYdFN/report.html' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.1.0-SNAPSHOT/compose/result/report.html'
removed directory '/tmp/robot-data-m5Y35O'
