No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8' to the OZONE_OPTS
2025-12-18 17:04:21,123 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting OzoneManager
STARTUP_MSG:        host = om/172.18.0.2
STARTUP_MSG:     version = 2.2.0-SNAPSHOT
STARTUP_MSG:       build = https://github.com/apache/ozone/1c817b0460f1e66e75aef0b8f5fde63c691f2a65
STARTUP_MSG:        java = 21.0.2
STARTUP_MSG:        args = [--init]
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.16.2.jar:/opt/hadoop/share/ozone/lib/guava-33.5.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.3.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jspecify-1.0.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.29.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-3.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-3.25.8.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.18.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.18.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.77.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.77.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.24.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.77.0.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.77.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.27.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/grpc-util-1.77.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.77.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-2.1.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.27.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.18.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.4.2.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.14.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-10.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.4.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.11.1.jar:/opt/hadoop/share/ozone/lib/commons-collections4-4.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.5.4.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.13.1.jar:/opt/hadoop/share/ozone/lib/re2j-1.7.jar:/opt/hadoop/share/ozone/lib/gson-2.13.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.55.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-epoll-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-classes-epoll-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.4.2.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.16.2.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.10.0.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.11.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentelemetry-api-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-context-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-otlp-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-metrics-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-logs-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-otlp-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-sender-okhttp-1.57.0.jar:/opt/hadoop/share/ozone/lib/okhttp-jvm-5.3.2.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.16.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.25.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-extension-autoconfigure-spi-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-trace-1.57.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/bcutil-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.2.1.jar:/opt/hadoop/share/ozone/lib/jcl-over-slf4j-2.0.17.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-2.0.17.jar:/opt/hadoop/share/ozone/lib/hdds-config-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.26.jar:/opt/hadoop/share/ozone/lib/jnr-constants-0.10.4.jar:/opt/hadoop/share/ozone/lib/jnr-posix-3.1.21.jar:/opt/hadoop/share/ozone/lib/jnr-ffi-2.2.18.jar:/opt/hadoop/share/ozone/lib/jffi-1.3.14.jar:/opt/hadoop/share/ozone/lib/jffi-1.3.14-native.jar:/opt/hadoop/share/ozone/lib/asm-commons-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-analysis-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-tree-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-util-9.7.1.jar:/opt/hadoop/share/ozone/lib/jnr-a64asm-1.0.0.jar:/opt/hadoop/share/ozone/lib/jnr-x86asm-1.0.2.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.6.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.16.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.47.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.47.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.47.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.47.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.4.jar:/opt/hadoop/share/ozone/lib/apache-log4j-extras-1.2.17.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.25.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.10.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.77.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.59.2.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.77.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.10.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.24.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/reflections-0.10.2.jar:/opt/hadoop/share/ozone/lib/javassist-3.30.2-GA.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/slf4j-api-2.0.17.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.74.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/asm-9.8.jar:/opt/hadoop/share/ozone/lib/ozone-manager-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-base-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-json-provider-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.16.2.jar:/opt/hadoop/share/ozone/lib/gethostname4j-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.6.0.jar:/opt/hadoop/share/ozone/lib/aws-java-sdk-logs-1.12.765.jar:/opt/hadoop/share/ozone/lib/aws-java-sdk-core-1.12.788.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-cbor-2.16.2.jar:/opt/hadoop/share/ozone/lib/jmespath-java-1.12.765.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/aircompressor-0.27.jar:/opt/hadoop/share/ozone/lib/joda-time-2.12.7.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.4.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.16.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.13.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/orc-shims-1.5.8.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.6.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ozone-multitenancy-ranger-2.2.0-SNAPSHOT.jar
STARTUP_MSG:        conf = {hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.keep.log.file.num=10, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.max.log.file.size=100MB, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.chunk.write.sync=false, hdds.container.close.threshold=0.9f, hdds.container.ipc.port=9859, hdds.container.ipc.random.port=false, hdds.container.ratis.admin.port=9857, hdds.container.ratis.datastream.enabled=false, hdds.container.ratis.datastream.port=9855, hdds.container.ratis.datastream.random.port=false, hdds.container.ratis.enabled=false, hdds.container.ratis.ipc.port=9858, hdds.container.ratis.ipc.random.port=false, hdds.container.ratis.leader.pending.bytes.limit=1GB, hdds.container.ratis.log.appender.queue.byte-limit=32MB, hdds.container.ratis.log.appender.queue.num-elements=1024, hdds.container.ratis.log.purge.gap=1000000, hdds.container.ratis.log.queue.byte-limit=4GB, hdds.container.ratis.log.queue.num-elements=1024, hdds.container.ratis.num.container.op.executors=10, hdds.container.ratis.num.write.chunk.threads.per.volume=10, hdds.container.ratis.rpc.type=GRPC, hdds.container.ratis.segment.preallocated.size=4MB, hdds.container.ratis.segment.size=64MB, hdds.container.ratis.server.port=9856, hdds.container.ratis.statemachine.max.pending.apply-transactions=100000, hdds.container.ratis.statemachine.write.wait.interval=10m, hdds.container.ratis.statemachinedata.sync.timeout=10s, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=true, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.dns.interface=default, hdds.datanode.dns.nameserver=default, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=10, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.kerberos.keytab.file=/etc/security/keytabs/dn.keytab, hdds.datanode.kerberos.principal=dn/dn@EXAMPLE.COM, hdds.datanode.metadata.rocksdb.cache.size=1GB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.threadpool=10, hdds.datanode.slow.op.warning.threshold=500ms, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.use.datanode.hostname=false, hdds.datanode.volume.choosing.policy=org.apache.hadoop.ozone.container.common.volume.CapacityVolumeChoosingPolicy, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.volume.min.free.space.percent=0, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.initial-interval=2s, hdds.heartbeat.interval=30s, hdds.heartbeat.recon.initial-interval=60s, hdds.heartbeat.recon.interval=60s, hdds.key.algo=RSA, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.retries=2147483647, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.watch.type=ALL_COMMITTED, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.leader.election.minimum.timeout.duration=5s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=30s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.retry-cache.timeout.duration=600000ms, hdds.ratis.snapshot.threshold=100000, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=500000, hdds.scm.block.deletion.txn.dn.commit.map.limit=5000000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=simple, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=9d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.security.provider=BC, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, hdds.xframe.enabled=true, hdds.xframe.value=SAMEORIGIN, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.administrators=*, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.max.count=0, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=16KB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.reconstruct.stripe.write.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.elastic.byte.buffer.pool.max.size=16GB, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.hbase.enhancements.allowed=false, ozone.client.incremental.chunk.list=false, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.key.write.concurrency=1, ozone.client.list.cache=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.max.retries=3, ozone.client.read.retry.interval=1, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.server-defaults.validity.period.ms=3600000, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.stream.putblock.piggybacking=false, ozone.client.stream.read.pre-read-size=33554432, ozone.client.stream.read.response-data-size=1048576, ozone.client.stream.read.timeout=10s, ozone.client.stream.readblock.enable=false, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.hbase.enhancements.allowed=false, ozone.http.basedir=/tmp/ozone_http, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=50000, ozone.key.preallocation.max.blocks=64, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.allow.leader.skip.linearizable.read=false, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.compaction.service.columnfamilies=keyTable,fileTable,directoryTable,deletedTable,deletedDirectoryTable,multipartInfoTable, ozone.om.compaction.service.enabled=false, ozone.om.compaction.service.run.interval=6h, ozone.om.compaction.service.timeout=10m, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.db.max.open.files=-1, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.edekcacheloader.initial.delay.ms=3000, ozone.om.edekcacheloader.interval.ms=1000, ozone.om.edekcacheloader.max-retries=10, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.follower.read.local.lease.enabled=false, ozone.om.follower.read.local.lease.lag.limit=10000, ozone.om.follower.read.local.lease.time.ms=5000, ozone.om.fs.snapshot.max.limit=10000, ozone.om.group.rights=READ, LIST, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.log.appender.wait-time.min=0ms, ozone.om.ha.raft.server.read.leader.lease.enabled=false, ozone.om.ha.raft.server.read.option=DEFAULT, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.hierarchical.resource.locks.hard.limit=10000, ozone.om.hierarchical.resource.locks.soft.limit=1024, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.kerberos.principal.pattern=*, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lease.hard.limit=7d, ozone.om.lease.soft.limit=60s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.network.topology.refresh.duration=1h, ozone.om.object.creation.ignore.client.acls=false, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=64MB, ozone.om.ratis.server.close.threshold=60s, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.pending.write.element-limit=4096, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.read.threadpool=10, ozone.om.s3.grpc.server_enabled=true, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.server.list.max.size=1000, ozone.om.snapshot.cache.cleanup.service.run.interval=1m, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compact.non.snapshot.diff.tables=false, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=10m, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.interval=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.local.data.manager.service.interval=5m, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.prune.compaction.backup.batch.size=2000, ozone.om.snapshot.rocksdb.metrics.enabled=false, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=20000, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dirs.permissions=750, ozone.recon.dn.metrics.collection.minimum.api.delay=30s, ozone.recon.dn.metrics.collection.timeout=10m, ozone.recon.filesizecount.flush.db.max.threshold=200000, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.event.buffer.capacity=20000, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=5s, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.scmclient.failover.max.retry=3, ozone.recon.scmclient.max.retry.timeout=6s, ozone.recon.scmclient.rpc.timeout=1m, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.reprocess.max.iterators=5, ozone.recon.task.reprocess.max.keys.in.memory=2000, ozone.recon.task.reprocess.max.workers=20, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=s3g, ozone.s3g.client.buffer.size=4MB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.https-address=0.0.0.0:9879, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.list.max.keys.limit=1000, ozone.s3g.metrics.percentiles.intervals.seconds=60, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.s3g.webadmin.http-address=0.0.0.0:19878, ozone.s3g.webadmin.http-bind-host=0.0.0.0, ozone.s3g.webadmin.http.enabled=true, ozone.s3g.webadmin.https-address=0.0.0.0:19879, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.per.dn.distribution.factor=8, ozone.scm.block.handler.count.key=100, ozone.scm.block.read.threadpool=10, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.handler.count.key=100, ozone.scm.client.port=9860, ozone.scm.client.read.threadpool=10, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.list.max.count=4096, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.handler.count.key=100, ozone.scm.datanode.id.dir=/data/metadata, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.datanode.read.threadpool=10, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=60s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=64MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.read.threadpool=1, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.security.crypto.compliance.mode=unrestricted, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.security.reconfigure.protocol.acl=*, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deep.cleaning.enabled=false, ozone.snapshot.defrag.limit.per.task=1, ozone.snapshot.defrag.service.interval=-1, ozone.snapshot.defrag.service.timeout=300s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.directory.service.interval=24h, ozone.snapshot.directory.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.volume.io.percentiles.intervals.seconds=60, ozone.xceiver.client.metrics.percentiles.intervals.seconds=60, recon.om.delta.update.lag.threshold=0, recon.om.delta.update.limit=50000, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2025-12-18 17:04:21,337 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [SIGHUP, SIGINT, SIGTERM]
2025-12-18 17:04:23,552 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
2025-12-18 17:04:23,619 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMSystemAudit to [].
2025-12-18 17:04:24,198 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2025-12-18 17:04:24,517 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.18.0.2:9862
2025-12-18 17:04:24,528 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2025-12-18 17:04:24,528 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
2025-12-18 17:04:28,735 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
2025-12-18 17:04:28,735 [main] INFO om.OzoneManager: Ozone Manager login successful.
2025-12-18 17:04:28,770 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2025-12-18 17:04:29,211 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created fail-over proxy for protocol ScmBlockLocationProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863]
2025-12-18 17:04:31,992 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
2025-12-18 17:04:33,996 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
2025-12-18 17:04:35,998 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
2025-12-18 17:04:38,000 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
2025-12-18 17:04:40,001 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
2025-12-18 17:04:42,003 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
2025-12-18 17:04:44,005 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
2025-12-18 17:04:46,006 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om/172.18.0.2 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
2025-12-18 17:04:48,471 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc_.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): SCM Server:1afd0f9f-aef7-4df7-8c5d-59ca2516510d(scm) is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:111)
	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:17426)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:539)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:508)
	at org.apache.hadoop.ipc_.RPC$Server.call(RPC.java:1164)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:1020)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:948)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc_.Server$Handler.run(Server.java:2951)
, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
2025-12-18 17:04:50,475 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc_.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): SCM Server:1afd0f9f-aef7-4df7-8c5d-59ca2516510d(scm) is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:111)
	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:17426)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:539)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:508)
	at org.apache.hadoop.ipc_.RPC$Server.call(RPC.java:1164)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:1020)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:948)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc_.Server$Handler.run(Server.java:2951)
, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
2025-12-18 17:04:52,478 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc_.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): SCM Server:1afd0f9f-aef7-4df7-8c5d-59ca2516510d(scm) is not the leader. Could not determine the leader node.
	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:111)
	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:17426)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:539)
	at org.apache.hadoop.ipc_.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:508)
	at org.apache.hadoop.ipc_.RPC$Server.call(RPC.java:1164)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:1020)
	at org.apache.hadoop.ipc_.Server$RpcCall.run(Server.java:948)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.hadoop.ipc_.Server$Handler.run(Server.java:2951)
, while invoking $Proxy33.send over nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3;layoutVersion=9
2025-12-18 17:04:54,693 [main] INFO om.OzoneManager: OM storage initialized. Initializing security
2025-12-18 17:04:54,693 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
2025-12-18 17:04:54,886 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
value: 9862
]
2025-12-18 17:04:54,904 [main] INFO proxy.SCMSecurityProtocolFailoverProxyProvider: Created fail-over proxy for protocol SCMSecurityProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9961]
2025-12-18 17:04:54,967 [main] INFO security.OMCertificateClient: Certificate serial ID set to null
2025-12-18 17:04:54,967 [main] ERROR security.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
2025-12-18 17:04:54,967 [main] INFO security.OMCertificateClient: Certificate client init case: 0
2025-12-18 17:04:54,968 [main] INFO security.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
2025-12-18 17:04:56,682 [main] INFO keys.KeyStorage: Storing public key to /data/metadata/om/keys/public.pem.
2025-12-18 17:04:56,697 [main] INFO keys.KeyStorage: Storing private key to /data/metadata/om/keys/private.pem.
2025-12-18 17:04:56,704 [main] INFO security.OMCertificateClient: Init response: GETCERT
2025-12-18 17:04:56,752 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.18.0.2,host:om
2025-12-18 17:04:56,752 [main] INFO ozone.OzoneSecurityUtil: ip:0:0:0:0:0:0:0:1%lo not returned.
2025-12-18 17:04:56,752 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
2025-12-18 17:04:56,810 [main] ERROR utils.CertificateSignRequest: Invalid domain om
2025-12-18 17:04:56,810 [main] INFO security.OMCertificateClient: Creating csr for OM->dns:om,ip:172.18.0.2,scmId:1afd0f9f-aef7-4df7-8c5d-59ca2516510d,clusterId:CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,subject:om
2025-12-18 17:04:57,615 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/om/certs/6.crt
2025-12-18 17:04:57,616 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDnjCCAoagAwIBAgIBBjANBgkqhkiG9w0BAQsFADCBhDEUMBIGA1UEAwwLc2Nt
LXN1YkBzY20xLTArBgNVBAsMJDFhZmQwZjlmLWFlZjctNGRmNy04YzVkLTU5Y2Ey
NTE2NTEwZDExMC8GA1UECgwoQ0lELThjOGIxMzI3LWVmYzUtNDliMi1iZjVkLTY3
ODNhMWM0ZWJlMzEKMAgGA1UEBRMBMjAeFw0yNTEyMTgxNzA0NTdaFw0yNjEyMTgx
NzA0NTdaMHsxCzAJBgNVBAMMAm9tMS0wKwYDVQQLDCQxYWZkMGY5Zi1hZWY3LTRk
ZjctOGM1ZC01OWNhMjUxNjUxMGQxMTAvBgNVBAoMKENJRC04YzhiMTMyNy1lZmM1
LTQ5YjItYmY1ZC02NzgzYTFjNGViZTMxCjAIBgNVBAUTATYwggEiMA0GCSqGSIb3
DQEBAQUAA4IBDwAwggEKAoIBAQD2k+3cqY9CyWcPtbI4Jyi8GrCfqqULNGNJcvFQ
Il5n3sSWs/HMj14I1E6mpwwFuEOfCwvkE1d9JXmIoNxxiURT4faCcBy6uyHuSPSO
9E3YypcSxsp56Vy4SGZY31leuce2vq6xa6FI/LTnyiZtt2y6vCrfe9BnXAnM0HeY
fF/u8W1WPLw+Xj02kf12ManVZZCmtkwDlNUwgY68fgOoi+ifdL4LekAAWpTWEiyg
NW6Y8pZQQt5RgrH4jp+krEUIGiCCw2EYIJ6MVhDzjYlFAUSdQj4RIhr53coTO22Q
1P2FLmusIzMfb/SQ3Dq/5zo5uKWsYOLAABLISSZX526ckitXAgMBAAGjIzAhMA4G
A1UdDwEB/wQEAwIDuDAPBgNVHREECDAGhwSsEgACMA0GCSqGSIb3DQEBCwUAA4IB
AQBaOU2XqR0buTiWo7VuCPqBd15xHnJ0RnxR0pCCio2OMrt7ZxHGvsufutpPom65
s5JPMQT2ZW0NZQf5TGAs2Glfl1EeKjxWg3I8w/9+Sp53LGXzweAdO086/qTTIgpp
nws31EjS/ypOSvl6YeZNI0iiswH/dhF/lXY7M1vsOT6LT3YlaDYhNrBSl/gJKg0D
5nf/Tsnk2m9j2A9oE/iY1rVooKFh5gSpIRKVf9R5QkPnveMk5DkieEoA1hDqOijS
hj2Ga1U7dipkIua7+Pz6l9fLi2WNH41oct79p0Av5ap18bmC4c/vJe5zYtTtevS8
lWfn5aLtrQ/Qy2RrprW13Kir
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkMWFmZDBmOWYtYWVmNy00ZGY3LThjNWQtNTljYTI1MTY1
MTBkMTEwLwYDVQQKDChDSUQtOGM4YjEzMjctZWZjNS00OWIyLWJmNWQtNjc4M2Ex
YzRlYmUzMQowCAYDVQQFEwExMB4XDTI1MTIxODE3MDQzNVoXDTMxMDEyNjE3MDQz
NVowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQxYWZkMGY5Zi1h
ZWY3LTRkZjctOGM1ZC01OWNhMjUxNjUxMGQxMTAvBgNVBAoMKENJRC04YzhiMTMy
Ny1lZmM1LTQ5YjItYmY1ZC02NzgzYTFjNGViZTMxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvjSoFBfXWg8fhaZVy9APUwzxJHzD+
5FqZN1iR0ntKjnsKqOFD6aabIRgC7pCXF38mAoxFHXbyDj3yyn5pBAoIPWAo0AVr
3lDgGdtmN7wjj1r8BtzAM4d0rLfWcNahhbpGyct1kS9udW5tIsP+QOKZI290YSjI
FpvEiQYpF8RKCPSPRJmMWNmKZvNHUwvg7Nel+M8Ef8Xk/Wj6mHr2eE+QpzNw9WCL
wGwu3fPi/szrQ9JATZSMevB0equfuUBUljd7xzi7rjCO3Xzo7KNQqzzSLisP+z7w
bs1Bxi4AvQk5qshj+kQ3cvsZLWOzz9wkFz2CiUw9a2oOeJWOMra1GA65AgMBAAGj
NDAyMA4GA1UdDwEB/wQEAwIBvjAPBgNVHREECDAGhwSsEgAEMA8GA1UdEwEB/wQF
MAMBAf8wDQYJKoZIhvcNAQELBQADggEBAJlBIWnjAMLvFFxO36rd8BcxEh3XIdWV
3JKy9Ini53dYSIAoibwJbQ55MqSSn6TDhlMn0+b2P1Rso+BPn71hcwsWkgqnJdq3
4ktLxkCVHW1lRarNq94RLqGeR6Xj/ww3PzZuA5xQMFs3X34qfQI/JN7uAOpveAT2
YOKK2qfYAzAfmoFmsIHhsX68w7xasnB4Lj3KOKI65786CQk5Gj981m5e/m40aXFp
7hvZa+Wkd6imsb85XF7SwQl1FAZcBjnYVArXXEFc3YNcAxugY6+3Wg5Fcm0SNJEv
0HXkPyLOroTvp4pOGJR0pAgXSuVyWmUKLzB9Wap5JQFOHTdO1X/o/5w=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkMWFmZDBmOWYtYWVmNy00ZGY3LThjNWQtNTljYTI1MTY1
MTBkMTEwLwYDVQQKDChDSUQtOGM4YjEzMjctZWZjNS00OWIyLWJmNWQtNjc4M2Ex
YzRlYmUzMQowCAYDVQQFEwExMB4XDTI1MTIxODE3MDQzNFoXDTMxMDEyNjE3MDQz
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDFhZmQwZjlmLWFlZjct
NGRmNy04YzVkLTU5Y2EyNTE2NTEwZDExMC8GA1UECgwoQ0lELThjOGIxMzI3LWVm
YzUtNDliMi1iZjVkLTY3ODNhMWM0ZWJlMzEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMa8PpS6yqTvWkY3VHP4R4CuMdPe8MBEml9x
uHN8F94v97JBmiA7Ht8YJ5ptresXSq/i7Wmyfw/Qy9ijy6dt6BktyGpejdocjbJI
jYAzebKf9mnUa1nK+lT1s9k7sFAEJ5WYf/RjLzRTRAq48EvsA3kLvb/ZpCT0iMer
+lAWpkv48Sezun1wBU2a+e5ZzunjJEXUE7wKETpkYx8WweqBrE5gEw43UyX6hjUI
rnPm+gNpapcIBOaYXYKEDK1gi+WAkbsd/NTVMOBSCandSqFc7dIdhANp3BR32tjq
NIlJfTdPHN7xb7FEURoU4sVAL4ek/I3UhRi1FWUgz8XXwKmibjcCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBIA
BDANBgkqhkiG9w0BAQsFAAOCAQEASGWjFN42ZMMFjqdBDlE6kKYz1vQMaptUPlA6
I24LRs0jftcZXund2CYKv9rUt9A1cz7EzHMnm2U+RR1xs/6AgsFTZiYcKepporlw
k1LUPZvSI6jpNFBKoyoT3Q7UOKBMsV33tHxwUuY64RqnOwUVMzPz72viB8fZip9S
1rKaCZgDTwOOr8pJnj6evlElHGnG5urRoT9yvDnYJ0kVDIHE8VpNN5aMf34NhPWx
f5MakwNgGprpFZUn+EwC7tgWnqymuwNQM8/LU0j+WA94R0nN+yVubC2thVktncOt
U0/cTKy6qYHGll1cDzPu/pHQ9vgDzXhQWIGAJNB8lZAjr66r0A==
-----END CERTIFICATE-----

2025-12-18 17:04:57,626 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/om/certs/CA-2.crt
2025-12-18 17:04:57,626 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDtTCCAp2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkMWFmZDBmOWYtYWVmNy00ZGY3LThjNWQtNTljYTI1MTY1
MTBkMTEwLwYDVQQKDChDSUQtOGM4YjEzMjctZWZjNS00OWIyLWJmNWQtNjc4M2Ex
YzRlYmUzMQowCAYDVQQFEwExMB4XDTI1MTIxODE3MDQzNVoXDTMxMDEyNjE3MDQz
NVowgYQxFDASBgNVBAMMC3NjbS1zdWJAc2NtMS0wKwYDVQQLDCQxYWZkMGY5Zi1h
ZWY3LTRkZjctOGM1ZC01OWNhMjUxNjUxMGQxMTAvBgNVBAoMKENJRC04YzhiMTMy
Ny1lZmM1LTQ5YjItYmY1ZC02NzgzYTFjNGViZTMxCjAIBgNVBAUTATIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvjSoFBfXWg8fhaZVy9APUwzxJHzD+
5FqZN1iR0ntKjnsKqOFD6aabIRgC7pCXF38mAoxFHXbyDj3yyn5pBAoIPWAo0AVr
3lDgGdtmN7wjj1r8BtzAM4d0rLfWcNahhbpGyct1kS9udW5tIsP+QOKZI290YSjI
FpvEiQYpF8RKCPSPRJmMWNmKZvNHUwvg7Nel+M8Ef8Xk/Wj6mHr2eE+QpzNw9WCL
wGwu3fPi/szrQ9JATZSMevB0equfuUBUljd7xzi7rjCO3Xzo7KNQqzzSLisP+z7w
bs1Bxi4AvQk5qshj+kQ3cvsZLWOzz9wkFz2CiUw9a2oOeJWOMra1GA65AgMBAAGj
NDAyMA4GA1UdDwEB/wQEAwIBvjAPBgNVHREECDAGhwSsEgAEMA8GA1UdEwEB/wQF
MAMBAf8wDQYJKoZIhvcNAQELBQADggEBAJlBIWnjAMLvFFxO36rd8BcxEh3XIdWV
3JKy9Ini53dYSIAoibwJbQ55MqSSn6TDhlMn0+b2P1Rso+BPn71hcwsWkgqnJdq3
4ktLxkCVHW1lRarNq94RLqGeR6Xj/ww3PzZuA5xQMFs3X34qfQI/JN7uAOpveAT2
YOKK2qfYAzAfmoFmsIHhsX68w7xasnB4Lj3KOKI65786CQk5Gj981m5e/m40aXFp
7hvZa+Wkd6imsb85XF7SwQl1FAZcBjnYVArXXEFc3YNcAxugY6+3Wg5Fcm0SNJEv
0HXkPyLOroTvp4pOGJR0pAgXSuVyWmUKLzB9Wap5JQFOHTdO1X/o/5w=
-----END CERTIFICATE-----

-----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkMWFmZDBmOWYtYWVmNy00ZGY3LThjNWQtNTljYTI1MTY1
MTBkMTEwLwYDVQQKDChDSUQtOGM4YjEzMjctZWZjNS00OWIyLWJmNWQtNjc4M2Ex
YzRlYmUzMQowCAYDVQQFEwExMB4XDTI1MTIxODE3MDQzNFoXDTMxMDEyNjE3MDQz
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDFhZmQwZjlmLWFlZjct
NGRmNy04YzVkLTU5Y2EyNTE2NTEwZDExMC8GA1UECgwoQ0lELThjOGIxMzI3LWVm
YzUtNDliMi1iZjVkLTY3ODNhMWM0ZWJlMzEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMa8PpS6yqTvWkY3VHP4R4CuMdPe8MBEml9x
uHN8F94v97JBmiA7Ht8YJ5ptresXSq/i7Wmyfw/Qy9ijy6dt6BktyGpejdocjbJI
jYAzebKf9mnUa1nK+lT1s9k7sFAEJ5WYf/RjLzRTRAq48EvsA3kLvb/ZpCT0iMer
+lAWpkv48Sezun1wBU2a+e5ZzunjJEXUE7wKETpkYx8WweqBrE5gEw43UyX6hjUI
rnPm+gNpapcIBOaYXYKEDK1gi+WAkbsd/NTVMOBSCandSqFc7dIdhANp3BR32tjq
NIlJfTdPHN7xb7FEURoU4sVAL4ek/I3UhRi1FWUgz8XXwKmibjcCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBIA
BDANBgkqhkiG9w0BAQsFAAOCAQEASGWjFN42ZMMFjqdBDlE6kKYz1vQMaptUPlA6
I24LRs0jftcZXund2CYKv9rUt9A1cz7EzHMnm2U+RR1xs/6AgsFTZiYcKepporlw
k1LUPZvSI6jpNFBKoyoT3Q7UOKBMsV33tHxwUuY64RqnOwUVMzPz72viB8fZip9S
1rKaCZgDTwOOr8pJnj6evlElHGnG5urRoT9yvDnYJ0kVDIHE8VpNN5aMf34NhPWx
f5MakwNgGprpFZUn+EwC7tgWnqymuwNQM8/LU0j+WA94R0nN+yVubC2thVktncOt
U0/cTKy6qYHGll1cDzPu/pHQ9vgDzXhQWIGAJNB8lZAjr66r0A==
-----END CERTIFICATE-----

2025-12-18 17:04:57,651 [main] INFO utils.CertificateCodec: Save certificate to /data/metadata/om/certs/ROOTCA-1.crt
2025-12-18 17:04:57,652 [main] INFO utils.CertificateCodec: Certificate -----BEGIN CERTIFICATE-----
MIIDsTCCApmgAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgDEQMA4GA1UEAwwHc2Nt
QHNjbTEtMCsGA1UECwwkMWFmZDBmOWYtYWVmNy00ZGY3LThjNWQtNTljYTI1MTY1
MTBkMTEwLwYDVQQKDChDSUQtOGM4YjEzMjctZWZjNS00OWIyLWJmNWQtNjc4M2Ex
YzRlYmUzMQowCAYDVQQFEwExMB4XDTI1MTIxODE3MDQzNFoXDTMxMDEyNjE3MDQz
NFowgYAxEDAOBgNVBAMMB3NjbUBzY20xLTArBgNVBAsMJDFhZmQwZjlmLWFlZjct
NGRmNy04YzVkLTU5Y2EyNTE2NTEwZDExMC8GA1UECgwoQ0lELThjOGIxMzI3LWVm
YzUtNDliMi1iZjVkLTY3ODNhMWM0ZWJlMzEKMAgGA1UEBRMBMTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMa8PpS6yqTvWkY3VHP4R4CuMdPe8MBEml9x
uHN8F94v97JBmiA7Ht8YJ5ptresXSq/i7Wmyfw/Qy9ijy6dt6BktyGpejdocjbJI
jYAzebKf9mnUa1nK+lT1s9k7sFAEJ5WYf/RjLzRTRAq48EvsA3kLvb/ZpCT0iMer
+lAWpkv48Sezun1wBU2a+e5ZzunjJEXUE7wKETpkYx8WweqBrE5gEw43UyX6hjUI
rnPm+gNpapcIBOaYXYKEDK1gi+WAkbsd/NTVMOBSCandSqFc7dIdhANp3BR32tjq
NIlJfTdPHN7xb7FEURoU4sVAL4ek/I3UhRi1FWUgz8XXwKmibjcCAwEAAaM0MDIw
DwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0RBAgwBocErBIA
BDANBgkqhkiG9w0BAQsFAAOCAQEASGWjFN42ZMMFjqdBDlE6kKYz1vQMaptUPlA6
I24LRs0jftcZXund2CYKv9rUt9A1cz7EzHMnm2U+RR1xs/6AgsFTZiYcKepporlw
k1LUPZvSI6jpNFBKoyoT3Q7UOKBMsV33tHxwUuY64RqnOwUVMzPz72viB8fZip9S
1rKaCZgDTwOOr8pJnj6evlElHGnG5urRoT9yvDnYJ0kVDIHE8VpNN5aMf34NhPWx
f5MakwNgGprpFZUn+EwC7tgWnqymuwNQM8/LU0j+WA94R0nN+yVubC2thVktncOt
U0/cTKy6qYHGll1cDzPu/pHQ9vgDzXhQWIGAJNB8lZAjr66r0A==
-----END CERTIFICATE-----

2025-12-18 17:04:57,654 [main] INFO security.OMCertificateClient: Certificate serial ID set to 6
2025-12-18 17:04:57,667 [main] INFO security.OMCertificateClient: Added certificate 1 from file: /data/metadata/om/certs/ROOTCA-1.crt.
2025-12-18 17:04:57,675 [main] INFO security.OMCertificateClient: Added certificate 2 from file: /data/metadata/om/certs/CA-2.crt.
2025-12-18 17:04:57,683 [main] INFO security.OMCertificateClient: Added certificate 6 from file: /data/metadata/om/certs/6.crt.
2025-12-18 17:04:57,700 [main] INFO security.OMCertificateClient: CertificateRenewerService for om is started with first delay 29116799306 ms and interval 86400000 ms.
2025-12-18 17:04:57,701 [main] INFO security.OMCertificateClient: Successfully stored OM signed certificate, case:GETCERT.
2025-12-18 17:04:57,716 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down OzoneManager at om/172.18.0.2
************************************************************/
No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8' to the OZONE_OPTS
2025-12-18 17:05:01,429 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting OzoneManager
STARTUP_MSG:        host = om/172.18.0.2
STARTUP_MSG:     version = 2.2.0-SNAPSHOT
STARTUP_MSG:       build = https://github.com/apache/ozone/1c817b0460f1e66e75aef0b8f5fde63c691f2a65
STARTUP_MSG:        java = 21.0.2
STARTUP_MSG:        args = []
STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.16.2.jar:/opt/hadoop/share/ozone/lib/guava-33.5.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.3.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jspecify-1.0.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.29.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-3.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-3.25.8.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.18.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.18.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.7.5.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.77.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.77.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.24.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.77.0.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.77.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.27.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/grpc-util-1.77.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.77.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-2.1.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.27.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.18.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.4.2.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.14.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-10.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.4.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.11.1.jar:/opt/hadoop/share/ozone/lib/commons-collections4-4.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.5.4.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.13.1.jar:/opt/hadoop/share/ozone/lib/re2j-1.7.jar:/opt/hadoop/share/ozone/lib/gson-2.13.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.55.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-epoll-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/netty-transport-classes-epoll-4.1.128.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-3.6.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.4.2.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.16.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.16.2.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.10.0.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.11.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentelemetry-api-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-context-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-otlp-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-metrics-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-logs-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-otlp-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-exporter-sender-okhttp-1.57.0.jar:/opt/hadoop/share/ozone/lib/okhttp-jvm-5.3.2.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.16.4.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.9.25.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-extension-autoconfigure-spi-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-common-1.57.0.jar:/opt/hadoop/share/ozone/lib/opentelemetry-sdk-trace-1.57.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/bcutil-jdk18on-1.83.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-dropwizard3-3.2.1.jar:/opt/hadoop/share/ozone/lib/jcl-over-slf4j-2.0.17.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-2.0.17.jar:/opt/hadoop/share/ozone/lib/hdds-config-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.26.jar:/opt/hadoop/share/ozone/lib/jnr-constants-0.10.4.jar:/opt/hadoop/share/ozone/lib/jnr-posix-3.1.21.jar:/opt/hadoop/share/ozone/lib/jnr-ffi-2.2.18.jar:/opt/hadoop/share/ozone/lib/jffi-1.3.14.jar:/opt/hadoop/share/ozone/lib/jffi-1.3.14-native.jar:/opt/hadoop/share/ozone/lib/asm-commons-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-analysis-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-tree-9.7.1.jar:/opt/hadoop/share/ozone/lib/asm-util-9.7.1.jar:/opt/hadoop/share/ozone/lib/jnr-a64asm-1.0.0.jar:/opt/hadoop/share/ozone/lib/jnr-x86asm-1.0.2.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.6.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.16.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.16.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-api-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.47.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.47.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.47.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.47.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.4.jar:/opt/hadoop/share/ozone/lib/apache-log4j-extras-1.2.17.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.25.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.10.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.77.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.59.2.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.77.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.10.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.24.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/reflections-0.10.2.jar:/opt/hadoop/share/ozone/lib/javassist-3.30.2-GA.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/slf4j-api-2.0.17.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.5.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.74.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.74.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/asm-9.8.jar:/opt/hadoop/share/ozone/lib/ozone-manager-2.2.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-base-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-json-provider-2.16.2.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.16.2.jar:/opt/hadoop/share/ozone/lib/gethostname4j-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.6.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.6.0.jar:/opt/hadoop/share/ozone/lib/aws-java-sdk-logs-1.12.765.jar:/opt/hadoop/share/ozone/lib/aws-java-sdk-core-1.12.788.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-cbor-2.16.2.jar:/opt/hadoop/share/ozone/lib/jmespath-java-1.12.765.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/aircompressor-0.27.jar:/opt/hadoop/share/ozone/lib/joda-time-2.12.7.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.4.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.16.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.13.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/orc-shims-1.5.8.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.58.v20250814.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.6.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ozone-multitenancy-ranger-2.2.0-SNAPSHOT.jar
STARTUP_MSG:        conf = {hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.keep.log.file.num=10, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.max.log.file.size=100MB, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=true, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.chunk.write.sync=false, hdds.container.close.threshold=0.9f, hdds.container.ipc.port=9859, hdds.container.ipc.random.port=false, hdds.container.ratis.admin.port=9857, hdds.container.ratis.datastream.enabled=false, hdds.container.ratis.datastream.port=9855, hdds.container.ratis.datastream.random.port=false, hdds.container.ratis.enabled=false, hdds.container.ratis.ipc.port=9858, hdds.container.ratis.ipc.random.port=false, hdds.container.ratis.leader.pending.bytes.limit=1GB, hdds.container.ratis.log.appender.queue.byte-limit=32MB, hdds.container.ratis.log.appender.queue.num-elements=1024, hdds.container.ratis.log.purge.gap=1000000, hdds.container.ratis.log.queue.byte-limit=4GB, hdds.container.ratis.log.queue.num-elements=1024, hdds.container.ratis.num.container.op.executors=10, hdds.container.ratis.num.write.chunk.threads.per.volume=10, hdds.container.ratis.rpc.type=GRPC, hdds.container.ratis.segment.preallocated.size=4MB, hdds.container.ratis.segment.size=64MB, hdds.container.ratis.server.port=9856, hdds.container.ratis.statemachine.max.pending.apply-transactions=100000, hdds.container.ratis.statemachine.write.wait.interval=10m, hdds.container.ratis.statemachinedata.sync.timeout=10s, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=true, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=19864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.dns.interface=default, hdds.datanode.dns.nameserver=default, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=10, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/dn@EXAMPLE.COM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.kerberos.keytab.file=/etc/security/keytabs/dn.keytab, hdds.datanode.kerberos.principal=dn/dn@EXAMPLE.COM, hdds.datanode.metadata.rocksdb.cache.size=1GB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.threadpool=10, hdds.datanode.slow.op.warning.threshold=500ms, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.use.datanode.hostname=false, hdds.datanode.volume.choosing.policy=org.apache.hadoop.ozone.container.common.volume.CapacityVolumeChoosingPolicy, hdds.datanode.volume.min.free.space=100MB, hdds.datanode.volume.min.free.space.percent=0, hdds.db.profile=DISK, hdds.grpc.tls.enabled=true, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.initial-interval=2s, hdds.heartbeat.interval=30s, hdds.heartbeat.recon.initial-interval=60s, hdds.heartbeat.recon.interval=60s, hdds.key.algo=RSA, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.retries=2147483647, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.watch.type=ALL_COMMITTED, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.leader.election.minimum.timeout.duration=5s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.log.appender.wait-time.min=0us, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=30s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.retry-cache.timeout.duration=600000ms, hdds.ratis.snapshot.threshold=100000, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=500000, hdds.scm.block.deletion.txn.dn.commit.map.limit=5000000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/scm.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/scm@EXAMPLE.COM, hdds.scm.http.auth.type=simple, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/scm.keytab, hdds.scm.kerberos.principal=scm/scm@EXAMPLE.COM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=9d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.security.provider=BC, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.expired.certificate.check.interval=P1D, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, hdds.xframe.enabled=true, hdds.xframe.value=SAMEORIGIN, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.administrators=*, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=1MB, ozone.chunk.read.mapped.buffer.max.count=0, ozone.chunk.read.mapped.buffer.threshold=32KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=16KB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.grpc.write.timeout=30s, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.reconstruct.stripe.write.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.elastic.byte.buffer.pool.max.size=16GB, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.hbase.enhancements.allowed=false, ozone.client.incremental.chunk.list=false, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.key.write.concurrency=1, ozone.client.list.cache=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.max.retries=3, ozone.client.read.retry.interval=1, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.server-defaults.validity.period.ms=3600000, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.stream.putblock.piggybacking=false, ozone.client.stream.read.pre-read-size=33554432, ozone.client.stream.read.response-data-size=1048576, ozone.client.stream.read.timeout=10s, ozone.client.stream.readblock.enable=false, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.handler.type=distributed, ozone.hbase.enhancements.allowed=false, ozone.http.basedir=/tmp/ozone_http, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=50000, ozone.key.preallocation.max.blocks=64, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.allow.leader.skip.linearizable.read=false, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.compaction.service.columnfamilies=keyTable,fileTable,directoryTable,deletedTable,deletedDirectoryTable,multipartInfoTable, ozone.om.compaction.service.enabled=false, ozone.om.compaction.service.run.interval=6h, ozone.om.compaction.service.timeout=10m, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.db.max.open.files=-1, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.edekcacheloader.initial.delay.ms=3000, ozone.om.edekcacheloader.interval.ms=1000, ozone.om.edekcacheloader.max-retries=10, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.follower.read.local.lease.enabled=false, ozone.om.follower.read.local.lease.lag.limit=10000, ozone.om.follower.read.local.lease.time.ms=5000, ozone.om.fs.snapshot.max.limit=10000, ozone.om.group.rights=READ, LIST, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.log.appender.wait-time.min=0ms, ozone.om.ha.raft.server.read.leader.lease.enabled=false, ozone.om.ha.raft.server.read.option=DEFAULT, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.hierarchical.resource.locks.hard.limit=10000, ozone.om.hierarchical.resource.locks.soft.limit=1024, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/om.keytab, ozone.om.http.auth.kerberos.principal=HTTP/om@EXAMPLE.COM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/om.keytab, ozone.om.kerberos.principal=om/om@EXAMPLE.COM, ozone.om.kerberos.principal.pattern=*, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lease.hard.limit=7d, ozone.om.lease.soft.limit=60s, ozone.om.lock.fair=false, ozone.om.max.buckets=100000, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.network.topology.refresh.duration=1h, ozone.om.object.creation.ignore.client.acls=false, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.open.mpu.cleanup.service.interval=24h, ozone.om.open.mpu.cleanup.service.timeout=300s, ozone.om.open.mpu.expire.threshold=30d, ozone.om.open.mpu.parts.cleanup.limit.per.task=1000, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=64MB, ozone.om.ratis.server.close.threshold=60s, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.pending.write.element-limit=4096, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.read.threadpool=10, ozone.om.s3.grpc.server_enabled=true, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.server.list.max.size=1000, ozone.om.snapshot.cache.cleanup.service.run.interval=1m, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.checkpoint.dir.creation.poll.timeout=20s, ozone.om.snapshot.compact.non.snapshot.diff.tables=false, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=10m, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.interval=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.load.native.lib=true, ozone.om.snapshot.local.data.manager.service.interval=5m, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.prune.compaction.backup.batch.size=2000, ozone.om.snapshot.rocksdb.metrics.enabled=false, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=20000, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dirs.permissions=750, ozone.recon.dn.metrics.collection.minimum.api.delay=30s, ozone.recon.dn.metrics.collection.timeout=10m, ozone.recon.filesizecount.flush.db.max.threshold=200000, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.event.buffer.capacity=20000, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=5s, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.scmclient.failover.max.retry=3, ozone.recon.scmclient.max.retry.timeout=6s, ozone.recon.scmclient.rpc.timeout=1m, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.reprocess.max.iterators=5, ozone.recon.task.reprocess.max.keys.in.memory=2000, ozone.recon.task.reprocess.max.workers=20, ozone.recon.task.thread.count=1, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3.administrators=s3g, ozone.s3g.client.buffer.size=4MB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/s3g.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/s3g@EXAMPLE.COM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.https-address=0.0.0.0:9879, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/s3g@EXAMPLE.COM, ozone.s3g.list-keys.shallow.enabled=true, ozone.s3g.list.max.keys.limit=1000, ozone.s3g.metrics.percentiles.intervals.seconds=60, ozone.s3g.secret.http.auth.type=kerberos, ozone.s3g.secret.http.enabled=true, ozone.s3g.volume.name=s3v, ozone.s3g.webadmin.http-address=0.0.0.0:19878, ozone.s3g.webadmin.http-bind-host=0.0.0.0, ozone.s3g.webadmin.http.enabled=true, ozone.s3g.webadmin.https-address=0.0.0.0:19879, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.per.dn.distribution.factor=8, ozone.scm.block.handler.count.key=100, ozone.scm.block.read.threadpool=10, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.handler.count.key=100, ozone.scm.client.port=9860, ozone.scm.client.read.threadpool=10, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.list.max.count=4096, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.admin.monitor.logging.limit=1000, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.handler.count.key=100, ozone.scm.datanode.id.dir=/data/metadata, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.datanode.read.threadpool=10, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=60s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.raft.server.log.appender.wait-time.min=0ms, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=64MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=scm:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.read.threadpool=1, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.security.crypto.compliance.mode=unrestricted, ozone.security.enabled=true, ozone.security.http.kerberos.enabled=true, ozone.security.reconfigure.protocol.acl=*, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deep.cleaning.enabled=false, ozone.snapshot.defrag.limit.per.task=1, ozone.snapshot.defrag.service.interval=-1, ozone.snapshot.defrag.service.timeout=300s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.directory.service.interval=24h, ozone.snapshot.directory.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.volume.io.percentiles.intervals.seconds=60, ozone.xceiver.client.metrics.percentiles.intervals.seconds=60, recon.om.delta.update.lag.threshold=0, recon.om.delta.update.limit=50000, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
************************************************************/
2025-12-18 17:05:01,567 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [SIGHUP, SIGINT, SIGTERM]
2025-12-18 17:05:02,674 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
2025-12-18 17:05:02,686 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMSystemAudit to [].
2025-12-18 17:05:02,961 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
2025-12-18 17:05:03,030 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.18.0.2:9862
2025-12-18 17:05:03,030 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
2025-12-18 17:05:03,030 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
2025-12-18 17:05:03,728 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2025-12-18 17:05:03,769 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SNAPSHOT_DEFRAG (version = 9), software layout = SNAPSHOT_DEFRAG (version = 9)
2025-12-18 17:05:04,107 [main] INFO reflections.Reflections: Reflections took 218 ms to scan 1 urls, producing 164 keys and 492 values
2025-12-18 17:05:04,119 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
2025-12-18 17:05:04,350 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file om.keytab. Keytab auto renewal enabled : false
2025-12-18 17:05:04,351 [main] INFO om.OzoneManager: Ozone Manager login successful.
2025-12-18 17:05:04,351 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2025-12-18 17:05:04,390 [main] INFO om.OzoneManager: Set default replication in OM: RATIS/3 -> RATIS/THREE
2025-12-18 17:05:04,427 [main] INFO proxy.SCMContainerLocationFailoverProxyProvider: Created fail-over proxy for protocol StorageContainerLocationProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9860]
2025-12-18 17:05:04,560 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created fail-over proxy for protocol ScmBlockLocationProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863]
2025-12-18 17:05:04,658 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created fail-over proxy for protocol ScmBlockLocationProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9863]
2025-12-18 17:05:05,423 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
value: 9862
]
2025-12-18 17:05:05,432 [main] INFO proxy.SCMSecurityProtocolFailoverProxyProvider: Created fail-over proxy for protocol SCMSecurityProtocolPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9961]
2025-12-18 17:05:05,457 [main] INFO security.OMCertificateClient: Certificate serial ID set to 6
2025-12-18 17:05:05,513 [main] INFO security.OMCertificateClient: Added certificate 1 from file: /data/metadata/om/certs/ROOTCA-1.crt.
2025-12-18 17:05:05,518 [main] INFO security.OMCertificateClient: Added certificate 2 from file: /data/metadata/om/certs/CA-2.crt.
2025-12-18 17:05:05,522 [main] INFO security.OMCertificateClient: Added certificate 6 from file: /data/metadata/om/certs/6.crt.
2025-12-18 17:05:05,528 [main] INFO security.OMCertificateClient: CertificateRenewerService for om is started with first delay 29116791475 ms and interval 86400000 ms.
2025-12-18 17:05:05,532 [main] INFO proxy.SecretKeyProtocolFailoverProxyProvider: Created fail-over proxy for protocol SecretKeyProtocolOmPB with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.18.0.4:9961]
2025-12-18 17:05:05,543 [main] INFO symmetric.DefaultSecretKeyVerifierClient: Initializing secret key cache with size 20, TTL PT216H
2025-12-18 17:05:05,544 [main] INFO security.OMCertificateClient: om has 1 Root CA certificates
2025-12-18 17:05:05,614 [main] INFO om.OzoneManager: OM start with adminUsers: [*, om]
2025-12-18 17:05:05,651 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2025-12-18 17:05:05,858 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
2025-12-18 17:05:05,860 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
java.nio.file.AccessDeniedException: /opt/hadoop/ozone_rocksdb_tools5167641526492789460
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:90)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:438)
	at java.base/java.nio.file.Files.createDirectory(Files.java:699)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:134)
	at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:171)
	at java.base/java.nio.file.Files.createTempDirectory(Files.java:976)
	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:174)
	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:131)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRawSSTFileReader.loadLibrary(ManagedRawSSTFileReader.java:53)
	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer.<init>(RocksDBCheckpointDiffer.java:252)
	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer$RocksDBCheckpointDifferHolder.lambda$getInstance$0(RocksDBCheckpointDiffer.java:1427)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1708)
	at org.apache.ozone.rocksdiff.RocksDBCheckpointDiffer$RocksDBCheckpointDifferHolder.getInstance(RocksDBCheckpointDiffer.java:1426)
	at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:105)
	at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:230)
	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.loadDB(OmMetadataManagerImpl.java:442)
	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.start(OmMetadataManagerImpl.java:425)
	at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.<init>(OmMetadataManagerImpl.java:221)
	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:918)
	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:689)
	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:883)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.start(OzoneManagerStarter.java:188)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOm(OzoneManagerStarter.java:85)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:73)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:1)
	at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
	at picocli.CommandLine.access$1500(CommandLine.java:148)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
	at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
	at picocli.CommandLine.execute(CommandLine.java:2170)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:89)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:80)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:57)
2025-12-18 17:05:05,862 [main] WARN rocksdiff.RocksDBCheckpointDiffer: Native Library for raw sst file reading loading failed. Cannot prune OMKeyInfo from SST files. Unable to load library ozone_rocksdb_tools from both java.library.path & resource file libozone_rocksdb_tools.so from jar.
2025-12-18 17:05:06,124 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
2025-12-18 17:05:06,136 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
2025-12-18 17:05:06,136 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
2025-12-18 17:05:06,155 [main] INFO acl.OzoneAuthorizerFactory: om1: Authorizer for OM is class org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer
2025-12-18 17:05:06,164 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
2025-12-18 17:05:06,182 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
2025-12-18 17:05:06,231 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
2025-12-18 17:05:06,232 [main] WARN utils.NativeLibraryLoader: Unable to load library: ozone_rocksdb_tools
java.nio.file.AccessDeniedException: /opt/hadoop/ozone_rocksdb_tools12286187564167027635
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:90)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:438)
	at java.base/java.nio.file.Files.createDirectory(Files.java:699)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:134)
	at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:171)
	at java.base/java.nio.file.Files.createTempDirectory(Files.java:976)
	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.copyResourceFromJarToTemp(NativeLibraryLoader.java:174)
	at org.apache.hadoop.hdds.utils.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:131)
	at org.apache.hadoop.hdds.utils.db.managed.ManagedRawSSTFileReader.loadLibrary(ManagedRawSSTFileReader.java:53)
	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.initNativeLibraryForEfficientDiff(SnapshotDiffManager.java:288)
	at org.apache.hadoop.ozone.om.snapshot.SnapshotDiffManager.<init>(SnapshotDiffManager.java:262)
	at org.apache.hadoop.ozone.om.OmSnapshotManager.<init>(OmSnapshotManager.java:255)
	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:1000)
	at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:689)
	at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:883)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.start(OzoneManagerStarter.java:188)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOm(OzoneManagerStarter.java:85)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:73)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:1)
	at picocli.CommandLine.executeUserObject(CommandLine.java:2041)
	at picocli.CommandLine.access$1500(CommandLine.java:148)
	at picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2461)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2453)
	at picocli.CommandLine$RunLast.handle(CommandLine.java:2415)
	at picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2273)
	at picocli.CommandLine$RunLast.execute(CommandLine.java:2417)
	at picocli.CommandLine.execute(CommandLine.java:2170)
	at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:89)
	at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:80)
	at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:57)
2025-12-18 17:05:06,232 [main] WARN snapshot.SnapshotDiffManager: Native Library for raw sst file reading loading failed. Fallback to performing a full diff instead. Unable to load library ozone_rocksdb_tools from both java.library.path & resource file libozone_rocksdb_tools.so from jar.
2025-12-18 17:05:06,237 [main] INFO utils.BackgroundService: Starting service SnapshotDiffCleanupService with interval 60000 milliseconds
2025-12-18 17:05:06,377 [main] INFO om.OzoneManager: Created Volume s3v With Owner om required for S3Gateway operations.
2025-12-18 17:05:06,426 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2025-12-18 17:05:06,427 [main] WARN server.ServerUtils: Snapshot directory for Ratis is not configured. Falling back to ozone.metadata.dirs
2025-12-18 17:05:06,443 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
2025-12-18 17:05:06,458 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
2025-12-18 17:05:06,466 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
2025-12-18 17:05:06,483 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMSystemAudit to [].
2025-12-18 17:05:06,485 [main] INFO ratis.OzoneManagerStateMachine: TransactionInfo not found in OM DB.
2025-12-18 17:05:06,519 [main] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
2025-12-18 17:05:06,525 [main] INFO keys.KeyStorage: Reading private key from /data/metadata/om/keys/private.pem.
2025-12-18 17:05:06,559 [main] INFO ssl.ReloadingX509KeyManager: Key manager is loaded with certificate chain
2025-12-18 17:05:06,592 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 6
             IssuerDN: CN=scm-sub@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=2
           Start Date: Thu Dec 18 17:04:57 UTC 2025
           Final Date: Fri Dec 18 17:04:57 UTC 2026
            SubjectDN: CN=om,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=6
           Public Key: RSA Public Key [fe:23:4f:d2:73:49:3b:28:51:bf:aa:b9:33:f9:d9:6f:1e:cc:bd:e5],[56:66:d1:a4]
        modulus: f693eddca98f42c9670fb5b2382728bc1ab09faaa50b34634972f150225e67dec496b3f1cc8f5e08d44ea6a70c05b8439f0b0be413577d257988a0dc71894453e1f682701cbabb21ee48f48ef44dd8ca9712c6ca79e95cb8486658df595eb9c7b6beaeb16ba148fcb4e7ca266db76cbabc2adf7bd0675c09ccd077987c5feef16d563cbc3e5e3d3691fd7631a9d56590a6b64c0394d530818ebc7e03a88be89f74be0b7a40005a94d6122ca0356e98f2965042de5182b1f88e9fa4ac45081a2082c36118209e8c5610f38d894501449d423e11221af9ddca133b6d90d4fd852e6bac23331f6ff490dc3abfe73a39b8a5ac60e2c00012c8492657e76e9c922b57
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 5a394d97a91d1bb93896a3b56e08fa81775e711e
                       7274467c51d290828a8d8e32bb7b6711c6becb9f
                       bada4fa26eb9b3924f3104f6656d0d6507f94c60
                       2cd8695f97511e2a3c5683723cc3ff7e4a9e772c
                       65f3c1e01d3b4f3afea4d3220a699f0b37d448d2
                       ff2a4e4af97a61e64d2348a2b301ff76117f9576
                       3b335bec393e8b4f762568362136b05297f8092a
                       0d03e677ff4ec9e4da6f63d80f6813f898d6b568
                       a0a161e604a92112957fd4794243e7bde324e439
                       22784a00d610ea3a28d2863d866b553b762a6422
                       e6bbf8fcfa97d7cb8b658d1f8d6872defda7402f
                       e5aa75f1b982e1cfef25ee7362d4ed7af4bc9567
                       e7e5a2edad0fd0cb646ba6b5b5dca8ab
       Extensions: 
                       critical(true) KeyUsage: 0xb8
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT
        DER Octet String[4]


2025-12-18 17:05:06,610 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 2
             IssuerDN: CN=scm@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=1
           Start Date: Thu Dec 18 17:04:35 UTC 2025
           Final Date: Sun Jan 26 17:04:35 UTC 2031
            SubjectDN: CN=scm-sub@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=2
           Public Key: RSA Public Key [ad:21:01:d5:4e:2a:29:75:38:83:7b:8c:4f:08:ec:80:0b:c3:d8:a1],[56:66:d1:a4]
        modulus: af8d2a0505f5d683c7e1699572f403d4c33c491f30fee45a99375891d27b4a8e7b0aa8e143e9a69b211802ee9097177f26028c451d76f20e3df2ca7e69040a083d6028d0056bde50e019db6637bc238f5afc06dcc0338774acb7d670d6a185ba46c9cb75912f6e756e6d22c3fe40e299236f746128c8169bc489062917c44a08f48f44998c58d98a66f347530be0ecd7a5f8cf047fc5e4fd68fa987af6784f90a73370f5608bc06c2eddf3e2fecceb43d2404d948c7af0747aab9fb9405496377bc738bbae308edd7ce8eca350ab3cd22e2b0ffb3ef06ecd41c62e00bd0939aac863fa443772fb192d63b3cfdc24173d82894c3d6b6a0e78958e32b6b5180eb9
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 99412169e300c2ef145c4edfaaddf01731121dd7
                       21d595dc92b2f489e2e7775848802889bc096d0e
                       7932a4929fa4c3865327d3e6f63f546ca3e04f9f
                       bd61730b16920aa725dab7e24b4bc640951d6d65
                       45aacdabde112ea19e47a5e3ff0c373f366e039c
                       50305b375f7e2a7d023f24deee00ea6f7804f660
                       e28adaa7d803301f9a8166b081e1b17ebcc3bc5a
                       b270782e3dca38a23ae7bf3a0909391a3f7cd66e
                       5efe6e34697169ee1bd96be5a477a8a6b1bf395c
                       5ed2c1097514065c0639d8540ad75c415cdd835c
                       031ba063afb75a0e45726d1234912fd075e43f22
                       ceae84efa78a4e189474a408174ae5725a650a2f
                       307d59aa7925014e1d374ed57fe8ff9c
       Extensions: 
                       critical(true) KeyUsage: 0xbe
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT
        DER Octet String[4]

                       critical(true) BasicConstraints: isCa(true)

2025-12-18 17:05:06,627 [main] INFO ssl.ReloadingX509KeyManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=1
           Start Date: Thu Dec 18 17:04:34 UTC 2025
           Final Date: Sun Jan 26 17:04:34 UTC 2031
            SubjectDN: CN=scm@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=1
           Public Key: RSA Public Key [90:21:05:2f:fa:b8:82:8d:7e:7b:b1:12:a0:02:3e:b6:62:19:db:7a],[56:66:d1:a4]
        modulus: c6bc3e94bacaa4ef5a46375473f84780ae31d3def0c0449a5f71b8737c17de2ff7b2419a203b1edf18279a6dadeb174aafe2ed69b27f0fd0cbd8a3cba76de8192dc86a5e8dda1c8db2488d803379b29ff669d46b59cafa54f5b3d93bb050042795987ff4632f3453440ab8f04bec03790bbdbfd9a424f488c7abfa5016a64bf8f127b3ba7d70054d9af9ee59cee9e32445d413bc0a113a64631f16c1ea81ac4e60130e375325fa863508ae73e6fa03696a970804e6985d82840cad608be58091bb1dfcd4d530e05209a9dd4aa15cedd21d840369dc1477dad8ea3489497d374f1cdef16fb144511a14e2c5402f87a4fc8dd48518b5156520cfc5d7c0a9a26e37
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 4865a314de3664c3058ea7410e513a90a633d6f4
                       0c6a9b543e503a236e0b46cd237ed7195ee9ddd8
                       260abfdad4b7d035733ec4cc73279b653e451d71
                       b3fe8082c15366261c29ea69a2b9709352d43d9b
                       d223a8e934504aa32a13dd0ed438a04cb15df7b4
                       7c7052e63ae11aa73b05153333f3ef6be207c7d9
                       8a9f52d6b29a0998034f038eafca499e3e9ebe51
                       251c69c6e6ead1a13f72bc39d82749150c81c4f1
                       5a4d37968c7f7e0d84f5b17f931a9303601a9ae9
                       159527f84c02eed8169eaca6bb035033cfcb5348
                       fe580f784749cdfb256e6c2dad85592d9dc3ad53
                       4fdc4cacbaa981c6965d5c0f33eefe91d0f6f803
                       cd785058818024d07c959023afaeabd0
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT
        DER Octet String[4]


2025-12-18 17:05:06,647 [main] INFO ssl.ReloadingX509TrustManager: Trust manager is loaded with certificates
2025-12-18 17:05:06,649 [main] INFO ssl.ReloadingX509TrustManager:   [0]         Version: 3
         SerialNumber: 1
             IssuerDN: CN=scm@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=1
           Start Date: Thu Dec 18 17:04:34 UTC 2025
           Final Date: Sun Jan 26 17:04:34 UTC 2031
            SubjectDN: CN=scm@scm,OU=1afd0f9f-aef7-4df7-8c5d-59ca2516510d,O=CID-8c8b1327-efc5-49b2-bf5d-6783a1c4ebe3,SERIALNUMBER=1
           Public Key: RSA Public Key [90:21:05:2f:fa:b8:82:8d:7e:7b:b1:12:a0:02:3e:b6:62:19:db:7a],[56:66:d1:a4]
        modulus: c6bc3e94bacaa4ef5a46375473f84780ae31d3def0c0449a5f71b8737c17de2ff7b2419a203b1edf18279a6dadeb174aafe2ed69b27f0fd0cbd8a3cba76de8192dc86a5e8dda1c8db2488d803379b29ff669d46b59cafa54f5b3d93bb050042795987ff4632f3453440ab8f04bec03790bbdbfd9a424f488c7abfa5016a64bf8f127b3ba7d70054d9af9ee59cee9e32445d413bc0a113a64631f16c1ea81ac4e60130e375325fa863508ae73e6fa03696a970804e6985d82840cad608be58091bb1dfcd4d530e05209a9dd4aa15cedd21d840369dc1477dad8ea3489497d374f1cdef16fb144511a14e2c5402f87a4fc8dd48518b5156520cfc5d7c0a9a26e37
public exponent: 10001

  Signature Algorithm: SHA256WITHRSA
            Signature: 4865a314de3664c3058ea7410e513a90a633d6f4
                       0c6a9b543e503a236e0b46cd237ed7195ee9ddd8
                       260abfdad4b7d035733ec4cc73279b653e451d71
                       b3fe8082c15366261c29ea69a2b9709352d43d9b
                       d223a8e934504aa32a13dd0ed438a04cb15df7b4
                       7c7052e63ae11aa73b05153333f3ef6be207c7d9
                       8a9f52d6b29a0998034f038eafca499e3e9ebe51
                       251c69c6e6ead1a13f72bc39d82749150c81c4f1
                       5a4d37968c7f7e0d84f5b17f931a9303601a9ae9
                       159527f84c02eed8169eaca6bb035033cfcb5348
                       fe580f784749cdfb256e6c2dad85592d9dc3ad53
                       4fdc4cacbaa981c6965d5c0f33eefe91d0f6f803
                       cd785058818024d07c959023afaeabd0
       Extensions: 
                       critical(true) BasicConstraints: isCa(true)
                       critical(true) KeyUsage: 0x6
                       critical(false) 2.5.29.17 value = Sequence
    Tagged [CONTEXT 7] IMPLICIT
        DER Octet String[4]


2025-12-18 17:05:06,652 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2025-12-18 17:05:06,653 [main] INFO netty.NettyConfigKeys$DataStream: setTlsConf GrpcTlsConfig0-
2025-12-18 17:05:06,672 [main] INFO server.RaftServer: Starting Apache Ratis Client -- RaftServerProxy om1
2025-12-18 17:05:06,672 [main] INFO server.RaftServer:                version: 3.2.1
2025-12-18 17:05:06,672 [main] INFO server.RaftServer:                    url: git@github.com:onesizefitsquorum/ratis.git
2025-12-18 17:05:06,672 [main] INFO server.RaftServer:               revision: 07a340e37662b1f0c9fd2dfde79fccb728060d34
2025-12-18 17:05:06,672 [main] INFO server.RaftServer:                   java: OpenJDK 64-Bit Server VM 21.0.2+13-58
2025-12-18 17:05:06,672 [main] INFO server.RaftServer:                   user: hadoop
2025-12-18 17:05:06,673 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
2025-12-18 17:05:06,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
2025-12-18 17:05:06,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
2025-12-18 17:05:06,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
2025-12-18 17:05:06,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
2025-12-18 17:05:06,863 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
2025-12-18 17:05:06,863 [main] INFO server.GrpcServicesImpl: raft.grpc.message.size.max = 34603008 (custom)
2025-12-18 17:05:06,863 [main] INFO server.GrpcServicesImpl: raft.grpc.flow.control.window = 1MB (=1048576) (default)
2025-12-18 17:05:06,864 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
2025-12-18 17:05:06,868 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
2025-12-18 17:05:06,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
2025-12-18 17:05:06,870 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
2025-12-18 17:05:06,943 [main] INFO server.GrpcServicesImpl: Setting TLS for 0.0.0.0/0.0.0.0:9872
2025-12-18 17:05:07,021 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
2025-12-18 17:05:07,023 [main] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
2025-12-18 17:05:07,023 [main] INFO server.RaftServerConfigKeys: raft.server.close.threshold = 60s (default)
2025-12-18 17:05:07,024 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
2025-12-18 17:05:07,027 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/om.ratis] (custom)
2025-12-18 17:05:07,027 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
2025-12-18 17:05:07,028 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
2025-12-18 17:05:07,033 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|om:9872] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@58052571[Not completed]
2025-12-18 17:05:07,034 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
2025-12-18 17:05:07,039 [main] INFO om.OzoneManager: Creating RPC Server
2025-12-18 17:05:07,041 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|om:9872] with OzoneManagerStateMachine-1:uninitialized
2025-12-18 17:05:07,043 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
2025-12-18 17:05:07,044 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
2025-12-18 17:05:07,044 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
2025-12-18 17:05:07,044 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
2025-12-18 17:05:07,045 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.member.majority-add = false (default)
2025-12-18 17:05:07,050 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=conf: {index: -1, cur=peers:[om1|om:9872]|listeners:[], old=null}, confs=<EMPTY_MAP>
2025-12-18 17:05:07,053 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
2025-12-18 17:05:07,055 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
2025-12-18 17:05:07,058 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
2025-12-18 17:05:07,058 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100ms (default)
2025-12-18 17:05:07,075 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.read-after-write-consistent.write-index-cache.expiry-time = 60s (default)
2025-12-18 17:05:07,183 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
2025-12-18 17:05:07,183 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
2025-12-18 17:05:07,184 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
2025-12-18 17:05:07,185 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
2025-12-18 17:05:07,186 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
2025-12-18 17:05:07,700 [main] INFO reflections.Reflections: Reflections took 623 ms to scan 8 urls, producing 24 keys and 936 values
2025-12-18 17:05:08,018 [main] INFO ipc_.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc_.DefaultRpcScheduler, ipcBackoff: false.
2025-12-18 17:05:08,022 [main] INFO ipc_.Server: Listener at om:9862
2025-12-18 17:05:08,023 [Socket Reader #1 for port 9862] INFO ipc_.Server: Starting Socket Reader #1 for port 9862
2025-12-18 17:05:08,024 [Socket Reader #3 for port 9862] INFO ipc_.Server: Starting Socket Reader #3 for port 9862
2025-12-18 17:05:08,024 [Socket Reader #2 for port 9862] INFO ipc_.Server: Starting Socket Reader #2 for port 9862
2025-12-18 17:05:08,025 [Socket Reader #4 for port 9862] INFO ipc_.Server: Starting Socket Reader #4 for port 9862
2025-12-18 17:05:08,025 [Socket Reader #6 for port 9862] INFO ipc_.Server: Starting Socket Reader #6 for port 9862
2025-12-18 17:05:08,026 [Socket Reader #8 for port 9862] INFO ipc_.Server: Starting Socket Reader #8 for port 9862
2025-12-18 17:05:08,025 [Socket Reader #5 for port 9862] INFO ipc_.Server: Starting Socket Reader #5 for port 9862
2025-12-18 17:05:08,026 [Socket Reader #7 for port 9862] INFO ipc_.Server: Starting Socket Reader #7 for port 9862
2025-12-18 17:05:08,026 [Socket Reader #9 for port 9862] INFO ipc_.Server: Starting Socket Reader #9 for port 9862
2025-12-18 17:05:08,029 [Socket Reader #10 for port 9862] INFO ipc_.Server: Starting Socket Reader #10 for port 9862
2025-12-18 17:05:09,125 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.18.0.2:9862
2025-12-18 17:05:09,129 [main] INFO keys.KeyStorage: Reading public key from /data/metadata/om/keys/public.pem.
2025-12-18 17:05:09,132 [main] INFO om.OzoneManager: Starting secret key client.
2025-12-18 17:05:09,268 [main] INFO symmetric.DefaultSecretKeySignerClient: Initial secret key fetched from SCM: SecretKey(id = a6755523-bfb6-4cb2-8fc9-f3090ea4ec15, creation at: 2025-12-18T17:04:52.813Z, expire at: 2025-12-27T17:04:52.813Z).
2025-12-18 17:05:09,269 [main] INFO symmetric.DefaultSecretKeySignerClient: Scheduling SecretKeyPoller with initial delay of PT23H59M43.544011129S and interval of PT10M
2025-12-18 17:05:09,272 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
2025-12-18 17:05:09,275 [main] INFO security.OzoneDelegationTokenSecretManager: Updated current master key for generating tokens. Cert id 6, Master key id 1
2025-12-18 17:05:09,278 [om1-ExpiredTokenRemover] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2025-12-18 17:05:09,279 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
2025-12-18 17:05:09,283 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
2025-12-18 17:05:09,283 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
2025-12-18 17:05:09,301 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/om.ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
2025-12-18 17:05:09,313 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/om.ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@om
2025-12-18 17:05:09,332 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/om.ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
2025-12-18 17:05:09,340 [om1-impl-thread1] INFO ratis.OzoneManagerStateMachine: om1: initialize group-C5BA1605619E with <INITIAL_VALUE>
2025-12-18 17:05:09,342 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
2025-12-18 17:05:09,349 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: getLatestSnapshot(OzoneManagerStateMachine-1:om1:group-C5BA1605619E) returns 0#-1
2025-12-18 17:05:09,349 [om1-impl-thread1] INFO raftlog.RaftLog: om1@group-C5BA1605619E-SegmentedRaftLog: snapshotIndexFromStateMachine = -1
2025-12-18 17:05:09,349 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
2025-12-18 17:05:09,351 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
2025-12-18 17:05:09,351 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
2025-12-18 17:05:09,358 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 67108864 (custom)
2025-12-18 17:05:09,367 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
2025-12-18 17:05:09,367 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
2025-12-18 17:05:09,369 [om1-impl-thread1] INFO util.AwaitToRun: Thread[#70,om1@group-C5BA1605619E-cacheEviction-AwaitToRun,5,main] started
2025-12-18 17:05:09,375 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/om.ratis/bf265839-605b-3f16-9796-c5ba1605619e
2025-12-18 17:05:09,375 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
2025-12-18 17:05:09,375 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
2025-12-18 17:05:09,377 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (default)
2025-12-18 17:05:09,377 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
2025-12-18 17:05:09,378 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
2025-12-18 17:05:09,378 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
2025-12-18 17:05:09,378 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
2025-12-18 17:05:09,380 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 33554440 (custom)
2025-12-18 17:05:09,386 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
2025-12-18 17:05:09,386 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
2025-12-18 17:05:09,387 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
2025-12-18 17:05:09,393 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
2025-12-18 17:05:09,393 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
2025-12-18 17:05:09,397 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=conf: {index: -1, cur=peers:[om1|om:9872]|listeners:[], old=null}
2025-12-18 17:05:09,397 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
2025-12-18 17:05:09,399 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
2025-12-18 17:05:09,400 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
2025-12-18 17:05:09,400 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
2025-12-18 17:05:09,401 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
2025-12-18 17:05:09,401 [om1-impl-thread1] INFO util.JmxRegister: register mxBean class org.apache.ratis.server.impl.RaftServerJmxAdapter as Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
2025-12-18 17:05:09,403 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-stop.enabled = true (default)
2025-12-18 17:05:09,403 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.trigger-when-remove.enabled = true (default)
2025-12-18 17:05:09,403 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
2025-12-18 17:05:09,403 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
2025-12-18 17:05:09,404 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
2025-12-18 17:05:09,404 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
2025-12-18 17:05:09,408 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: Successfully started.
2025-12-18 17:05:09,408 [main] INFO server.RaftServer: om1: start RPC server
2025-12-18 17:05:09,461 [main] INFO server.GrpcServicesImpl: om1: GrpcServicesImpl started, listening on 9872
2025-12-18 17:05:09,464 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
2025-12-18 17:05:09,465 [main] INFO om.OzoneManager: Version File has different layout version (9) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
2025-12-18 17:05:09,702 [main] INFO client.ScmTopologyClient: Initial network topology fetched from SCM: /.
2025-12-18 17:05:09,718 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-2.2.0-SNAPSHOT.jar!/network-topology-default.xml]
2025-12-18 17:05:09,720 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
2025-12-18 17:05:09,767 [main] INFO utils.BackgroundService: Starting service KeyDeletingService with interval 60000 milliseconds
2025-12-18 17:05:09,776 [main] INFO utils.BackgroundService: Starting service DirectoryDeletingService with interval 60000 milliseconds
2025-12-18 17:05:09,790 [main] INFO utils.BackgroundService: Starting service OpenKeyCleanupService with interval 86400000 milliseconds
2025-12-18 17:05:09,794 [main] INFO utils.BackgroundService: Starting service SstFilteringService with interval 60000 milliseconds
2025-12-18 17:05:09,797 [main] INFO om.KeyManagerImpl: SnapshotDefragService is disabled. Snapshot defragmentation will not run periodically.
2025-12-18 17:05:09,800 [main] INFO utils.BackgroundService: Starting service SnapshotDeletingService with interval 30000 milliseconds
2025-12-18 17:05:09,801 [main] INFO utils.BackgroundService: Starting service MultipartUploadCleanupService with interval 86400000 milliseconds
2025-12-18 17:05:09,816 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
2025-12-18 17:05:09,816 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: true Ozone Security Enabled: true Ozone HTTP Security Enabled: true 
2025-12-18 17:05:09,816 [main] INFO http.BaseHttpServer: HttpAuthType: ozone.om.http.auth.type = simple
2025-12-18 17:05:09,843 [main] INFO util.log: Logging initialized @11471ms to org.eclipse.jetty.util.log.Slf4jLog
2025-12-18 17:05:09,942 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
2025-12-18 17:05:09,944 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
2025-12-18 17:05:09,945 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2025-12-18 17:05:09,945 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2025-12-18 17:05:09,972 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /tmp/ozone_http
2025-12-18 17:05:09,978 [main] INFO http.HttpServer2: Jetty bound to port 9874
2025-12-18 17:05:09,979 [main] INFO server.Server: jetty-9.4.58.v20250814; built: 2025-08-14T02:28:49.637Z; git: 8f1440587e9e4ae7db3d74cf205643f3d707148d; jvm 21.0.2+13-58
2025-12-18 17:05:10,002 [main] INFO server.session: DefaultSessionIdManager workerName=node0
2025-12-18 17:05:10,002 [main] INFO server.session: No SessionScavenger set, using defaults
2025-12-18 17:05:10,003 [main] INFO server.session: node0 Scavenging every 660000ms
2025-12-18 17:05:10,012 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5fe9851f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
2025-12-18 17:05:10,012 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@426ee667{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-2.2.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2025-12-18 17:05:10,106 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d57feda{ozoneManager,/,file:///tmp/ozone_http/jetty-0_0_0_0-9874-ozone-manager-2_2_0-SNAPSHOT_jar-_-any-14615925080435062910/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-2.2.0-SNAPSHOT.jar!/webapps/ozoneManager}
2025-12-18 17:05:10,111 [main] INFO server.AbstractConnector: Started ServerConnector@52bd1783{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
2025-12-18 17:05:10,111 [main] INFO server.Server: Started @11739ms
2025-12-18 17:05:10,112 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
2025-12-18 17:05:10,113 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://om:9874
2025-12-18 17:05:10,113 [IPC Server listener on 9862] INFO ipc_.Server: IPC Server listener on 9862: starting
2025-12-18 17:05:10,113 [IPC Server Responder] INFO ipc_.Server: IPC Server Responder: starting
2025-12-18 17:05:10,132 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
2025-12-18 17:05:10,161 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
2025-12-18 17:05:14,484 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5085343664ns, electionTimeout:5083ms
2025-12-18 17:05:14,485 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
2025-12-18 17:05:14,485 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
2025-12-18 17:05:14,488 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
2025-12-18 17:05:14,488 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
2025-12-18 17:05:14,494 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for conf: {index: -1, cur=peers:[om1|om:9872]|listeners:[], old=null}
2025-12-18 17:05:14,494 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
2025-12-18 17:05:14,499 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for conf: {index: -1, cur=peers:[om1|om:9872]|listeners:[], old=null}
2025-12-18 17:05:14,499 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
2025-12-18 17:05:14,499 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
2025-12-18 17:05:14,499 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
2025-12-18 17:05:14,504 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
2025-12-18 17:05:14,504 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.timeout = 15600ms (fallback to raft.server.rpc.timeout.max)
2025-12-18 17:05:14,510 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
2025-12-18 17:05:14,510 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
2025-12-18 17:05:14,514 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
2025-12-18 17:05:14,514 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
2025-12-18 17:05:14,514 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
2025-12-18 17:05:14,522 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.enabled = false (default)
2025-12-18 17:05:14,524 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.read.leader.lease.timeout.ratio = 0.9 (default)
2025-12-18 17:05:14,524 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.log-metadata.enabled = false (custom)
2025-12-18 17:05:14,525 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
2025-12-18 17:05:14,527 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
2025-12-18 17:05:14,528 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set firstElectionSinceStartup to false for becomeLeader
2025-12-18 17:05:14,530 [Warm Up EDEK Cache Thread #0] INFO om.OzoneManager: Warming up 0 EDEKs... (initialDelay=3000, retryInterval=1000, maxRetries=10)
2025-12-18 17:05:14,541 [om1@group-C5BA1605619E-LeaderElection1] INFO ratis.OzoneManagerStateMachine: om1@group-C5BA1605619E: leader changed to om1
2025-12-18 17:05:14,541 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 7474ms
2025-12-18 17:05:14,564 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
2025-12-18 17:05:14,604 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.BufferedWriteChannel: open log_inprogress_0 at position 0
2025-12-18 17:05:14,609 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration conf: {index: 0, cur=peers:[om1|om:9872]|listeners:[], old=null}
2025-12-18 17:05:14,612 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/om.ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
2025-12-18 17:05:14,721 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: notifyConfigurationChanged from Ratis: term=1, index=0, New Peer list: om1(om:9872), New Listener list
2025-12-18 17:05:14,722 [om1@group-C5BA1605619E-StateMachineUpdater] INFO server.RaftServer$Division: Leader om1@group-C5BA1605619E-LeaderStateImpl is ready since appliedIndex == startIndex == 0
2025-12-18 17:05:17,531 [Warm Up EDEK Cache Thread #0] INFO om.OzoneManager: Successfully warmed up 0 EDEKs.
2025-12-18 17:05:18,931 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:46331
2025-12-18 17:05:22,077 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:37151
2025-12-18 17:05:22,964 [om1-OMStateMachineApplyTransactionThread - 0] INFO volume.OMVolumeCreateRequest: created volume:volume1 for user:hadoop
2025-12-18 17:05:25,105 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:38253
2025-12-18 17:05:26,030 [om1-OMStateMachineApplyTransactionThread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout FILE_SYSTEM_OPTIMIZED in volume: volume1
2025-12-18 17:05:28,070 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:37173
2025-12-18 17:05:31,755 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:33917
2025-12-18 17:05:34,325 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:35775
2025-12-18 17:05:39,976 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:38523
2025-12-18 17:05:45,051 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:36885
2025-12-18 17:05:50,088 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:40777
2025-12-18 17:05:52,909 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.4:39117
2025-12-18 17:05:59,375 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43181
2025-12-18 17:06:03,752 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45163
2025-12-18 17:06:06,048 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:44605
2025-12-18 17:06:09,708 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39353
2025-12-18 17:06:13,828 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46769
2025-12-18 17:06:17,601 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:33159
2025-12-18 17:06:19,700 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:44117
2025-12-18 17:06:23,332 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37573
2025-12-18 17:06:31,745 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:38325
2025-12-18 17:06:37,731 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37969
2025-12-18 17:06:40,349 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45861
2025-12-18 17:06:44,847 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:33687
2025-12-18 17:06:49,849 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43907
2025-12-18 17:06:52,678 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:42223
2025-12-18 17:06:54,140 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:38109
2025-12-18 17:06:54,161 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:44473
2025-12-18 17:06:54,975 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46851
2025-12-18 17:06:55,346 [IPC Server handler 5 on default port 9862] INFO symmetric.DefaultSecretKeyVerifierClient: Secret key fetched from SCM: SecretKey(id = a6755523-bfb6-4cb2-8fc9-f3090ea4ec15, creation at: 2025-12-18T17:04:52.813Z, expire at: 2025-12-27T17:04:52.813Z)
2025-12-18 17:06:57,910 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:56800
2025-12-18 17:07:02,371 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:56802
2025-12-18 17:07:02,986 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:56816
2025-12-18 17:07:03,006 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:56824
2025-12-18 17:07:03,042 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:56832
2025-12-18 17:07:12,280 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:50492
2025-12-18 17:07:19,086 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:39898
2025-12-18 17:07:25,204 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:39904
2025-12-18 17:07:31,077 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:47666
2025-12-18 17:07:33,462 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:47678
2025-12-18 17:07:36,097 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39125
2025-12-18 17:07:38,792 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43255
2025-12-18 17:07:39,472 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:36299
2025-12-18 17:07:40,692 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39173
2025-12-18 17:07:42,630 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39071
2025-12-18 17:07:42,654 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34173
2025-12-18 17:07:42,827 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37311
2025-12-18 17:07:43,647 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44462
2025-12-18 17:07:46,353 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44474
2025-12-18 17:07:46,940 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48212
2025-12-18 17:07:46,960 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48228
2025-12-18 17:07:46,978 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48242
2025-12-18 17:07:56,355 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48244
2025-12-18 17:08:02,110 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:45874
2025-12-18 17:08:08,276 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:58400
2025-12-18 17:08:09,784 [om1-KeyDeletingService#4] INFO service.KeyDeletingService: Send 3 key(s) to SCM, first 3 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/COMMIT_STARTED/-9223372036854762496/0', deletedBlocks=[ localID: 115816896921600018 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/COMMIT_STARTED/-9223372036854762496'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/_SUCCESS/-9223372036854761728/0', deletedBlocks=[ localID: 115816896921600019 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/_SUCCESS/-9223372036854761728'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/COMMIT_SUCCESS/-9223372036854761216/0', deletedBlocks=[ localID: 115816896921600020 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/COMMIT_SUCCESS/-9223372036854761216'}]
2025-12-18 17:08:09,872 [om1-OMStateMachineApplyTransactionThread - 0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMSystemAudit to [].
2025-12-18 17:08:09,878 [om1-KeyDeletingService#4] INFO service.KeyDeletingService: 3 BlockGroup deletion are acked by SCM in 93 ms
2025-12-18 17:08:09,883 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 3, Number of sub-dir deleted: 5, Number of sub-files moved: 14 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 84ms,  totalRunCount: 3
2025-12-18 17:08:09,913 [om1-OMStateMachineApplyTransactionThread - 0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMSystemAudit to [].
2025-12-18 17:08:09,915 [om1-KeyDeletingService#4] INFO service.KeyDeletingService: Blocks for 3 (out of 3) keys are deleted from DB in 38 ms. Limit per task is 50000.
2025-12-18 17:08:13,230 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:58408
2025-12-18 17:08:15,494 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:58422
2025-12-18 17:08:20,633 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45889
2025-12-18 17:08:22,682 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41501
2025-12-18 17:08:25,299 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:42023
2025-12-18 17:08:27,704 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:38281
2025-12-18 17:08:32,008 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:33803
2025-12-18 17:08:36,957 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:35001
2025-12-18 17:08:37,353 [om1-OMStateMachineApplyTransactionThread - 0] INFO volume.OMVolumeCreateRequest: created volume:user for user:hadoop
2025-12-18 17:08:37,393 [om1-OMStateMachineApplyTransactionThread - 0] INFO bucket.OMBucketCreateRequest: created bucket: hadoop of layout FILE_SYSTEM_OPTIMIZED in volume: user
2025-12-18 17:08:39,578 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43897
2025-12-18 17:08:40,771 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37661
2025-12-18 17:08:40,787 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34861
2025-12-18 17:08:40,924 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34063
2025-12-18 17:08:41,763 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43076
2025-12-18 17:08:44,301 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43088
2025-12-18 17:08:44,858 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43098
2025-12-18 17:08:44,878 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43110
2025-12-18 17:08:44,910 [om1-OMStateMachineApplyTransactionThread - 0] INFO bucket.OMBucketCreateRequest: created bucket: history of layout FILE_SYSTEM_OPTIMIZED in volume: user
2025-12-18 17:08:44,932 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43122
2025-12-18 17:08:53,979 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:46324
2025-12-18 17:09:00,822 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43608
2025-12-18 17:09:06,990 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:57610
2025-12-18 17:09:09,790 [om1-KeyDeletingService#5] INFO service.KeyDeletingService: Send 17 key(s) to SCM, first 10 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/COMMIT_STARTED/-9223372036854752256/0', deletedBlocks=[ localID: 115816896921600031 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/COMMIT_STARTED/-9223372036854752256'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077608377_1411287306/in/part2/-9223372036854769151/0', deletedBlocks=[ localID: 115816896921600009 containerID: 1 size: 118 replicatedSize: 354]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=354, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077608377_1411287306/in/part2/-9223372036854769151'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.xml/-9223372036854766335/0', deletedBlocks=[ localID: 115816896921600013 containerID: 2 size: 315638 replicatedSize: 946914]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=946914, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.xml/-9223372036854766335'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.jar/-9223372036854767871/0', deletedBlocks=[ localID: 115816896921600010 containerID: 2 size: 281348 replicatedSize: 844044]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=844044, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.jar/-9223372036854767871'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/_SUCCESS/-9223372036854751488/0', deletedBlocks=[ localID: 115816896921600032 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/_SUCCESS/-9223372036854751488'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job_1766077591739_0001_1.jhist/-9223372036854764799/0', deletedBlocks=[ localID: 115816896921600014 containerID: 1 size: 29705 replicatedSize: 89115]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=89115, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job_1766077591739_0001_1.jhist/-9223372036854764799'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job_1766077591739_0001_1_conf.xml/-9223372036854764543/0', deletedBlocks=[ localID: 115816896921600015 containerID: 1 size: 365392 replicatedSize: 1096176]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1096176, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job_1766077591739_0001_1_conf.xml/-9223372036854764543'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/COMMIT_STARTED/-9223372036854762751/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/COMMIT_STARTED/-9223372036854762751'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077608377_1411287306/out/part-r-00000/-9223372036854764029/0', deletedBlocks=[ localID: 115816896921600016 containerID: 2 size: 97 replicatedSize: 291]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=291, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077608377_1411287306/out/part-r-00000/-9223372036854764029'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.splitmetainfo/-9223372036854766847/0', deletedBlocks=[ localID: 115816896921600012 containerID: 2 size: 463 replicatedSize: 1389]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1389, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0001/job.splitmetainfo/-9223372036854766847'}]
2025-12-18 17:09:09,818 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 2, Number of sub-dir deleted: 3, Number of sub-files moved: 8 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 19ms,  totalRunCount: 4
2025-12-18 17:09:09,832 [om1-KeyDeletingService#5] INFO service.KeyDeletingService: 17 BlockGroup deletion are acked by SCM in 43 ms
2025-12-18 17:09:09,847 [om1-KeyDeletingService#5] INFO service.KeyDeletingService: Blocks for 17 (out of 17) keys are deleted from DB in 15 ms. Limit per task is 50000.
2025-12-18 17:09:11,943 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:57624
2025-12-18 17:09:14,286 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:57640
2025-12-18 17:09:16,585 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41515
2025-12-18 17:09:19,288 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46309
2025-12-18 17:09:19,887 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:44195
2025-12-18 17:09:21,487 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43075
2025-12-18 17:09:22,928 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37791
2025-12-18 17:09:22,951 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39563
2025-12-18 17:09:23,118 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41083
2025-12-18 17:09:23,509 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:38522
2025-12-18 17:09:25,980 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:38532
2025-12-18 17:09:26,487 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:36188
2025-12-18 17:09:26,504 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:36196
2025-12-18 17:09:26,520 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:36206
2025-12-18 17:09:35,862 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:36220
2025-12-18 17:09:42,520 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:58864
2025-12-18 17:09:48,698 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:49352
2025-12-18 17:09:53,572 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:49358
2025-12-18 17:09:56,795 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:42024
2025-12-18 17:10:08,608 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41923
2025-12-18 17:10:09,789 [om1-KeyDeletingService#2] INFO service.KeyDeletingService: Send 14 key(s) to SCM, first 10 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/COMMIT_STARTED/-9223372036854737664/0', deletedBlocks=[ localID: 115816896921600049 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/COMMIT_STARTED/-9223372036854737664'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/COMMIT_SUCCESS/-9223372036854751231/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/COMMIT_SUCCESS/-9223372036854751231'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job_1766077591739_0002_1_conf.xml/-9223372036854754303/0', deletedBlocks=[ localID: 115816896921600029 containerID: 2 size: 365055 replicatedSize: 1095165]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1095165, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job_1766077591739_0002_1_conf.xml/-9223372036854754303'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.jar/-9223372036854757375/0', deletedBlocks=[ localID: 115816896921600024 containerID: 1 size: 281348 replicatedSize: 844044]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=844044, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.jar/-9223372036854757375'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/COMMIT_STARTED/-9223372036854727936/0', deletedBlocks=[ localID: 115816896921600062 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/COMMIT_STARTED/-9223372036854727936'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/_SUCCESS/-9223372036854727168/0', deletedBlocks=[ localID: 115816896921600063 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/_SUCCESS/-9223372036854727168'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.splitmetainfo/-9223372036854756095/0', deletedBlocks=[ localID: 115816896921600026 containerID: 2 size: 467 replicatedSize: 1401]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1401, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.splitmetainfo/-9223372036854756095'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/COMMIT_SUCCESS/-9223372036854726656/0', deletedBlocks=[ localID: 115816896921600064 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/COMMIT_SUCCESS/-9223372036854726656'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/COMMIT_STARTED/-9223372036854752511/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/COMMIT_STARTED/-9223372036854752511'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.split/-9223372036854756607/0', deletedBlocks=[ localID: 115816896921600025 containerID: 1 size: 310 replicatedSize: 930]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=930, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077591739_0002/job.split/-9223372036854756607'}]
2025-12-18 17:10:09,805 [om1-KeyDeletingService#2] INFO service.KeyDeletingService: 14 BlockGroup deletion are acked by SCM in 17 ms
2025-12-18 17:10:09,811 [om1-KeyDeletingService#2] INFO service.KeyDeletingService: Blocks for 14 (out of 14) keys are deleted from DB in 6 ms. Limit per task is 50000.
2025-12-18 17:10:09,832 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 5, Number of sub-dir deleted: 8, Number of sub-files moved: 22 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 30ms,  totalRunCount: 5
2025-12-18 17:10:15,223 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46719
2025-12-18 17:10:17,607 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:37359
2025-12-18 17:10:21,495 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34965
2025-12-18 17:10:26,216 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:33405
2025-12-18 17:10:28,571 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:42863
2025-12-18 17:10:29,620 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39319
2025-12-18 17:10:29,643 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:32907
2025-12-18 17:10:30,373 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41975
2025-12-18 17:10:32,469 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44982
2025-12-18 17:10:36,403 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44986
2025-12-18 17:10:36,935 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44774
2025-12-18 17:10:36,961 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44778
2025-12-18 17:10:36,985 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44784
2025-12-18 17:10:45,979 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:44786
2025-12-18 17:10:51,796 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43262
2025-12-18 17:10:57,790 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:47398
2025-12-18 17:11:02,875 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:47402
2025-12-18 17:11:05,118 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:47406
2025-12-18 17:11:07,310 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:42743
2025-12-18 17:11:09,784 [om1-KeyDeletingService#7] INFO service.KeyDeletingService: Send 25 key(s) to SCM, first 10 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0004/job_1766077591739_0004_1.jhist/-9223372036854729727/0', deletedBlocks=[ localID: 115816896921600059 containerID: 2 size: 30012 replicatedSize: 90036]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=90036, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0004/job_1766077591739_0004_1.jhist/-9223372036854729727'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0003/COMMIT_SUCCESS/-9223372036854736639/0', deletedBlocks=[]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=0, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0003/COMMIT_SUCCESS/-9223372036854736639'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/_SUCCESS/-9223372036854714368/0', deletedBlocks=[ localID: 115816896921600081 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/_SUCCESS/-9223372036854714368'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0004/COMMIT_SUCCESS/-9223372036854726911/0', deletedBlocks=[]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=0, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0004/COMMIT_SUCCESS/-9223372036854726911'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0003/job.jar/-9223372036854743807/0', deletedBlocks=[ localID: 115816896921600041 containerID: 2 size: 281348 replicatedSize: 844044]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=844044, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0003/job.jar/-9223372036854743807'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0004/job_1766077591739_0004_1_conf.xml/-9223372036854729471/0', deletedBlocks=[ localID: 115816896921600060 containerID: 1 size: 365034 replicatedSize: 1095102]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=1095102, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0004/job_1766077591739_0004_1_conf.xml/-9223372036854729471'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0004/job.jar/-9223372036854732543/0', deletedBlocks=[ localID: 115816896921600055 containerID: 2 size: 281348 replicatedSize: 844044]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=844044, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0004/job.jar/-9223372036854732543'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/COMMIT_STARTED/-9223372036854715136/0', deletedBlocks=[ localID: 115816896921600080 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/COMMIT_STARTED/-9223372036854715136'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0003/COMMIT_STARTED/-9223372036854737919/0', deletedBlocks=[]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=0, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0003/COMMIT_STARTED/-9223372036854737919'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/.staging/job_1766077591739_0004/job.splitmetainfo/-9223372036854731263/0', deletedBlocks=[ localID: 115816896921600057 containerID: 1 size: 467 replicatedSize: 1401]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=1401, isCommittedKey=true, deleteKeyName='/user/hadoop/.staging/job_1766077591739_0004/job.splitmetainfo/-9223372036854731263'}]
2025-12-18 17:11:09,796 [om1-KeyDeletingService#7] INFO service.KeyDeletingService: 25 BlockGroup deletion are acked by SCM in 11 ms
2025-12-18 17:11:09,804 [om1-KeyDeletingService#7] INFO service.KeyDeletingService: Blocks for 25 (out of 25) keys are deleted from DB in 8 ms. Limit per task is 50000.
2025-12-18 17:11:09,815 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 3, Number of sub-dir deleted: 5, Number of sub-files moved: 14 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 14ms,  totalRunCount: 6
2025-12-18 17:11:09,929 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:44765
2025-12-18 17:11:10,537 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:35557
2025-12-18 17:11:11,792 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:49434
2025-12-18 17:11:12,308 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:35737
2025-12-18 17:11:13,427 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45105
2025-12-18 17:11:13,447 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34945
2025-12-18 17:11:13,577 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:42123
2025-12-18 17:11:14,305 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:49436
2025-12-18 17:11:16,852 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:37372
2025-12-18 17:11:17,325 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:37374
2025-12-18 17:11:17,345 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:37378
2025-12-18 17:11:17,365 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:37382
2025-12-18 17:11:26,060 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:37398
2025-12-18 17:11:32,080 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43348
2025-12-18 17:11:37,939 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:38496
2025-12-18 17:11:42,992 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:38502
2025-12-18 17:11:45,198 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:38508
2025-12-18 17:11:50,101 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:36283
2025-12-18 17:11:51,765 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:57054
2025-12-18 17:11:52,235 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45901
2025-12-18 17:11:54,356 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:41627
2025-12-18 17:11:56,682 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:36819
2025-12-18 17:12:00,853 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46813
2025-12-18 17:12:05,639 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:45815
2025-12-18 17:12:07,995 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:35303
2025-12-18 17:12:09,170 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34769
2025-12-18 17:12:09,188 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:40249
2025-12-18 17:12:09,321 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:40623
2025-12-18 17:12:09,784 [om1-KeyDeletingService#6] INFO service.KeyDeletingService: Send 17 key(s) to SCM, first 10 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077824780_844444987/out/reduce-out/-9223372036854716415/0', deletedBlocks=[ localID: 115816896921600079 containerID: 1 size: 118 replicatedSize: 354]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=354, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077824780_844444987/out/reduce-out/-9223372036854716415'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/COMMIT_SUCCESS/-9223372036854703616/0', deletedBlocks=[ localID: 115816896921600095 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/volume1/bucket1/COMMIT_SUCCESS/-9223372036854703616'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.jar/-9223372036854719999/0', deletedBlocks=[ localID: 115816896921600072 containerID: 1 size: 281633 replicatedSize: 844899]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=844899, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.jar/-9223372036854719999'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.splitmetainfo/-9223372036854718975/0', deletedBlocks=[ localID: 115816896921600074 containerID: 2 size: 463 replicatedSize: 1389]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1389, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.splitmetainfo/-9223372036854718975'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077824780_844444987/in/part1/-9223372036854721535/0', deletedBlocks=[ localID: 115816896921600070 containerID: 2 size: 118 replicatedSize: 354]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=354, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/QuasiMonteCarlo_1766077824780_844444987/in/part1/-9223372036854721535'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job_1766077808500_0001_1.jhist/-9223372036854717439/0', deletedBlocks=[ localID: 115816896921600076 containerID: 2 size: 29701 replicatedSize: 89103]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=89103, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job_1766077808500_0001_1.jhist/-9223372036854717439'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/COMMIT_SUCCESS/-9223372036854714111/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/COMMIT_SUCCESS/-9223372036854714111'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job_1766077808500_0001_1_conf.xml/-9223372036854717183/0', deletedBlocks=[ localID: 115816896921600077 containerID: 2 size: 398053 replicatedSize: 1194159]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1194159, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job_1766077808500_0001_1_conf.xml/-9223372036854717183'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.split/-9223372036854719487/0', deletedBlocks=[ localID: 115816896921600073 containerID: 1 size: 460 replicatedSize: 1380]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1380, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.split/-9223372036854719487'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.xml/-9223372036854718463/0', deletedBlocks=[ localID: 115816896921600075 containerID: 2 size: 343931 replicatedSize: 1031793]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1031793, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0001/job.xml/-9223372036854718463'}]
2025-12-18 17:12:09,789 [om1-KeyDeletingService#6] INFO service.KeyDeletingService: 17 BlockGroup deletion are acked by SCM in 5 ms
2025-12-18 17:12:09,792 [om1-KeyDeletingService#6] INFO service.KeyDeletingService: Blocks for 17 (out of 17) keys are deleted from DB in 4 ms. Limit per task is 50000.
2025-12-18 17:12:09,806 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 2, Number of sub-dir deleted: 3, Number of sub-files moved: 8 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 5ms,  totalRunCount: 7
2025-12-18 17:12:10,307 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:33568
2025-12-18 17:12:12,871 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:33584
2025-12-18 17:12:13,400 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:33588
2025-12-18 17:12:13,423 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:33590
2025-12-18 17:12:13,441 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:33598
2025-12-18 17:12:22,368 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:35362
2025-12-18 17:12:28,189 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48776
2025-12-18 17:12:34,236 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:48778
2025-12-18 17:12:39,329 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:42774
2025-12-18 17:12:41,370 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:42786
2025-12-18 17:12:43,139 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:39801
2025-12-18 17:12:45,805 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:43271
2025-12-18 17:12:46,339 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:36761
2025-12-18 17:12:48,004 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43376
2025-12-18 17:12:48,493 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:46763
2025-12-18 17:12:49,297 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:34267
2025-12-18 17:12:49,318 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:32919
2025-12-18 17:12:49,455 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for rm/rm@EXAMPLE.COM (auth:KERBEROS) from 172.18.0.10:33867
2025-12-18 17:12:50,499 [Socket Reader #9 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43388
2025-12-18 17:12:53,004 [Socket Reader #10 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43400
2025-12-18 17:12:53,557 [Socket Reader #1 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43414
2025-12-18 17:12:53,577 [Socket Reader #2 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43418
2025-12-18 17:12:53,594 [Socket Reader #3 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:43426
2025-12-18 17:13:02,472 [Socket Reader #4 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:39388
2025-12-18 17:13:08,337 [Socket Reader #5 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:50718
2025-12-18 17:13:09,786 [om1-KeyDeletingService#8] INFO service.KeyDeletingService: Send 11 key(s) to SCM, first 10 keys: [PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/COMMIT_STARTED/-9223372036854705151/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/COMMIT_STARTED/-9223372036854705151'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/_SUCCESS/-9223372036854691072/0', deletedBlocks=[ localID: 115816896921600112 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/_SUCCESS/-9223372036854691072'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job_1766077808500_0002_1.jhist/-9223372036854706687/0', deletedBlocks=[ localID: 115816896921600090 containerID: 1 size: 30137 replicatedSize: 90411]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=90411, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job_1766077808500_0002_1.jhist/-9223372036854706687'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job_1766077808500_0002_1_conf.xml/-9223372036854706431/0', deletedBlocks=[ localID: 115816896921600091 containerID: 1 size: 397718 replicatedSize: 1193154]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1193154, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job_1766077808500_0002_1_conf.xml/-9223372036854706431'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/COMMIT_SUCCESS/-9223372036854703871/0', deletedBlocks=[]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=0, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/COMMIT_SUCCESS/-9223372036854703871'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/COMMIT_SUCCESS/-9223372036854690560/0', deletedBlocks=[ localID: 115816896921600113 containerID: 1 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/COMMIT_SUCCESS/-9223372036854690560'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.splitmetainfo/-9223372036854708223/0', deletedBlocks=[ localID: 115816896921600088 containerID: 2 size: 467 replicatedSize: 1401]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1401, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.splitmetainfo/-9223372036854708223'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.jar/-9223372036854709503/0', deletedBlocks=[ localID: 115816896921600086 containerID: 1 size: 281633 replicatedSize: 844899]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=844899, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.jar/-9223372036854709503'}, PurgedKey{blockGroup=BlockGroup[groupID='/user/hadoop/COMMIT_STARTED/-9223372036854691840/0', deletedBlocks=[ localID: 115816896921600111 containerID: 2 size: 268435456 replicatedSize: 805306368]], volume='user', bucket='hadoop', bucketId=-9223372036854746624, purgedBytes=805306368, isCommittedKey=false, deleteKeyName='/user/hadoop/COMMIT_STARTED/-9223372036854691840'}, PurgedKey{blockGroup=BlockGroup[groupID='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.xml/-9223372036854707711/0', deletedBlocks=[ localID: 115816896921600089 containerID: 2 size: 343620 replicatedSize: 1030860]], volume='volume1', bucket='bucket1', bucketId=-9223372036854775296, purgedBytes=1030860, isCommittedKey=true, deleteKeyName='/volume1/bucket1/user/hadoop/.staging/job_1766077808500_0002/job.xml/-9223372036854707711'}]
2025-12-18 17:13:09,815 [om1-KeyDeletingService#8] INFO service.KeyDeletingService: 11 BlockGroup deletion are acked by SCM in 29 ms
2025-12-18 17:13:09,819 [pool-37-thread-1] INFO service.DirectoryDeletingService: Number of dirs deleted: 3, Number of sub-dir deleted: 5, Number of sub-files moved: 14 to DeletedTable, Number of sub-dirs moved 0 to DeletedDirectoryTable, iteration elapsed: 16ms,  totalRunCount: 8
2025-12-18 17:13:09,821 [om1-KeyDeletingService#8] INFO service.KeyDeletingService: Blocks for 11 (out of 11) keys are deleted from DB in 7 ms. Limit per task is 50000.
2025-12-18 17:13:14,385 [Socket Reader #6 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:50724
2025-12-18 17:13:19,366 [Socket Reader #7 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:52096
2025-12-18 17:13:21,565 [Socket Reader #8 for port 9862] INFO ipc_.Server: Auth successful for hadoop/rm@EXAMPLE.COM (auth:TOKEN) from 172.18.0.11:52106
