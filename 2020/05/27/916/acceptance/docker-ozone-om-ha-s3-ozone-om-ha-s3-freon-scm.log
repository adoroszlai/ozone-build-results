Attaching to ozone-om-ha-s3_om3_1, ozone-om-ha-s3_s3g_1, ozone-om-ha-s3_datanode_2, ozone-om-ha-s3_om1_1, ozone-om-ha-s3_datanode_1, ozone-om-ha-s3_datanode_3, ozone-om-ha-s3_scm_1, ozone-om-ha-s3_om2_1
datanode_2  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2  | 2020-05-27 13:56:27,663 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2  | /************************************************************
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2  | STARTUP_MSG:   host = 5549b6933d8c/172.21.0.5
datanode_2  | STARTUP_MSG:   args = []
datanode_2  | STARTUP_MSG:   version = 3.2.1
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
datanode_2  | STARTUP_MSG:   java = 11.0.6
datanode_2  | ************************************************************/
datanode_2  | 2020-05-27 13:56:27,717 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2020-05-27 13:56:29,426 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2020-05-27 13:56:29,988 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2  | 2020-05-27 13:56:31,166 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 2020-05-27 13:56:31,175 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2  | 2020-05-27 13:56:31,554 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5549b6933d8c ip:172.21.0.5
datanode_2  | 2020-05-27 13:56:31,807 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 2020-05-27 13:56:31,818 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_2  | 2020-05-27 13:56:31,820 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2020-05-27 13:56:31,866 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2  | 2020-05-27 13:56:32,072 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2020-05-27 13:56:36,790 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2020-05-27 13:56:36,934 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_2  | 2020-05-27 13:56:37,151 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_2  | 2020-05-27 13:56:37,151 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2  | 2020-05-27 13:56:37,152 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-05-27 13:56:37,153 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2  | 2020-05-27 13:56:37,155 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-05-27 13:56:37,915 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-05-27 13:56:38,715 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2  | 2020-05-27 13:56:38,830 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2  | 2020-05-27 13:56:38,975 [main] INFO util.log: Logging initialized @16038ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | 2020-05-27 13:56:39,330 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2020-05-27 13:56:39,346 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2020-05-27 13:56:39,350 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2020-05-27 13:56:39,357 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode_2  | 2020-05-27 13:56:39,357 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode_2  | 2020-05-27 13:56:39,358 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode_2  | 2020-05-27 13:56:39,417 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_2  | 2020-05-27 13:56:39,433 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2  | 2020-05-27 13:56:39,442 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_2  | 2020-05-27 13:56:39,494 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2  | 2020-05-27 13:56:39,494 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2  | 2020-05-27 13:56:39,500 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2  | 2020-05-27 13:56:39,520 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2020-05-27 13:56:39,535 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f0f7849{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2020-05-27 13:56:39,536 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77bbadc{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2020-05-27 13:56:39,790 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3f50b680{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-17325092459884051928.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2  | 2020-05-27 13:56:39,813 [main] INFO server.AbstractConnector: Started ServerConnector@20ab3e3a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2020-05-27 13:56:39,814 [main] INFO server.Server: Started @16877ms
datanode_2  | 2020-05-27 13:56:39,817 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2  | 2020-05-27 13:56:39,817 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2  | 2020-05-27 13:56:39,822 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2020-05-27 13:56:39,896 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7e54f27f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 2020-05-27 13:56:40,393 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2  | 2020-05-27 13:56:42,941 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-05-27 13:56:43,472 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2  | 2020-05-27 13:56:43,473 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2  | 2020-05-27 13:56:43,473 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a22687bf-1be9-4837-b2b6-cbf11c380a1a at port 9858
datanode_2  | 2020-05-27 13:56:43,492 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start RPC server
datanode_2  | 2020-05-27 13:56:43,843 [Datanode State Machine Thread - 0] INFO server.GrpcService: a22687bf-1be9-4837-b2b6-cbf11c380a1a: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2  | 2020-05-27 13:56:46,941 [Command processor thread] INFO impl.RaftServerProxy: a22687bf-1be9-4837-b2b6-cbf11c380a1a: addNew group-66E4E1AC37E8:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858] returns group-66E4E1AC37E8:java.util.concurrent.CompletableFuture@5afa4390[Not completed]
datanode_2  | 2020-05-27 13:56:47,098 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a: new RaftServerImpl for group-66E4E1AC37E8:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-05-27 13:56:47,119 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-05-27 13:56:47,127 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-05-27 13:56:47,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-05-27 13:56:47,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-05-27 13:56:47,130 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-05-27 13:56:47,145 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: ConfigurationManager, init=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-05-27 13:56:47,145 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-05-27 13:56:47,197 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-05-27 13:56:47,202 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b7e70a29-c087-4293-8da3-66e4e1ac37e8 does not exist. Creating ...
datanode_2  | 2020-05-27 13:56:47,245 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b7e70a29-c087-4293-8da3-66e4e1ac37e8/in_use.lock acquired by nodename 6@5549b6933d8c
datanode_2  | 2020-05-27 13:56:47,258 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b7e70a29-c087-4293-8da3-66e4e1ac37e8 has been successfully formatted.
datanode_2  | 2020-05-27 13:56:47,280 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-66E4E1AC37E8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-05-27 13:56:47,282 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-05-27 13:56:47,285 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-05-27 13:56:47,292 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-05-27 13:56:47,328 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-05-27 13:56:47,342 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:47,374 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_2  | 2020-05-27 13:56:47,436 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-05-27 13:56:47,461 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/b7e70a29-c087-4293-8da3-66e4e1ac37e8
datanode_2  | 2020-05-27 13:56:47,474 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-05-27 13:56:47,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-05-27 13:56:47,580 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:47,583 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-05-27 13:56:47,583 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:47,584 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-05-27 13:56:47,587 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-05-27 13:56:47,594 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-05-27 13:56:47,594 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-05-27 13:56:47,695 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-05-27 13:56:47,730 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-05-27 13:56:47,770 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-05-27 13:56:47,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-05-27 13:56:47,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-05-27 13:56:47,792 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2020-05-27 13:56:47,796 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-05-27 13:56:47,861 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8
datanode_2  | 2020-05-27 13:56:47,872 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8
datanode_2  | 2020-05-27 13:56:47,888 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: start as a follower, conf=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858], old=null
datanode_2  | 2020-05-27 13:56:47,899 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-05-27 13:56:47,904 [pool-19-thread-1] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start FollowerState
datanode_2  | 2020-05-27 13:56:47,921 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-66E4E1AC37E8,id=a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_2  | 2020-05-27 13:56:47,923 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8
datanode_2  | 2020-05-27 13:56:47,988 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "b7e70a29-c087-4293-8da3-66e4e1ac37e8"
datanode_2  | .
datanode_2  | 2020-05-27 13:56:48,003 [Command processor thread] INFO impl.RaftServerProxy: a22687bf-1be9-4837-b2b6-cbf11c380a1a: addNew group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] returns group-330873A83A56:java.util.concurrent.CompletableFuture@181ded2f[Not completed]
datanode_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2020-05-27 13:56:23,025 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = 5fb60f47d154/172.21.0.2
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 3.2.1
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
datanode_1  | STARTUP_MSG:   java = 11.0.6
datanode_1  | ************************************************************/
datanode_1  | 2020-05-27 13:56:23,171 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2020-05-27 13:56:24,426 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2020-05-27 13:56:25,213 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2020-05-27 13:56:26,592 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2020-05-27 13:56:26,592 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1  | 2020-05-27 13:56:26,982 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5fb60f47d154 ip:172.21.0.2
datanode_1  | 2020-05-27 13:56:27,370 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1  | 2020-05-27 13:56:27,387 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_1  | 2020-05-27 13:56:27,396 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1  | 2020-05-27 13:56:27,459 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1  | 2020-05-27 13:56:27,529 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2020-05-27 13:56:33,139 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2020-05-27 13:56:33,381 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_1  | 2020-05-27 13:56:33,725 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_1  | 2020-05-27 13:56:33,743 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1  | 2020-05-27 13:56:33,747 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-05-27 13:56:33,755 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1  | 2020-05-27 13:56:33,756 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-05-27 13:56:35,069 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-05-27 13:56:35,895 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1  | 2020-05-27 13:56:35,983 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1  | 2020-05-27 13:56:36,098 [main] INFO util.log: Logging initialized @15518ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1  | 2020-05-27 13:56:36,531 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1  | 2020-05-27 13:56:36,540 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2020-05-27 13:56:36,555 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2020-05-27 13:56:36,558 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode_1  | 2020-05-27 13:56:36,559 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode_1  | 2020-05-27 13:56:36,559 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode_1  | 2020-05-27 13:56:36,693 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_1  | 2020-05-27 13:56:36,725 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1  | 2020-05-27 13:56:36,739 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_1  | 2020-05-27 13:56:36,910 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2020-05-27 13:56:36,912 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 2020-05-27 13:56:36,913 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1  | 2020-05-27 13:56:36,973 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1  | 2020-05-27 13:56:36,992 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3ee0b4f7{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2020-05-27 13:56:37,007 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7ceb4478{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2020-05-27 13:56:37,417 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-1876555299452778161.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2020-05-27 13:56:37,452 [main] INFO server.AbstractConnector: Started ServerConnector@709ed6f3{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1  | 2020-05-27 13:56:37,452 [main] INFO server.Server: Started @16873ms
datanode_1  | 2020-05-27 13:56:37,466 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1  | 2020-05-27 13:56:37,466 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  | 2020-05-27 13:56:37,483 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1  | 2020-05-27 13:56:37,659 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@e9b55a5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1  | 2020-05-27 13:56:38,269 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1  | 2020-05-27 13:56:40,724 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-05-27 13:56:41,724 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-05-27 13:56:42,725 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-05-27 13:56:43,412 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1  | 2020-05-27 13:56:43,413 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1  | 2020-05-27 13:56:43,415 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 64aa2774-d5fb-40de-bd90-35cfd81f6839 at port 9858
datanode_1  | 2020-05-27 13:56:43,453 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start RPC server
datanode_1  | 2020-05-27 13:56:43,752 [Datanode State Machine Thread - 0] INFO server.GrpcService: 64aa2774-d5fb-40de-bd90-35cfd81f6839: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1  | 2020-05-27 13:56:48,707 [Command processor thread] INFO impl.RaftServerProxy: 64aa2774-d5fb-40de-bd90-35cfd81f6839: addNew group-B3A618137565:[64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] returns group-B3A618137565:java.util.concurrent.CompletableFuture@118b62d[Not completed]
datanode_1  | 2020-05-27 13:56:48,805 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839: new RaftServerImpl for group-B3A618137565:[64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-05-27 13:56:48,822 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-05-27 13:56:48,831 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-05-27 13:56:48,831 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-05-27 13:56:48,831 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-05-27 13:56:48,832 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-05-27 13:56:48,842 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: ConfigurationManager, init=-1: [64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-05-27 13:56:48,863 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-05-27 13:56:48,871 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-05-27 13:56:48,872 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a0748133-47ae-4a27-85f0-b3a618137565 does not exist. Creating ...
datanode_1  | 2020-05-27 13:56:48,900 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a0748133-47ae-4a27-85f0-b3a618137565/in_use.lock acquired by nodename 6@5fb60f47d154
datanode_1  | 2020-05-27 13:56:48,910 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a0748133-47ae-4a27-85f0-b3a618137565 has been successfully formatted.
datanode_1  | 2020-05-27 13:56:48,913 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B3A618137565: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-05-27 13:56:48,914 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-05-27 13:56:48,935 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-05-27 13:56:48,963 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-05-27 13:56:48,967 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-05-27 13:56:48,968 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,052 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.64aa2774-d5fb-40de-bd90-35cfd81f6839
datanode_1  | 2020-05-27 13:56:49,240 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-05-27 13:56:49,306 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/a0748133-47ae-4a27-85f0-b3a618137565
datanode_1  | 2020-05-27 13:56:49,325 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-05-27 13:56:49,358 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-05-27 13:56:49,359 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,363 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-05-27 13:56:49,363 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,364 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-05-27 13:56:49,367 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-05-27 13:56:49,367 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-05-27 13:56:49,368 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-05-27 13:56:49,515 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-05-27 13:56:49,526 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-05-27 13:56:49,547 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-05-27 13:56:49,548 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-05-27 13:56:49,557 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-05-27 13:56:49,557 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1  | 2020-05-27 13:56:49,563 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-05-27 13:56:49,656 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565
datanode_1  | 2020-05-27 13:56:49,671 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565
datanode_1  | 2020-05-27 13:56:49,692 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: start as a follower, conf=-1: [64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_1  | 2020-05-27 13:56:49,693 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-05-27 13:56:49,696 [pool-19-thread-1] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start FollowerState
datanode_1  | 2020-05-27 13:56:49,714 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B3A618137565,id=64aa2774-d5fb-40de-bd90-35cfd81f6839
datanode_1  | 2020-05-27 13:56:49,715 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565
datanode_2  | 2020-05-27 13:56:48,055 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a: new RaftServerImpl for group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-05-27 13:56:48,063 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-05-27 13:56:48,063 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-05-27 13:56:48,063 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-05-27 13:56:48,065 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-05-27 13:56:48,075 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-05-27 13:56:48,075 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: ConfigurationManager, init=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-05-27 13:56:48,075 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-05-27 13:56:48,076 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-05-27 13:56:48,076 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 does not exist. Creating ...
datanode_2  | 2020-05-27 13:56:48,103 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/in_use.lock acquired by nodename 6@5549b6933d8c
datanode_2  | 2020-05-27 13:56:48,111 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 has been successfully formatted.
datanode_2  | 2020-05-27 13:56:48,112 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-330873A83A56: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-05-27 13:56:48,117 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-05-27 13:56:48,127 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-05-27 13:56:48,127 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-05-27 13:56:48,128 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-05-27 13:56:48,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-05-27 13:56:48,130 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-05-27 13:56:48,130 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-05-27 13:56:48,167 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-05-27 13:56:48,167 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-05-27 13:56:48,168 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-05-27 13:56:48,168 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2  | 2020-05-27 13:56:48,168 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-05-27 13:56:48,168 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56
datanode_2  | 2020-05-27 13:56:48,169 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56
datanode_2  | 2020-05-27 13:56:48,170 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: start as a follower, conf=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_2  | 2020-05-27 13:56:48,171 [pool-19-thread-1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-05-27 13:56:48,171 [pool-19-thread-1] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start FollowerState
datanode_2  | 2020-05-27 13:56:48,172 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-330873A83A56,id=a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_2  | 2020-05-27 13:56:48,175 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56
datanode_2  | 2020-05-27 13:56:51,658 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "b321e0da-40fa-404a-8ad9-330873a83a56"
datanode_2  | .
datanode_2  | 2020-05-27 13:56:52,958 [Thread-20] INFO impl.FollowerState: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-FollowerState: change to CANDIDATE, lastRpcTime:5054ms, electionTimeout:5036ms
datanode_2  | 2020-05-27 13:56:52,959 [Thread-20] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: shutdown FollowerState
datanode_1  | 2020-05-27 13:56:49,740 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "a0748133-47ae-4a27-85f0-b3a618137565"
datanode_1  | .
datanode_1  | 2020-05-27 13:56:49,746 [Command processor thread] INFO impl.RaftServerProxy: 64aa2774-d5fb-40de-bd90-35cfd81f6839: addNew group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] returns group-330873A83A56:java.util.concurrent.CompletableFuture@57178e92[Not completed]
datanode_1  | 2020-05-27 13:56:49,752 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839: new RaftServerImpl for group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-05-27 13:56:49,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-05-27 13:56:49,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-05-27 13:56:49,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-05-27 13:56:49,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-05-27 13:56:49,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-05-27 13:56:49,763 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: ConfigurationManager, init=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-05-27 13:56:49,763 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-05-27 13:56:49,763 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-05-27 13:56:49,764 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 does not exist. Creating ...
datanode_1  | 2020-05-27 13:56:49,777 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/in_use.lock acquired by nodename 6@5fb60f47d154
datanode_1  | 2020-05-27 13:56:49,783 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 has been successfully formatted.
datanode_1  | 2020-05-27 13:56:49,783 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-330873A83A56: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-05-27 13:56:49,784 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-05-27 13:56:49,788 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-05-27 13:56:49,788 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-05-27 13:56:49,789 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-05-27 13:56:49,790 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-05-27 13:56:49,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-05-27 13:56:49,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-05-27 13:56:49,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-05-27 13:56:49,791 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-05-27 13:56:49,792 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-05-27 13:56:49,792 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-05-27 13:56:49,799 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-05-27 13:56:49,818 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-05-27 13:56:49,822 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-05-27 13:56:49,842 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-05-27 13:56:49,842 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1  | 2020-05-27 13:56:49,843 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-05-27 13:56:49,843 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56
datanode_1  | 2020-05-27 13:56:49,844 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56
datanode_1  | 2020-05-27 13:56:49,846 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: start as a follower, conf=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_1  | 2020-05-27 13:56:49,847 [pool-19-thread-1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-05-27 13:56:49,851 [pool-19-thread-1] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start FollowerState
datanode_1  | 2020-05-27 13:56:49,851 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-330873A83A56,id=64aa2774-d5fb-40de-bd90-35cfd81f6839
datanode_1  | 2020-05-27 13:56:49,851 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56
datanode_1  | 2020-05-27 13:56:52,075 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "b321e0da-40fa-404a-8ad9-330873a83a56"
datanode_1  | .
datanode_1  | 2020-05-27 13:56:53,471 [grpc-default-executor-0] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_1  | 2020-05-27 13:56:53,473 [grpc-default-executor-0] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: shutdown FollowerState
datanode_1  | 2020-05-27 13:56:53,473 [grpc-default-executor-0] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start FollowerState
datanode_1  | 2020-05-27 13:56:53,473 [Thread-22] INFO impl.FollowerState: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1  | 2020-05-27 13:56:53,751 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-330873A83A56 with new leaderId: a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_1  | 2020-05-27 13:56:53,751 [grpc-default-executor-0] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: change Leader from null to a22687bf-1be9-4837-b2b6-cbf11c380a1a at term 1 for appendEntries, leader elected after 3967ms
datanode_1  | 2020-05-27 13:56:53,800 [grpc-default-executor-0] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56: set configuration 0: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null at 0
datanode_1  | 2020-05-27 13:56:53,859 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-05-27 13:56:54,240 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-330873A83A56-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/current/log_inprogress_0
datanode_1  | 2020-05-27 13:56:54,870 [Thread-20] INFO impl.FollowerState: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-FollowerState: change to CANDIDATE, lastRpcTime:5174ms, electionTimeout:5167ms
datanode_1  | 2020-05-27 13:56:54,871 [Thread-20] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: shutdown FollowerState
datanode_1  | 2020-05-27 13:56:54,871 [Thread-20] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-05-27 13:56:54,873 [Thread-20] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start LeaderElection
datanode_1  | 2020-05-27 13:56:54,882 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.LeaderElection: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1: begin an election at term 1 for -1: [64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_1  | 2020-05-27 13:56:54,883 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: shutdown LeaderElection
datanode_1  | 2020-05-27 13:56:54,884 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2020-05-27 13:56:54,884 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B3A618137565 with new leaderId: 64aa2774-d5fb-40de-bd90-35cfd81f6839
datanode_1  | 2020-05-27 13:56:54,886 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: change Leader from null to 64aa2774-d5fb-40de-bd90-35cfd81f6839 at term 1 for becomeLeader, leader elected after 5971ms
datanode_1  | 2020-05-27 13:56:54,887 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-05-27 13:56:54,887 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2020-05-27 13:56:54,914 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565
datanode_1  | 2020-05-27 13:56:54,923 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2020-05-27 13:56:54,927 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1  | 2020-05-27 13:56:54,951 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2020-05-27 13:56:54,951 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-05-27 13:56:54,959 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-05-27 13:56:54,984 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.RoleInfo: 64aa2774-d5fb-40de-bd90-35cfd81f6839: start LeaderState
datanode_1  | 2020-05-27 13:56:54,995 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-05-27 13:56:54,999 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-LeaderElection1] INFO impl.RaftServerImpl: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565: set configuration 0: [64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null at 0
datanode_1  | 2020-05-27 13:56:55,011 [64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 64aa2774-d5fb-40de-bd90-35cfd81f6839@group-B3A618137565-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a0748133-47ae-4a27-85f0-b3a618137565/current/log_inprogress_0
datanode_3  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3  | 2020-05-27 13:56:24,207 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | /************************************************************
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3  | STARTUP_MSG:   host = c65910b34e7f/172.21.0.4
datanode_3  | STARTUP_MSG:   args = []
datanode_3  | STARTUP_MSG:   version = 3.2.1
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
datanode_3  | STARTUP_MSG:   java = 11.0.6
datanode_3  | ************************************************************/
datanode_3  | 2020-05-27 13:56:24,259 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2020-05-27 13:56:26,388 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3  | 2020-05-27 13:56:26,929 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3  | 2020-05-27 13:56:28,115 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3  | 2020-05-27 13:56:28,116 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3  | 2020-05-27 13:56:28,498 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c65910b34e7f ip:172.21.0.4
datanode_3  | 2020-05-27 13:56:28,854 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3  | 2020-05-27 13:56:28,879 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_3  | 2020-05-27 13:56:28,881 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3  | 2020-05-27 13:56:28,951 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-05-27 13:56:29,069 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-05-27 13:56:33,793 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2020-05-27 13:56:34,052 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_3  | 2020-05-27 13:56:34,512 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_3  | 2020-05-27 13:56:34,522 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3  | 2020-05-27 13:56:34,524 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-05-27 13:56:34,533 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3  | 2020-05-27 13:56:34,539 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2020-05-27 13:56:35,743 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-05-27 13:56:36,536 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3  | 2020-05-27 13:56:36,620 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3  | 2020-05-27 13:56:36,819 [main] INFO util.log: Logging initialized @15913ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3  | 2020-05-27 13:56:37,415 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2020-05-27 13:56:37,431 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3  | 2020-05-27 13:56:37,442 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3  | 2020-05-27 13:56:37,443 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context hddsDatanode
datanode_3  | 2020-05-27 13:56:37,447 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
datanode_3  | 2020-05-27 13:56:37,447 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
datanode_3  | 2020-05-27 13:56:37,562 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_3  | 2020-05-27 13:56:37,602 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3  | 2020-05-27 13:56:37,609 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_3  | 2020-05-27 13:56:37,810 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3  | 2020-05-27 13:56:37,810 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3  | 2020-05-27 13:56:37,829 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3  | 2020-05-27 13:56:37,875 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2020-05-27 13:56:37,883 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f0f7849{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2020-05-27 13:56:37,883 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77bbadc{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2020-05-27 13:56:38,210 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3f50b680{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-8194964561491964337.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3  | 2020-05-27 13:56:38,264 [main] INFO server.AbstractConnector: Started ServerConnector@20ab3e3a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3  | 2020-05-27 13:56:38,275 [main] INFO server.Server: Started @17369ms
datanode_3  | 2020-05-27 13:56:38,317 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3  | 2020-05-27 13:56:38,317 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | 2020-05-27 13:56:38,338 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3  | 2020-05-27 13:56:38,532 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@33e55302] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3  | 2020-05-27 13:56:39,003 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3  | 2020-05-27 13:56:41,687 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-05-27 13:56:42,688 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-05-27 13:56:43,431 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3  | 2020-05-27 13:56:43,432 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3  | 2020-05-27 13:56:43,432 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis bbe1b503-f6e3-42f6-9e2c-66477e765538 at port 9858
datanode_3  | 2020-05-27 13:56:43,465 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: bbe1b503-f6e3-42f6-9e2c-66477e765538: start RPC server
datanode_3  | 2020-05-27 13:56:43,653 [Datanode State Machine Thread - 0] INFO server.GrpcService: bbe1b503-f6e3-42f6-9e2c-66477e765538: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3  | 2020-05-27 13:56:47,588 [Command processor thread] INFO impl.RaftServerProxy: bbe1b503-f6e3-42f6-9e2c-66477e765538: addNew group-817ECCA4B57C:[bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858] returns group-817ECCA4B57C:java.util.concurrent.CompletableFuture@59311b03[Not completed]
datanode_3  | 2020-05-27 13:56:47,751 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538: new RaftServerImpl for group-817ECCA4B57C:[bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-05-27 13:56:47,762 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-05-27 13:56:47,763 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-05-27 13:56:47,764 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-05-27 13:56:47,767 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-05-27 13:56:47,769 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-05-27 13:56:47,784 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: ConfigurationManager, init=-1: [bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-05-27 13:56:47,792 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-05-27 13:56:47,801 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-05-27 13:56:47,805 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2ca17910-3619-4b83-ad3b-817ecca4b57c does not exist. Creating ...
datanode_3  | 2020-05-27 13:56:47,829 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2ca17910-3619-4b83-ad3b-817ecca4b57c/in_use.lock acquired by nodename 7@c65910b34e7f
datanode_3  | 2020-05-27 13:56:47,849 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2ca17910-3619-4b83-ad3b-817ecca4b57c has been successfully formatted.
datanode_3  | 2020-05-27 13:56:47,853 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-817ECCA4B57C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-05-27 13:56:47,884 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-05-27 13:56:47,892 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-05-27 13:56:47,933 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-05-27 13:56:47,944 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-05-27 13:56:47,950 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:47,982 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.bbe1b503-f6e3-42f6-9e2c-66477e765538
datanode_3  | 2020-05-27 13:56:48,044 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-05-27 13:56:48,106 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2ca17910-3619-4b83-ad3b-817ecca4b57c
datanode_3  | 2020-05-27 13:56:48,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-05-27 13:56:48,110 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-05-27 13:56:48,130 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:48,131 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-05-27 13:56:48,147 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:48,148 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-05-27 13:56:48,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-05-27 13:56:48,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-05-27 13:56:48,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-05-27 13:56:48,274 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-05-27 13:56:48,423 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-05-27 13:56:48,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-05-27 13:56:48,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-05-27 13:56:48,426 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-05-27 13:56:48,461 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3  | 2020-05-27 13:56:48,461 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-05-27 13:56:48,558 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C
datanode_3  | 2020-05-27 13:56:48,574 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C
datanode_3  | 2020-05-27 13:56:48,594 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: start as a follower, conf=-1: [bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858], old=null
datanode_3  | 2020-05-27 13:56:48,608 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-05-27 13:56:48,609 [pool-19-thread-1] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: start FollowerState
datanode_3  | 2020-05-27 13:56:48,639 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-817ECCA4B57C,id=bbe1b503-f6e3-42f6-9e2c-66477e765538
datanode_3  | 2020-05-27 13:56:48,640 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C
datanode_3  | 2020-05-27 13:56:48,671 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "2ca17910-3619-4b83-ad3b-817ecca4b57c"
datanode_3  | .
datanode_2  | 2020-05-27 13:56:52,960 [Thread-20] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-05-27 13:56:52,962 [Thread-20] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start LeaderElection
datanode_2  | 2020-05-27 13:56:52,984 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.LeaderElection: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1: begin an election at term 1 for -1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858], old=null
datanode_2  | 2020-05-27 13:56:52,984 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: shutdown LeaderElection
datanode_2  | 2020-05-27 13:56:52,985 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2020-05-27 13:56:52,985 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-66E4E1AC37E8 with new leaderId: a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_2  | 2020-05-27 13:56:52,986 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: change Leader from null to a22687bf-1be9-4837-b2b6-cbf11c380a1a at term 1 for becomeLeader, leader elected after 5704ms
datanode_2  | 2020-05-27 13:56:52,992 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2020-05-27 13:56:52,995 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2020-05-27 13:56:53,002 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8
datanode_2  | 2020-05-27 13:56:53,016 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2020-05-27 13:56:53,018 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2  | 2020-05-27 13:56:53,033 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2020-05-27 13:56:53,033 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2020-05-27 13:56:53,035 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2020-05-27 13:56:53,053 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start LeaderState
datanode_2  | 2020-05-27 13:56:53,093 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-05-27 13:56:53,151 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-LeaderElection1] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8: set configuration 0: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858], old=null at 0
datanode_2  | 2020-05-27 13:56:53,252 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-66E4E1AC37E8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b7e70a29-c087-4293-8da3-66e4e1ac37e8/current/log_inprogress_0
datanode_2  | 2020-05-27 13:56:53,369 [Thread-22] INFO impl.FollowerState: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-FollowerState: change to CANDIDATE, lastRpcTime:5197ms, electionTimeout:5190ms
datanode_2  | 2020-05-27 13:56:53,369 [Thread-22] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: shutdown FollowerState
datanode_2  | 2020-05-27 13:56:53,370 [Thread-22] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-05-27 13:56:53,370 [Thread-22] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start LeaderElection
datanode_2  | 2020-05-27 13:56:53,390 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.LeaderElection: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2: begin an election at term 1 for -1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_2  | 2020-05-27 13:56:53,529 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.LeaderElection: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2: Election PASSED; received 1 response(s) [a22687bf-1be9-4837-b2b6-cbf11c380a1a<-64aa2774-d5fb-40de-bd90-35cfd81f6839#0:OK-t1] and 0 exception(s); a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56:t1, leader=null, voted=a22687bf-1be9-4837-b2b6-cbf11c380a1a, raftlog=a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_2  | 2020-05-27 13:56:53,533 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: shutdown LeaderElection
datanode_2  | 2020-05-27 13:56:53,534 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2020-05-27 13:56:53,534 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-330873A83A56 with new leaderId: a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_2  | 2020-05-27 13:56:53,534 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: change Leader from null to a22687bf-1be9-4837-b2b6-cbf11c380a1a at term 1 for becomeLeader, leader elected after 5416ms
datanode_2  | 2020-05-27 13:56:53,586 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2020-05-27 13:56:53,586 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2020-05-27 13:56:53,591 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56
datanode_2  | 2020-05-27 13:56:53,591 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2020-05-27 13:56:53,591 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
om2_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om2_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1       | 2020-05-27 13:56:24,304 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1       | /************************************************************
om2_1       | STARTUP_MSG: Starting OzoneManager
om2_1       | STARTUP_MSG:   host = 2be3b56642a6/172.21.0.3
om2_1       | STARTUP_MSG:   args = [--init]
om2_1       | STARTUP_MSG:   version = 3.2.1
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om2_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om2_1       | STARTUP_MSG:   java = 11.0.6
om2_1       | ************************************************************/
om2_1       | 2020-05-27 13:56:24,343 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1       | 2020-05-27 13:56:29,371 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1       | 2020-05-27 13:56:29,687 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1       | 2020-05-27 13:56:29,687 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om2: om2
om2_1       | 2020-05-27 13:56:29,725 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2020-05-27 13:56:32,025 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:33,026 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:34,027 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:35,027 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:36,028 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:37,029 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:38,030 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:39,031 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:40,031 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:41,032 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om2_1       | 2020-05-27 13:56:41,034 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om2_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b464b64b-2e94-4301-9394-a54c0dc63980
om2_1       | 2020-05-27 13:56:46,192 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1       | /************************************************************
om2_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 2be3b56642a6/172.21.0.3
om2_1       | ************************************************************/
om2_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om2_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1       | 2020-05-27 13:56:50,907 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1       | /************************************************************
om2_1       | STARTUP_MSG: Starting OzoneManager
om2_1       | STARTUP_MSG:   host = 2be3b56642a6/172.21.0.3
om2_1       | STARTUP_MSG:   args = []
om2_1       | STARTUP_MSG:   version = 3.2.1
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om2_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om2_1       | STARTUP_MSG:   java = 11.0.6
om2_1       | ************************************************************/
om2_1       | 2020-05-27 13:56:50,940 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1       | 2020-05-27 13:56:54,370 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1       | 2020-05-27 13:56:54,541 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1       | 2020-05-27 13:56:54,545 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om2: om2
om2_1       | 2020-05-27 13:56:54,563 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2020-05-27 13:56:54,583 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2020-05-27 13:56:57,403 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1       | 2020-05-27 13:56:59,039 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1       | 2020-05-27 13:56:59,123 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2020-05-27 13:56:59,169 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om2:9872, om1:9872, om3:9872
om2_1       | 2020-05-27 13:56:59,198 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex set from SnapShotInfo (t:0, i:~)
om2_1       | 2020-05-27 13:56:59,258 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
om2_1       | 2020-05-27 13:56:59,618 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
om2_1       | 	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
om2_1       | 2020-05-27 13:56:59,631 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1       | 2020-05-27 13:56:59,636 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1       | 2020-05-27 13:56:59,638 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1       | 2020-05-27 13:56:59,644 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1       | 2020-05-27 13:56:59,646 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1       | 2020-05-27 13:57:00,154 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1       | 2020-05-27 13:57:00,192 [main] INFO impl.RaftServerProxy: om2: addNew group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] returns group-562213E44849:java.util.concurrent.CompletableFuture@1f53481b[Not completed]
om2_1       | 2020-05-27 13:57:00,207 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1       | 2020-05-27 13:57:00,207 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2020-05-27 13:57:00,207 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | 2020-05-27 13:57:00,209 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
datanode_3  | 2020-05-27 13:56:48,676 [Command processor thread] INFO impl.RaftServerProxy: bbe1b503-f6e3-42f6-9e2c-66477e765538: addNew group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] returns group-330873A83A56:java.util.concurrent.CompletableFuture@308101c7[Not completed]
datanode_3  | 2020-05-27 13:56:48,713 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538: new RaftServerImpl for group-330873A83A56:[a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-05-27 13:56:48,715 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: ConfigurationManager, init=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-05-27 13:56:48,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-05-27 13:56:48,720 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-05-27 13:56:48,720 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 does not exist. Creating ...
datanode_3  | 2020-05-27 13:56:48,739 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/in_use.lock acquired by nodename 7@c65910b34e7f
datanode_3  | 2020-05-27 13:56:48,747 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56 has been successfully formatted.
datanode_3  | 2020-05-27 13:56:48,752 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-330873A83A56: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-05-27 13:56:48,752 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-05-27 13:56:48,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-05-27 13:56:48,753 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-05-27 13:56:48,754 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-05-27 13:56:48,754 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:48,755 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-05-27 13:56:48,755 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56
datanode_3  | 2020-05-27 13:56:48,755 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-05-27 13:56:48,757 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-05-27 13:56:48,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-05-27 13:56:48,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-05-27 13:56:48,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-05-27 13:56:48,763 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-05-27 13:56:48,770 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-05-27 13:56:48,770 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-05-27 13:56:48,771 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-05-27 13:56:48,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3  | 2020-05-27 13:56:48,779 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-05-27 13:56:48,779 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56
datanode_3  | 2020-05-27 13:56:48,784 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56
datanode_3  | 2020-05-27 13:56:48,784 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: start as a follower, conf=-1: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null
datanode_3  | 2020-05-27 13:56:48,785 [pool-19-thread-1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-05-27 13:56:48,785 [pool-19-thread-1] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: start FollowerState
datanode_3  | 2020-05-27 13:56:48,787 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-330873A83A56,id=bbe1b503-f6e3-42f6-9e2c-66477e765538
datanode_3  | 2020-05-27 13:56:48,787 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56
datanode_3  | 2020-05-27 13:56:51,648 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "b321e0da-40fa-404a-8ad9-330873a83a56"
datanode_3  | .
datanode_3  | 2020-05-27 13:56:53,497 [grpc-default-executor-0] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_3  | 2020-05-27 13:56:53,498 [grpc-default-executor-0] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: shutdown FollowerState
datanode_3  | 2020-05-27 13:56:53,498 [Thread-22] INFO impl.FollowerState: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-05-27 13:56:53,512 [grpc-default-executor-0] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: start FollowerState
datanode_3  | 2020-05-27 13:56:53,670 [Thread-20] INFO impl.FollowerState: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-FollowerState: change to CANDIDATE, lastRpcTime:5060ms, electionTimeout:5044ms
datanode_3  | 2020-05-27 13:56:53,672 [Thread-20] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: shutdown FollowerState
datanode_3  | 2020-05-27 13:56:53,673 [Thread-20] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2020-05-27 13:56:53,674 [Thread-20] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: start LeaderElection
datanode_3  | 2020-05-27 13:56:53,689 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.LeaderElection: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1: begin an election at term 1 for -1: [bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858], old=null
datanode_3  | 2020-05-27 13:56:53,691 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: shutdown LeaderElection
datanode_3  | 2020-05-27 13:56:53,691 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2020-05-27 13:56:53,691 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-817ECCA4B57C with new leaderId: bbe1b503-f6e3-42f6-9e2c-66477e765538
datanode_3  | 2020-05-27 13:56:53,692 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: change Leader from null to bbe1b503-f6e3-42f6-9e2c-66477e765538 at term 1 for becomeLeader, leader elected after 5812ms
datanode_3  | 2020-05-27 13:56:53,696 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2020-05-27 13:56:53,709 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2020-05-27 13:56:53,771 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-330873A83A56 with new leaderId: a22687bf-1be9-4837-b2b6-cbf11c380a1a
datanode_3  | 2020-05-27 13:56:53,771 [grpc-default-executor-0] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: change Leader from null to a22687bf-1be9-4837-b2b6-cbf11c380a1a at term 1 for appendEntries, leader elected after 5018ms
datanode_3  | 2020-05-27 13:56:53,778 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C
datanode_3  | 2020-05-27 13:56:53,832 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2020-05-27 13:56:53,843 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3  | 2020-05-27 13:56:53,849 [grpc-default-executor-0] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56: set configuration 0: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null at 0
datanode_3  | 2020-05-27 13:56:53,869 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3  | 2020-05-27 13:56:53,891 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2020-05-27 13:56:53,892 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2020-05-27 13:56:53,894 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-05-27 13:56:53,991 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.RoleInfo: bbe1b503-f6e3-42f6-9e2c-66477e765538: start LeaderState
datanode_3  | 2020-05-27 13:56:54,037 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-05-27 13:56:54,059 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-LeaderElection1] INFO impl.RaftServerImpl: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C: set configuration 0: [bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858], old=null at 0
datanode_3  | 2020-05-27 13:56:54,250 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-817ECCA4B57C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2ca17910-3619-4b83-ad3b-817ecca4b57c/current/log_inprogress_0
datanode_3  | 2020-05-27 13:56:54,253 [bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: bbe1b503-f6e3-42f6-9e2c-66477e765538@group-330873A83A56-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/current/log_inprogress_0
om1_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om1_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | 2020-05-27 13:56:29,714 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1       | /************************************************************
om1_1       | STARTUP_MSG: Starting OzoneManager
om1_1       | STARTUP_MSG:   host = fb6cfbd1e8df/172.21.0.9
om1_1       | STARTUP_MSG:   args = [--init]
om1_1       | STARTUP_MSG:   version = 3.2.1
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om1_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om1_1       | STARTUP_MSG:   java = 11.0.6
om1_1       | ************************************************************/
om1_1       | 2020-05-27 13:56:29,746 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1       | 2020-05-27 13:56:34,034 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1       | 2020-05-27 13:56:34,305 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1       | 2020-05-27 13:56:34,305 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om1: om1
om1_1       | 2020-05-27 13:56:34,346 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2020-05-27 13:56:36,497 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:37,498 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:38,499 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:39,499 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:40,500 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:41,501 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-05-27 13:56:42,502 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b464b64b-2e94-4301-9394-a54c0dc63980
om1_1       | 2020-05-27 13:56:43,417 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1       | /************************************************************
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at fb6cfbd1e8df/172.21.0.9
om1_1       | ************************************************************/
om1_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om1_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1       | 2020-05-27 13:56:45,569 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1       | /************************************************************
om1_1       | STARTUP_MSG: Starting OzoneManager
om1_1       | STARTUP_MSG:   host = fb6cfbd1e8df/172.21.0.9
om1_1       | STARTUP_MSG:   args = []
om1_1       | STARTUP_MSG:   version = 3.2.1
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om1_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om1_1       | STARTUP_MSG:   java = 11.0.6
om1_1       | ************************************************************/
om1_1       | 2020-05-27 13:56:45,583 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1       | 2020-05-27 13:56:48,325 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1       | 2020-05-27 13:56:48,524 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1       | 2020-05-27 13:56:48,524 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om1: om1
om1_1       | 2020-05-27 13:56:48,544 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2020-05-27 13:56:48,568 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2020-05-27 13:56:52,065 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2020-05-27 13:56:53,651 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1       | 2020-05-27 13:56:53,920 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1       | 2020-05-27 13:56:54,158 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om1:9872, om2:9872, om3:9872
om1_1       | 2020-05-27 13:56:54,231 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex set from SnapShotInfo (t:0, i:~)
om1_1       | 2020-05-27 13:56:54,319 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
om1_1       | 2020-05-27 13:56:54,585 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
om1_1       | 	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
om1_1       | 2020-05-27 13:56:54,622 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1       | 2020-05-27 13:56:54,623 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1       | 2020-05-27 13:56:54,630 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2020-05-27 13:56:54,639 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1       | 2020-05-27 13:56:54,640 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1       | 2020-05-27 13:56:55,409 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1       | 2020-05-27 13:56:55,453 [main] INFO impl.RaftServerProxy: om1: addNew group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] returns group-562213E44849:java.util.concurrent.CompletableFuture@51ec2856[Not completed]
om1_1       | 2020-05-27 13:56:55,480 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1       | 2020-05-27 13:56:55,481 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1       | 2020-05-27 13:56:55,481 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1       | 2020-05-27 13:56:55,482 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1       | 2020-05-27 13:56:55,602 [pool-17-thread-1] INFO impl.RaftServerImpl: om1: new RaftServerImpl for group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] with OzoneManagerStateMachine:uninitialized
om1_1       | 2020-05-27 13:56:55,659 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 1s (custom)
om1_1       | 2020-05-27 13:56:55,670 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 1200ms (custom)
om1_1       | 2020-05-27 13:56:55,671 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
om1_1       | 2020-05-27 13:56:55,672 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
om1_1       | 2020-05-27 13:56:55,673 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2020-05-27 13:56:55,736 [pool-17-thread-1] INFO impl.RaftServerImpl: om1@group-562213E44849: ConfigurationManager, init=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null, confs=<EMPTY_MAP>
om1_1       | 2020-05-27 13:56:55,739 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1       | 2020-05-27 13:56:55,776 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1       | 2020-05-27 13:56:55,796 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om1_1       | 2020-05-27 13:56:55,911 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 6@fb6cfbd1e8df
om1_1       | 2020-05-27 13:56:55,965 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om1_1       | 2020-05-27 13:56:55,970 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1       | 2020-05-27 13:56:55,974 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1       | 2020-05-27 13:56:56,019 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1       | 2020-05-27 13:56:56,020 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2020-05-27 13:56:56,022 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om1_1       | 2020-05-27 13:56:56,045 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1
om1_1       | 2020-05-27 13:56:56,168 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1       | 2020-05-27 13:56:56,196 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-562213E44849-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om1_1       | 2020-05-27 13:56:56,219 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1       | 2020-05-27 13:56:56,221 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1       | 2020-05-27 13:56:56,248 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om1_1       | 2020-05-27 13:56:56,249 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om1_1       | 2020-05-27 13:56:56,251 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1       | 2020-05-27 13:56:56,252 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1       | 2020-05-27 13:56:56,253 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1       | 2020-05-27 13:56:56,263 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1       | 2020-05-27 13:56:56,264 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1       | 2020-05-27 13:56:56,360 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1       | 2020-05-27 13:56:56,407 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1       | 2020-05-27 13:56:56,590 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1       | 2020-05-27 13:56:56,616 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1       | 2020-05-27 13:56:56,618 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1       | 2020-05-27 13:56:56,639 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1       | 2020-05-27 13:56:56,643 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om1_1       | 2020-05-27 13:56:56,813 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-562213E44849
om1_1       | 2020-05-27 13:56:56,821 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1       | 2020-05-27 13:56:56,832 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-562213E44849
om1_1       | 2020-05-27 13:56:56,877 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1       | 2020-05-27 13:56:57,318 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1       | 2020-05-27 13:56:57,439 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1       | 2020-05-27 13:56:57,440 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1       | 2020-05-27 13:56:57,532 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/172.21.0.9:9862
om1_1       | 2020-05-27 13:56:57,536 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1       | 2020-05-27 13:56:57,540 [Listener at om1/9862] INFO impl.RaftServerImpl: om1@group-562213E44849: start as a follower, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:56:57,541 [Listener at om1/9862] INFO impl.RaftServerImpl: om1@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1       | 2020-05-27 13:56:57,549 [Listener at om1/9862] INFO impl.RoleInfo: om1: start FollowerState
om1_1       | 2020-05-27 13:56:57,556 [Listener at om1/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om1
om1_1       | 2020-05-27 13:56:57,565 [Listener at om1/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-562213E44849
om1_1       | 2020-05-27 13:56:57,568 [Listener at om1/9862] INFO impl.RaftServerProxy: om1: start RPC server
om1_1       | 2020-05-27 13:56:57,757 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 0.0.0.0/0.0.0.0:9872
om1_1       | 2020-05-27 13:56:57,787 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1       | 2020-05-27 13:56:57,833 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1       | 2020-05-27 13:56:58,278 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1       | 2020-05-27 13:56:58,279 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1       | 2020-05-27 13:56:58,335 [Listener at om1/9862] INFO util.log: Logging initialized @14572ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1       | 2020-05-27 13:57:00,253 [pool-17-thread-1] INFO impl.RaftServerImpl: om2: new RaftServerImpl for group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] with OzoneManagerStateMachine:uninitialized
om2_1       | 2020-05-27 13:57:00,268 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 1s (custom)
om2_1       | 2020-05-27 13:57:00,272 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 1200ms (custom)
om2_1       | 2020-05-27 13:57:00,272 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
om2_1       | 2020-05-27 13:57:00,274 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
om2_1       | 2020-05-27 13:57:00,277 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1       | 2020-05-27 13:57:00,305 [pool-17-thread-1] INFO impl.RaftServerImpl: om2@group-562213E44849: ConfigurationManager, init=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null, confs=<EMPTY_MAP>
om2_1       | 2020-05-27 13:57:00,307 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1       | 2020-05-27 13:57:00,368 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1       | 2020-05-27 13:57:00,370 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om2_1       | 2020-05-27 13:57:00,429 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 6@2be3b56642a6
om2_1       | 2020-05-27 13:57:00,497 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om2_1       | 2020-05-27 13:57:00,521 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1       | 2020-05-27 13:57:00,525 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1       | 2020-05-27 13:57:00,545 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1       | 2020-05-27 13:57:00,546 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1       | 2020-05-27 13:57:00,551 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om2_1       | 2020-05-27 13:57:00,565 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om2
om2_1       | 2020-05-27 13:57:00,614 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1       | 2020-05-27 13:57:00,629 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om2@group-562213E44849-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om2_1       | 2020-05-27 13:57:00,630 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1       | 2020-05-27 13:57:00,630 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1       | 2020-05-27 13:57:00,631 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om2_1       | 2020-05-27 13:57:00,633 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om2_1       | 2020-05-27 13:57:00,633 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1       | 2020-05-27 13:57:00,633 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1       | 2020-05-27 13:57:00,634 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1       | 2020-05-27 13:57:00,639 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1       | 2020-05-27 13:57:00,639 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1       | 2020-05-27 13:57:00,682 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1       | 2020-05-27 13:57:00,688 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1       | 2020-05-27 13:57:00,692 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1       | 2020-05-27 13:57:00,692 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1       | 2020-05-27 13:57:00,695 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1       | 2020-05-27 13:57:00,696 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om2_1       | 2020-05-27 13:57:00,696 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om2_1       | 2020-05-27 13:57:00,720 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om2@group-562213E44849
om2_1       | 2020-05-27 13:57:00,728 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1       | 2020-05-27 13:57:00,729 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om2@group-562213E44849
om2_1       | 2020-05-27 13:57:00,736 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1       | 2020-05-27 13:57:00,829 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1       | 2020-05-27 13:57:00,874 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1       | 2020-05-27 13:57:00,874 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1       | 2020-05-27 13:57:00,907 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/172.21.0.3:9862
om2_1       | 2020-05-27 13:57:00,907 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1       | 2020-05-27 13:57:00,909 [Listener at om2/9862] INFO impl.RaftServerImpl: om2@group-562213E44849: start as a follower, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om2_1       | 2020-05-27 13:57:00,910 [Listener at om2/9862] INFO impl.RaftServerImpl: om2@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1       | 2020-05-27 13:57:00,911 [Listener at om2/9862] INFO impl.RoleInfo: om2: start FollowerState
om2_1       | 2020-05-27 13:57:00,912 [Listener at om2/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om2
om2_1       | 2020-05-27 13:57:00,914 [Listener at om2/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om2@group-562213E44849
om2_1       | 2020-05-27 13:57:00,916 [Listener at om2/9862] INFO impl.RaftServerProxy: om2: start RPC server
om2_1       | 2020-05-27 13:57:00,953 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 0.0.0.0/0.0.0.0:9872
om2_1       | 2020-05-27 13:57:00,962 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1       | 2020-05-27 13:56:58,503 [Listener at om1/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om1_1       | 2020-05-27 13:56:58,507 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1       | 2020-05-27 13:56:58,525 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1       | 2020-05-27 13:56:58,538 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om1_1       | 2020-05-27 13:56:58,538 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om1_1       | 2020-05-27 13:56:58,544 [Listener at om1/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om1_1       | 2020-05-27 13:56:58,597 [Thread-10] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcTime:1050ms, electionTimeout:1031ms
om1_1       | 2020-05-27 13:56:58,598 [Thread-10] INFO impl.RoleInfo: om1: shutdown FollowerState
om1_1       | 2020-05-27 13:56:58,599 [Thread-10] INFO impl.RaftServerImpl: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1       | 2020-05-27 13:56:58,600 [Thread-10] INFO impl.RoleInfo: om1: start LeaderElection
om1_1       | 2020-05-27 13:56:58,620 [Listener at om1/9862] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
om1_1       | 2020-05-27 13:56:58,624 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1       | 2020-05-27 13:56:58,625 [Listener at om1/9862] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
om1_1       | 2020-05-27 13:56:58,672 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: begin an election at term 1 for -1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:56:58,751 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1       | 2020-05-27 13:56:58,752 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1       | 2020-05-27 13:56:58,753 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1       | 2020-05-27 13:56:58,805 [Listener at om1/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om1_1       | 2020-05-27 13:56:58,822 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1039bfc4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1       | 2020-05-27 13:56:58,827 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6436e181{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1       | 2020-05-27 13:56:59,512 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:56:59,754 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7dee835{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-0_6_0-SNAPSHOT_jar-_-any-11046933958924886996.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/ozoneManager}
om1_1       | 2020-05-27 13:56:59,771 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@64b70f41{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om1_1       | 2020-05-27 13:56:59,771 [Listener at om1/9862] INFO server.Server: Started @16009ms
om1_1       | 2020-05-27 13:56:59,774 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1       | 2020-05-27 13:56:59,774 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1       | 2020-05-27 13:56:59,783 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1       | 2020-05-27 13:56:59,796 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53aa2fc9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1       | 2020-05-27 13:56:59,825 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: Election TIMEOUT; received 0 response(s) [] and 1 exception(s); om1@group-562213E44849:t1, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:56:59,825 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:56:59,837 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: begin an election at term 2 for -1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:56:59,851 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:57:00,416 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection1: Election REJECTED; received 1 response(s) [om1<-om3#0:FAIL-t2] and 1 exception(s); om1@group-562213E44849:t2, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:57:00,417 [om1@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:57:00,417 [om1@group-562213E44849-LeaderElection1] INFO impl.RaftServerImpl: om1@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for DISCOVERED_A_NEW_TERM
om1_1       | 2020-05-27 13:57:00,417 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: shutdown LeaderElection
om1_1       | 2020-05-27 13:57:00,422 [om1@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om1: start FollowerState
om1_1       | 2020-05-27 13:57:01,501 [Thread-130] INFO impl.FollowerState: om1@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcTime:1078ms, electionTimeout:1072ms
om1_1       | 2020-05-27 13:57:01,501 [Thread-130] INFO impl.RoleInfo: om1: shutdown FollowerState
om1_1       | 2020-05-27 13:57:01,502 [Thread-130] INFO impl.RaftServerImpl: om1@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
om1_1       | 2020-05-27 13:57:01,502 [Thread-130] INFO impl.RoleInfo: om1: start LeaderElection
om1_1       | 2020-05-27 13:57:01,519 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2: begin an election at term 3 for -1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:57:01,521 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:57:01,557 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection: om1@group-562213E44849-LeaderElection2: Election PASSED; received 1 response(s) [om1<-om3#0:OK-t3] and 1 exception(s); om1@group-562213E44849:t3, leader=null, voted=om1, raftlog=om1@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om1_1       | 2020-05-27 13:57:01,557 [om1@group-562213E44849-LeaderElection2] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:57:01,557 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: shutdown LeaderElection
om1_1       | 2020-05-27 13:57:01,558 [om1@group-562213E44849-LeaderElection2] INFO impl.RaftServerImpl: om1@group-562213E44849: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
om1_1       | 2020-05-27 13:57:01,559 [om1@group-562213E44849-LeaderElection2] INFO impl.RaftServerImpl: om1@group-562213E44849: change Leader from null to om1 at term 3 for becomeLeader, leader elected after 5588ms
om1_1       | 2020-05-27 13:57:01,562 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1       | 2020-05-27 13:57:01,563 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1       | 2020-05-27 13:57:01,564 [om1@group-562213E44849-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-562213E44849
om1_1       | 2020-05-27 13:57:01,566 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1       | 2020-05-27 13:57:01,567 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1       | 2020-05-27 13:57:01,573 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1       | 2020-05-27 13:57:01,573 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1       | 2020-05-27 13:57:01,574 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1       | 2020-05-27 13:57:01,583 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1       | 2020-05-27 13:57:01,583 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2020-05-27 13:57:01,583 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1       | 2020-05-27 13:57:01,586 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1       | 2020-05-27 13:57:01,586 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1       | 2020-05-27 13:57:01,587 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1       | 2020-05-27 13:57:01,602 [om1@group-562213E44849-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2020-05-27 13:57:01,605 [om1@group-562213E44849-LeaderElection2] INFO impl.RoleInfo: om1: start LeaderState
om1_1       | 2020-05-27 13:57:01,621 [om1@group-562213E44849-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om1_1       | 2020-05-27 13:57:01,657 [om1@group-562213E44849-LeaderElection2] INFO impl.RaftServerImpl: om1@group-562213E44849: set configuration 0: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null at 0
om1_1       | 2020-05-27 13:57:01,672 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-562213E44849->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1       | 2020-05-27 13:57:01,730 [grpc-default-executor-1] INFO impl.FollowerInfo: om1@group-562213E44849->om2: nextIndex: updateUnconditionally 1 -> 0
om1_1       | 2020-05-27 13:57:01,863 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om1_1       | 2020-05-27 13:57:48,410 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:s3v for user:hadoop
om1_1       | 2020-05-27 13:57:51,944 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:01,010 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 2020-05-27 13:56:53,592 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2020-05-27 13:56:53,592 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2020-05-27 13:56:53,593 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2020-05-27 13:56:53,594 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2020-05-27 13:56:53,603 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-05-27 13:56:53,604 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-05-27 13:56:53,614 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-05-27 13:56:53,632 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-05-27 13:56:53,632 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-05-27 13:56:53,636 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2020-05-27 13:56:53,646 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-05-27 13:56:53,647 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-05-27 13:56:53,648 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-05-27 13:56:53,649 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-05-27 13:56:53,652 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-05-27 13:56:53,659 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.RoleInfo: a22687bf-1be9-4837-b2b6-cbf11c380a1a: start LeaderState
datanode_2  | 2020-05-27 13:56:53,659 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-05-27 13:56:53,662 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-LeaderElection2] INFO impl.RaftServerImpl: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56: set configuration 0: [a22687bf-1be9-4837-b2b6-cbf11c380a1a:172.21.0.5:9858, bbe1b503-f6e3-42f6-9e2c-66477e765538:172.21.0.4:9858, 64aa2774-d5fb-40de-bd90-35cfd81f6839:172.21.0.2:9858], old=null at 0
datanode_2  | 2020-05-27 13:56:53,675 [a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a22687bf-1be9-4837-b2b6-cbf11c380a1a@group-330873A83A56-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b321e0da-40fa-404a-8ad9-330873a83a56/current/log_inprogress_0
om2_1       | 2020-05-27 13:57:00,972 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1       | 2020-05-27 13:57:01,084 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1       | 2020-05-27 13:57:01,084 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1       | 2020-05-27 13:57:01,115 [Listener at om2/9862] INFO util.log: Logging initialized @14636ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1       | 2020-05-27 13:57:01,330 [Listener at om2/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om2_1       | 2020-05-27 13:57:01,336 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1       | 2020-05-27 13:57:01,351 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1       | 2020-05-27 13:57:01,356 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om2_1       | 2020-05-27 13:57:01,358 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om2_1       | 2020-05-27 13:57:01,358 [Listener at om2/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om2_1       | 2020-05-27 13:57:01,387 [Listener at om2/9862] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
om2_1       | 2020-05-27 13:57:01,389 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1       | 2020-05-27 13:57:01,390 [Listener at om2/9862] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
om2_1       | 2020-05-27 13:57:01,412 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1       | 2020-05-27 13:57:01,412 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1       | 2020-05-27 13:57:01,413 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1       | 2020-05-27 13:57:01,420 [Listener at om2/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om2_1       | 2020-05-27 13:57:01,424 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6436e181{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1       | 2020-05-27 13:57:01,425 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3d88e6b9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1       | 2020-05-27 13:57:01,823 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@21ea996f{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-0_6_0-SNAPSHOT_jar-_-any-3385922883149347969.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1       | 2020-05-27 13:57:01,844 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@1980a3f{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om2_1       | 2020-05-27 13:57:01,844 [Listener at om2/9862] INFO server.Server: Started @15366ms
om2_1       | 2020-05-27 13:57:01,846 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1       | 2020-05-27 13:57:01,846 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1       | 2020-05-27 13:57:01,868 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1       | 2020-05-27 13:57:01,883 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7bbcf6f0] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1       | 2020-05-27 13:57:02,023 [grpc-default-executor-0] INFO impl.RaftServerImpl: om2@group-562213E44849: change Leader from null to om1 at term 3 for appendEntries, leader elected after 1502ms
om2_1       | 2020-05-27 13:57:02,026 [grpc-default-executor-0] INFO impl.RaftServerImpl: om2@group-562213E44849: set configuration 0: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null at 0
om2_1       | 2020-05-27 13:57:02,032 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1       | 2020-05-27 13:57:02,141 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om2_1       | 2020-05-27 13:57:48,982 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:s3v for user:hadoop
om2_1       | 2020-05-27 13:57:51,962 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:01,019 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:02,561 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-45191 in volume:s3v
om2_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:186)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:06,295 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:07,790 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om2_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket doesn't exist
om2_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:121)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:11,069 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:16,003 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:20,263 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:25,971 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-0_70 to index:70
om2_1       | 2020-05-27 13:58:25,975 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_0-70
om2_1       | 2020-05-27 13:58:25,992 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71
om2_1       | 2020-05-27 13:58:30,261 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-49783/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om2_1       | 2020-05-27 13:58:30,261 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey2 in Volume/Bucket s3v/bucket-49783
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:02,553 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-45191 in volume:s3v
om1_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:186)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:06,279 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:07,775 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om1_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket doesn't exist
om1_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:121)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:11,063 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:15,990 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:20,254 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:25,964 [IPC Server handler 28 on default port 9862] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-0_70 to index:70
om1_1       | 2020-05-27 13:58:25,967 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_0-70
om2_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: Entity too small: volume: s3vbucket: bucket-49783key: multipartKey2
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:241)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:30,274 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey2"
om2_1       |   multipartUploadID: "132dcb2f-7894-4330-b4bb-0f768ff03297-104240769132199945"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587910224
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 1
om2_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769164247050"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769244200971"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:31,210 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om2_1       | partName: "etag1"
om2_1       | , partNumber: 2
om2_1       | partName: "etag2"
om2_1       | ]
om2_1       | 2020-05-27 13:58:31,211 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:31,213 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey3"
om2_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587911172
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 1
om2_1       |   partName: "etag1"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "etag2"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:31,654 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om2_1       | partName: "etag1"
om2_1       | , partNumber: 1
om2_1       | partName: "etag2"
om2_1       | ]
om2_1       | 2020-05-27 13:58:31,656 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om3_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1       | 2020-05-27 13:56:30,687 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1       | /************************************************************
om3_1       | STARTUP_MSG: Starting OzoneManager
om3_1       | STARTUP_MSG:   host = c7a70e8da821/172.21.0.8
om3_1       | STARTUP_MSG:   args = [--init]
om3_1       | STARTUP_MSG:   version = 3.2.1
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:31,658 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey3"
om2_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587911637
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "etag1"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 1
om2_1       |   partName: "etag2"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:37,242 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag1, 1}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769410400269
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om3_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om3_1       | STARTUP_MSG:   java = 11.0.6
om3_1       | ************************************************************/
om3_1       | 2020-05-27 13:56:30,723 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | 2020-05-27 13:56:35,225 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1       | 2020-05-27 13:56:35,515 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1       | 2020-05-27 13:56:35,519 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om3: om3
om3_1       | 2020-05-27 13:56:35,547 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2020-05-27 13:56:37,517 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-05-27 13:56:38,518 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-05-27 13:56:39,519 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-05-27 13:56:40,520 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-05-27 13:56:41,520 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-05-27 13:56:42,521 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-b464b64b-2e94-4301-9394-a54c0dc63980
om3_1       | 2020-05-27 13:56:43,389 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1       | /************************************************************
om3_1       | SHUTDOWN_MSG: Shutting down OzoneManager at c7a70e8da821/172.21.0.8
om3_1       | ************************************************************/
om3_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om3_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1       | 2020-05-27 13:56:45,576 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1       | /************************************************************
om3_1       | STARTUP_MSG: Starting OzoneManager
om3_1       | STARTUP_MSG:   host = c7a70e8da821/172.21.0.8
om3_1       | STARTUP_MSG:   args = []
om3_1       | STARTUP_MSG:   version = 3.2.1
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om3_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
om3_1       | STARTUP_MSG:   java = 11.0.6
om3_1       | ************************************************************/
om3_1       | 2020-05-27 13:56:45,583 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1       | 2020-05-27 13:56:48,589 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1       | 2020-05-27 13:56:48,852 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: id1, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1       | 2020-05-27 13:56:48,853 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.om3: om3
om3_1       | 2020-05-27 13:56:48,880 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2020-05-27 13:56:48,926 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2020-05-27 13:56:52,286 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | 2020-05-27 13:56:53,721 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1       | 2020-05-27 13:56:53,963 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1       | 2020-05-27 13:56:54,174 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: id1 and Raft Peers: om3:9872, om1:9872, om2:9872
om3_1       | 2020-05-27 13:56:54,230 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex set from SnapShotInfo (t:0, i:~)
om3_1       | 2020-05-27 13:56:54,334 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
om3_1       | 2020-05-27 13:56:54,756 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.
om3_1       | 	It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.
om3_1       | 2020-05-27 13:56:54,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1       | 2020-05-27 13:56:54,771 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1       | 2020-05-27 13:56:54,781 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2020-05-27 13:56:54,788 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1       | 2020-05-27 13:56:54,789 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1       | 2020-05-27 13:56:55,853 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | 2020-05-27 13:56:55,911 [main] INFO impl.RaftServerProxy: om3: addNew group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] returns group-562213E44849:java.util.concurrent.CompletableFuture@51ec2856[Not completed]
om3_1       | 2020-05-27 13:56:55,929 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1       | 2020-05-27 13:56:55,929 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1       | 2020-05-27 13:56:55,929 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1       | 2020-05-27 13:56:55,933 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1       | 2020-05-27 13:56:56,079 [pool-17-thread-1] INFO impl.RaftServerImpl: om3: new RaftServerImpl for group-562213E44849:[om1:om1:9872, om3:om3:9872, om2:om2:9872] with OzoneManagerStateMachine:uninitialized
om3_1       | 2020-05-27 13:56:56,081 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 1s (custom)
om3_1       | 2020-05-27 13:56:56,081 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 1200ms (custom)
om3_1       | 2020-05-27 13:56:56,081 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 120s (custom)
om3_1       | 2020-05-27 13:56:56,081 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
om3_1       | 2020-05-27 13:56:56,082 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1       | 2020-05-27 13:58:25,975 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71
om1_1       | 2020-05-27 13:58:30,234 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-49783/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om1_1       | 2020-05-27 13:58:30,235 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey2 in Volume/Bucket s3v/bucket-49783
om1_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: Entity too small: volume: s3vbucket: bucket-49783key: multipartKey2
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:241)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:30,238 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey2"
om1_1       |   multipartUploadID: "132dcb2f-7894-4330-b4bb-0f768ff03297-104240769132199945"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587910224
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 1
om1_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769164247050"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769244200971"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:31,181 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om1_1       | partName: "etag1"
om1_1       | , partNumber: 2
om1_1       | partName: "etag2"
om1_1       | ]
om1_1       | 2020-05-27 13:58:31,184 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:31,184 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey3"
om1_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587911172
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 1
om1_1       |   partName: "etag1"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "etag2"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:31,651 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om1_1       | partName: "etag1"
om1_1       | , partNumber: 1
om1_1       | partName: "etag2"
om1_1       | ]
om1_1       | 2020-05-27 13:58:31,652 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:31,653 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey3"
om1_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om3_1       | 2020-05-27 13:56:56,167 [pool-17-thread-1] INFO impl.RaftServerImpl: om3@group-562213E44849: ConfigurationManager, init=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null, confs=<EMPTY_MAP>
om3_1       | 2020-05-27 13:56:56,168 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | 2020-05-27 13:56:56,196 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1       | 2020-05-27 13:56:56,197 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 does not exist. Creating ...
om3_1       | 2020-05-27 13:56:56,271 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/in_use.lock acquired by nodename 6@c7a70e8da821
om3_1       | 2020-05-27 13:56:56,409 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849 has been successfully formatted.
om3_1       | 2020-05-27 13:56:56,438 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1       | 2020-05-27 13:56:56,483 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1       | 2020-05-27 13:56:56,506 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1       | 2020-05-27 13:56:56,523 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | 2020-05-27 13:56:56,525 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om3_1       | 2020-05-27 13:56:56,602 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om3
om3_1       | 2020-05-27 13:56:56,768 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1       | 2020-05-27 13:56:56,806 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om3@group-562213E44849-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849
om3_1       | 2020-05-27 13:56:56,819 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1       | 2020-05-27 13:56:56,830 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1       | 2020-05-27 13:56:56,832 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 16384 (custom)
om3_1       | 2020-05-27 13:56:56,833 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om3_1       | 2020-05-27 13:56:56,834 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1       | 2020-05-27 13:56:56,835 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1       | 2020-05-27 13:56:56,844 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1       | 2020-05-27 13:56:56,853 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1       | 2020-05-27 13:56:56,854 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1       | 2020-05-27 13:56:57,032 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1       | 2020-05-27 13:56:57,067 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1       | 2020-05-27 13:56:57,090 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1       | 2020-05-27 13:56:57,103 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1       | 2020-05-27 13:56:57,112 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1       | 2020-05-27 13:56:57,119 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om3_1       | 2020-05-27 13:56:57,123 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om3_1       | 2020-05-27 13:56:57,196 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1       | 2020-05-27 13:56:57,204 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om3@group-562213E44849
om3_1       | 2020-05-27 13:56:57,229 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om3@group-562213E44849
om3_1       | 2020-05-27 13:56:57,276 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1       | 2020-05-27 13:56:57,646 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1       | 2020-05-27 13:56:57,816 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1       | 2020-05-27 13:56:57,816 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1       | 2020-05-27 13:56:57,933 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/172.21.0.8:9862
om3_1       | 2020-05-27 13:56:57,933 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1       | 2020-05-27 13:56:57,941 [Listener at om3/9862] INFO impl.RaftServerImpl: om3@group-562213E44849: start as a follower, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om3_1       | 2020-05-27 13:56:57,942 [Listener at om3/9862] INFO impl.RaftServerImpl: om3@group-562213E44849: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1       | 2020-05-27 13:56:57,946 [Listener at om3/9862] INFO impl.RoleInfo: om3: start FollowerState
om3_1       | 2020-05-27 13:56:57,964 [Listener at om3/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-562213E44849,id=om3
om3_1       | 2020-05-27 13:56:57,970 [Listener at om3/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om3@group-562213E44849
om3_1       | 2020-05-27 13:56:57,986 [Listener at om3/9862] INFO impl.RaftServerProxy: om3: start RPC server
om3_1       | 2020-05-27 13:56:58,123 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 0.0.0.0/0.0.0.0:9872
om3_1       | 2020-05-27 13:56:58,150 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1       | 2020-05-27 13:56:58,286 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1       | 2020-05-27 13:56:58,522 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1       | 2020-05-27 13:56:58,523 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1       | 2020-05-27 13:56:58,573 [Listener at om3/9862] INFO util.log: Logging initialized @14822ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1       | 2020-05-27 13:56:58,740 [Listener at om3/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587911637
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "etag1"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 1
om1_1       |   partName: "etag2"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:37,236 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
s3g_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1       | 2020-05-27 13:56:29,905 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1       | 2020-05-27 13:56:29,911 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1       | 2020-05-27 13:56:30,071 [main] INFO util.log: Logging initialized @6550ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1       | 2020-05-27 13:56:30,807 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1       | 2020-05-27 13:56:30,941 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1       | 2020-05-27 13:56:30,966 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1       | 2020-05-27 13:56:30,983 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context s3gateway
s3g_1       | 2020-05-27 13:56:30,987 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
s3g_1       | 2020-05-27 13:56:30,987 [main] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
s3g_1       | 2020-05-27 13:56:31,201 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
s3g_1       | 2020-05-27 13:56:31,206 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1       | 2020-05-27 13:56:31,258 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1       | 2020-05-27 13:56:31,300 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
s3g_1       | 2020-05-27 13:56:31,455 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1       | 2020-05-27 13:56:31,455 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1       | 2020-05-27 13:56:31,456 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1       | 2020-05-27 13:56:31,531 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1       | 2020-05-27 13:56:31,706 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@38145825{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1       | 2020-05-27 13:56:31,706 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e4f4a5c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1       | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1       | WARNING: An illegal reflective access operation has occurred
s3g_1       | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1       | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1       | WARNING: All illegal access operations will be denied in a future release
s3g_1       | May 27, 2020 1:56:41 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1       | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1       | 
s3g_1       | 2020-05-27 13:56:41,627 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d3e3085{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-0_6_0-SNAPSHOT_jar-_-any-11123022015376213990.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-0.6.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1       | 2020-05-27 13:56:41,646 [main] INFO server.AbstractConnector: Started ServerConnector@33f676f6{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1       | 2020-05-27 13:56:41,646 [main] INFO server.Server: Started @18155ms
s3g_1       | 2020-05-27 13:56:41,657 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1       | 2020-05-27 13:57:52,450 [qtp1804379080-18] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:57:52,819 [qtp1804379080-18] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-69315, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:57:52,872 [qtp1804379080-18] INFO endpoint.BucketEndpoint: Location is /bucket-69315
s3g_1       | 2020-05-27 13:57:53,304 [qtp1804379080-14] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:57:54,046 [qtp1804379080-14] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1       | 2020-05-27 13:57:54,086 [qtp1804379080-14] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1       | 2020-05-27 13:57:54,086 [qtp1804379080-14] INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started
s3g_1       | 2020-05-27 13:57:54,090 [qtp1804379080-14] WARN impl.MetricsSystemImpl: Sink prometheus already exists!
s3g_1       | 2020-05-27 13:57:56,101 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:57:56,763 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:57:57,265 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:57:57,712 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:01,486 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:01,503 [qtp1804379080-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-91835, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:01,518 [qtp1804379080-17] INFO endpoint.BucketEndpoint: Location is /bucket-91835
s3g_1       | 2020-05-27 13:58:02,005 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:02,025 [qtp1804379080-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-45191, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:02,047 [qtp1804379080-17] INFO endpoint.BucketEndpoint: Location is /bucket-45191
s3g_1       | 2020-05-27 13:58:02,525 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag1, 1}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769410400269
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:37,236 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey3"
om1_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587917227
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 1
om1_1       |   partName: "etag1"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "etag2"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:37,650 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om1_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag2, 2}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769480065038
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:37,650 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey3"
om1_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587917641
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 1
om1_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "etag2"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:38,109 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-49783/multipartKey3
om1_1       | 2020-05-27 13:58:38,109 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om1_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3because parts are in Invalid order.
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om3_1       | 2020-05-27 13:56:58,745 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1       | 2020-05-27 13:56:58,756 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1       | 2020-05-27 13:56:58,777 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context ozoneManager
om3_1       | 2020-05-27 13:56:58,779 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
om3_1       | 2020-05-27 13:56:58,780 [Listener at om3/9862] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
om3_1       | 2020-05-27 13:56:58,811 [Listener at om3/9862] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
om3_1       | 2020-05-27 13:56:58,813 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1       | 2020-05-27 13:56:58,814 [Listener at om3/9862] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
om3_1       | 2020-05-27 13:56:58,868 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1       | 2020-05-27 13:56:58,868 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1       | 2020-05-27 13:56:58,872 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
om3_1       | 2020-05-27 13:56:58,937 [Listener at om3/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om3_1       | 2020-05-27 13:56:58,966 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1039bfc4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1       | 2020-05-27 13:56:58,967 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6436e181{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om3_1       | 2020-05-27 13:56:59,104 [Thread-10] INFO impl.FollowerState: om3@group-562213E44849-FollowerState: change to CANDIDATE, lastRpcTime:1158ms, electionTimeout:1140ms
om3_1       | 2020-05-27 13:56:59,105 [Thread-10] INFO impl.RoleInfo: om3: shutdown FollowerState
om3_1       | 2020-05-27 13:56:59,106 [Thread-10] INFO impl.RaftServerImpl: om3@group-562213E44849: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om3_1       | 2020-05-27 13:56:59,107 [Thread-10] INFO impl.RoleInfo: om3: start LeaderElection
om3_1       | 2020-05-27 13:56:59,155 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: begin an election at term 1 for -1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om3_1       | 2020-05-27 13:57:00,061 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7dee835{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-0_6_0-SNAPSHOT_jar-_-any-16691134020362112425.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/ozoneManager}
om3_1       | 2020-05-27 13:57:00,107 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@64b70f41{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om3_1       | 2020-05-27 13:57:00,107 [Listener at om3/9862] INFO server.Server: Started @16356ms
om3_1       | 2020-05-27 13:57:00,142 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1       | 2020-05-27 13:57:00,142 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1       | 2020-05-27 13:57:00,150 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1       | 2020-05-27 13:57:00,152 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1       | 2020-05-27 13:57:00,278 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@53aa2fc9] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1       | 2020-05-27 13:57:00,307 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: Election TIMEOUT; received 0 response(s) [] and 1 exception(s); om3@group-562213E44849:t1, leader=null, voted=om3, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om3_1       | 2020-05-27 13:57:00,309 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1       | 2020-05-27 13:57:00,326 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: begin an election at term 2 for -1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om3_1       | 2020-05-27 13:57:00,379 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1       | 2020-05-27 13:57:00,424 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection: om3@group-562213E44849-LeaderElection1: Election REJECTED; received 1 response(s) [om3<-om1#0:FAIL-t2] and 1 exception(s); om3@group-562213E44849:t2, leader=null, voted=om3, raftlog=om3@group-562213E44849-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null
om3_1       | 2020-05-27 13:57:00,425 [om3@group-562213E44849-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om3_1       | 2020-05-27 13:57:00,425 [om3@group-562213E44849-LeaderElection1] INFO impl.RaftServerImpl: om3@group-562213E44849: changes role from CANDIDATE to FOLLOWER at term 2 for DISCOVERED_A_NEW_TERM
om3_1       | 2020-05-27 13:57:00,425 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: shutdown LeaderElection
om3_1       | 2020-05-27 13:57:00,426 [om3@group-562213E44849-LeaderElection1] INFO impl.RoleInfo: om3: start FollowerState
om3_1       | 2020-05-27 13:57:01,532 [grpc-default-executor-0] INFO impl.RaftServerImpl: om3@group-562213E44849: changes role from  FOLLOWER to FOLLOWER at term 3 for recognizeCandidate:om1
om3_1       | 2020-05-27 13:57:01,533 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown FollowerState
om3_1       | 2020-05-27 13:57:01,534 [Thread-130] INFO impl.FollowerState: om3@group-562213E44849-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
om3_1       | 2020-05-27 13:57:01,537 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start FollowerState
om3_1       | 2020-05-27 13:57:01,712 [grpc-default-executor-0] INFO impl.RaftServerImpl: om3@group-562213E44849: change Leader from null to om1 at term 3 for appendEntries, leader elected after 5274ms
om3_1       | 2020-05-27 13:57:01,745 [grpc-default-executor-0] INFO impl.RaftServerImpl: om3@group-562213E44849: set configuration 0: [om1:om1:9872, om3:om3:9872, om2:om2:9872], old=null at 0
om3_1       | 2020-05-27 13:57:01,759 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Starting segment from index:0
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:37,244 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey3"
om2_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587917227
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 1
om2_1       |   partName: "etag1"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "etag2"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:37,658 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om2_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag2, 2}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769480065038
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:37,661 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey3"
om2_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587917641
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 1
om2_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "etag2"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:38,123 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-49783/multipartKey3
om2_1       | 2020-05-27 13:58:38,124 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om2_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3because parts are in Invalid order.
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:38,124 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey3"
om2_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om2_1       |   acls {
om2_1       |     type: USER
om2_1       |     name: "dlfknslnfslf"
om2_1       |     rights: "\200"
om2_1       |     aclScope: ACCESS
om2_1       |   }
om2_1       |   modificationTime: 1590587918099
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 4
om3_1       | 2020-05-27 13:57:01,975 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0
om3_1       | 2020-05-27 13:57:48,896 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:s3v for user:hadoop
om3_1       | 2020-05-27 13:57:51,979 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:01,028 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:02,562 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:bucket-45191 in volume:s3v
om3_1       | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:186)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:06,303 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:07,783 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketDeleteRequest: Delete bucket failed for bucket:nosuchbucket in volume:s3v
om3_1       | BUCKET_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Bucket doesn't exist
om3_1       | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketDeleteRequest.validateAndUpdateCache(OMBucketDeleteRequest.java:121)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:11,076 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:16,013 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:20,269 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:25,970 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-0_70 to index:70
om3_1       | 2020-05-27 13:58:25,974 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_0 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_0-70
om3_1       | 2020-05-27 13:58:25,980 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71
om3_1       | 2020-05-27 13:58:30,244 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload: /s3v/bucket-49783/multipartKey2 Part number: 1 size 6  is less than minimum part size 5242880
om3_1       | 2020-05-27 13:58:30,247 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey2 in Volume/Bucket s3v/bucket-49783
om3_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: Entity too small: volume: s3vbucket: bucket-49783key: multipartKey2
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:241)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:30,275 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey2"
om3_1       |   multipartUploadID: "132dcb2f-7894-4330-b4bb-0f768ff03297-104240769132199945"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om2_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om2_1       | }
om2_1       | partsList {
om2_1       |   partNumber: 2
om2_1       |   partName: "etag2"
om2_1       | }
om2_1       | 
om2_1       | 2020-05-27 13:58:40,555 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName multipartKey5 in VolumeName/Bucket s3v/bucket-49783
om2_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey5
om2_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:120)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:40,557 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Unrecognized Result for S3MultipartUploadAbortRequest: keyArgs {
om2_1       |   volumeName: "s3v"
om2_1       |   bucketName: "bucket-49783"
om2_1       |   keyName: "multipartKey5"
om2_1       |   multipartUploadID: "random"
om2_1       |   modificationTime: 1590587920538
om2_1       | }
om2_1       | 
s3g_1       | 2020-05-27 13:58:02,542 [qtp1804379080-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-45191, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:02,565 [qtp1804379080-21] INFO endpoint.BucketEndpoint: Location is /bucket-45191
s3g_1       | 2020-05-27 13:58:03,052 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:03,069 [qtp1804379080-21] ERROR endpoint.BucketEndpoint: Error in Create Bucket Request for bucket: bucket_1
s3g_1       | INVALID_BUCKET_NAME org.apache.hadoop.ozone.om.exceptions.OMException: Bucket or Volume name has an unsupported character : _
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.verifyBucketName(RpcClient.java:469)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.createBucket(RpcClient.java:419)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.createBucket(RpcClient.java:410)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneVolume.createBucket(OzoneVolume.java:213)
s3g_1       | 	at org.apache.hadoop.ozone.client.ObjectStore.createS3Bucket(ObjectStore.java:118)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.createS3Bucket(EndpointBase.java:96)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.put(BucketEndpoint.java:205)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
om2_1       | 2020-05-27 13:58:41,006 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-49783, KeymultipartKey. Exception:{}
om2_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartKeyInfo(OMKeyRequest.java:372)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:314)
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:58:41,944 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-71_132 to index:132
om2_1       | 2020-05-27 13:58:41,945 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_71-132
om2_1       | 2020-05-27 13:58:42,014 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133
om2_1       | 2020-05-27 13:58:56,795 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-133_182 to index:182
om2_1       | 2020-05-27 13:58:56,797 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_133-182
om2_1       | 2020-05-27 13:58:56,829 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183
om2_1       | 2020-05-27 13:59:07,065 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:16,152 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:17,197 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-183_234 to index:234
om2_1       | 2020-05-27 13:59:17,197 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_183-234
om2_1       | 2020-05-27 13:59:17,241 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235
om2_1       | 2020-05-27 13:59:33,621 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:36,928 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-36913, Keymultidelete/f4. Exception:{}
om2_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om2_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:130)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:38,109 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey3"
om1_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om1_1       |   acls {
om1_1       |     type: USER
om1_1       |     name: "dlfknslnfslf"
om1_1       |     rights: "\200"
om1_1       |     aclScope: ACCESS
om1_1       |   }
om1_1       |   modificationTime: 1590587918099
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 4
om1_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om1_1       | }
om1_1       | partsList {
om1_1       |   partNumber: 2
om1_1       |   partName: "etag2"
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:40,546 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName multipartKey5 in VolumeName/Bucket s3v/bucket-49783
om1_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey5
om1_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:120)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       |   modificationTime: 1590587910224
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 1
om3_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769164247050"
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 2
om3_1       |   partName: "/s3v/bucket-49783/multipartKey2104240769244200971"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:31,188 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 1
om3_1       | partName: "etag1"
om3_1       | , partNumber: 2
om3_1       | partName: "etag2"
om3_1       | ]
om3_1       | 2020-05-27 13:58:31,189 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:31,194 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey3"
om3_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om3_1       |   modificationTime: 1590587911172
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 1
om3_1       |   partName: "etag1"
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 2
om3_1       |   partName: "etag2"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:31,655 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Complete MultipartUpload failed for key /s3v/bucket-49783/multipartKey3 , MPU Key has no parts in OM, parts given to upload are [partNumber: 2
om3_1       | partName: "etag1"
om3_1       | , partNumber: 1
om3_1       | partName: "etag2"
om3_1       | ]
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:06,773 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:06,789 [qtp1804379080-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-26783, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:06,801 [qtp1804379080-17] INFO endpoint.BucketEndpoint: Location is /bucket-26783
s3g_1       | 2020-05-27 13:58:07,304 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:07,753 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:11,431 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:11,443 [qtp1804379080-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-17216, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:11,455 [qtp1804379080-21] INFO endpoint.BucketEndpoint: Location is /bucket-17216
s3g_1       | 2020-05-27 13:58:11,944 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:12,431 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:12,459 [qtp1804379080-21] ERROR endpoint.BucketEndpoint: Exception occurred in headBucket
s3g_1       | org.apache.hadoop.ozone.s3.exception.OS3Exception
s3g_1       | 	at org.apache.hadoop.ozone.s3.exception.S3ErrorTable.newError(S3ErrorTable.java:107)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.EndpointBase.getBucket(EndpointBase.java:72)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.BucketEndpoint.head(BucketEndpoint.java:253)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
om3_1       | 2020-05-27 13:58:31,656 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:181)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:31,664 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey3"
om3_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om3_1       |   modificationTime: 1590587911637
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 2
om3_1       |   partName: "etag1"
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 1
om3_1       |   partName: "etag2"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:37,244 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag1, 1}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769410400269
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:37,245 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey3"
om3_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om3_1       |   modificationTime: 1590587917227
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 1
om3_1       |   partName: "etag1"
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 2
om3_1       |   partName: "etag2"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:37,660 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om3_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag2, 2}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769480065038
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:223)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:37,660 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey3"
om3_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om3_1       |   modificationTime: 1590587917641
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 1
om3_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om3_1       | }
om3_1       | partsList {
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:16,381 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:16,397 [qtp1804379080-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-59110, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:16,413 [qtp1804379080-21] INFO endpoint.BucketEndpoint: Location is /bucket-59110
s3g_1       | 2020-05-27 13:58:16,842 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:20,611 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:20,624 [qtp1804379080-17] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-49783, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:58:20,637 [qtp1804379080-17] INFO endpoint.BucketEndpoint: Location is /bucket-49783
s3g_1       | 2020-05-27 13:58:21,022 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:21,545 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:22,069 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:23,366 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:24,336 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:24,854 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:25,811 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:26,545 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:27,053 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:27,881 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:28,372 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:29,587 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:30,204 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:30,244 [qtp1804379080-21] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey2
s3g_1       | ENTITY_TOO_SMALL org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: Entity too small: volume: s3vbucket: bucket-49783key: multipartKey2
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
om3_1       |   partNumber: 2
om3_1       |   partName: "etag2"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:38,129 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: PartNumber at index 1 is 2, and its previous partNumber at index 0 is 4 for ozonekey is /s3v/bucket-49783/multipartKey3
om3_1       | 2020-05-27 13:58:38,129 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: multipartKey3 in Volume/Bucket s3v/bucket-49783
om3_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3because parts are in Invalid order.
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:198)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:40,528 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:52,969 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om2_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om2_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om2_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om2_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om2_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om2_1       | 2020-05-27 13:59:59,139 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-0-22017 for user:hadoop
om2_1       | 2020-05-27 13:59:59,198 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-235_294 to index:294
om2_1       | 2020-05-27 13:59:59,199 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_235-294
om2_1       | 2020-05-27 13:59:59,353 [om2@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_295
om2_1       | 2020-05-27 14:00:06,188 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:40,547 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Unrecognized Result for S3MultipartUploadAbortRequest: keyArgs {
om1_1       |   volumeName: "s3v"
om1_1       |   bucketName: "bucket-49783"
om1_1       |   keyName: "multipartKey5"
om1_1       |   multipartUploadID: "random"
om1_1       |   modificationTime: 1590587920538
om1_1       | }
om1_1       | 
om1_1       | 2020-05-27 13:58:40,997 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-49783, KeymultipartKey. Exception:{}
om1_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartKeyInfo(OMKeyRequest.java:372)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:314)
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:58:41,941 [IPC Server handler 41 on default port 9862] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-71_132 to index:132
om1_1       | 2020-05-27 13:58:41,942 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_71-132
om1_1       | 2020-05-27 13:58:42,009 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133
om1_1       | 2020-05-27 13:58:56,791 [IPC Server handler 49 on default port 9862] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-133_182 to index:182
om1_1       | 2020-05-27 13:58:56,791 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_133-182
om1_1       | 2020-05-27 13:58:56,824 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183
om1_1       | 2020-05-27 13:59:07,052 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:16,145 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:17,194 [IPC Server handler 72 on default port 9862] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-183_234 to index:234
om1_1       | 2020-05-27 13:59:17,195 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_183-234
om1_1       | 2020-05-27 13:59:17,235 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235
om1_1       | 2020-05-27 13:59:33,614 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:30,681 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:31,150 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:31,186 [qtp1804379080-21] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey3
s3g_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:38,130 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadCompleteRequest: Unrecognized Result for S3MultipartUploadCommitRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey3"
om3_1       |   multipartUploadID: "fe6ff939-a9a4-4c13-a629-31639e8880dc-104240769315045388"
om3_1       |   acls {
om3_1       |     type: USER
om3_1       |     name: "dlfknslnfslf"
om3_1       |     rights: "\200"
om3_1       |     aclScope: ACCESS
om3_1       |   }
om3_1       |   modificationTime: 1590587918099
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 4
om3_1       |   partName: "/s3v/bucket-49783/multipartKey3104240769410400269"
om3_1       | }
om3_1       | partsList {
om3_1       |   partNumber: 2
om3_1       |   partName: "etag2"
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:40,568 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Abort Multipart request is failed for KeyName multipartKey5 in VolumeName/Bucket s3v/bucket-49783
om3_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Abort Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey5
om3_1       | 	at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadAbortRequest.validateAndUpdateCache(S3MultipartUploadAbortRequest.java:120)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:40,570 [OM StateMachine ApplyTransaction Thread - 0] ERROR multipart.S3MultipartUploadAbortRequest: Unrecognized Result for S3MultipartUploadAbortRequest: keyArgs {
om3_1       |   volumeName: "s3v"
om3_1       |   bucketName: "bucket-49783"
om3_1       |   keyName: "multipartKey5"
om3_1       |   multipartUploadID: "random"
om3_1       |   modificationTime: 1590587920538
om3_1       | }
om3_1       | 
om3_1       | 2020-05-27 13:58:41,007 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:bucket-49783, KeymultipartKey. Exception:{}
om3_1       | NO_SUCH_MULTIPART_UPLOAD_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No such Multipart upload is with specified uploadId random
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareMultipartKeyInfo(OMKeyRequest.java:372)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareKeyInfo(OMKeyRequest.java:314)
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:58:41,943 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-71_132 to index:132
om3_1       | 2020-05-27 13:58:41,944 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_71 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_71-132
om3_1       | 2020-05-27 13:58:42,014 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133
om3_1       | 2020-05-27 13:58:56,796 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-133_182 to index:182
om3_1       | 2020-05-27 13:58:56,797 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_133 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_133-182
om3_1       | 2020-05-27 13:58:56,829 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183
om3_1       | 2020-05-27 13:59:07,062 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:16,152 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:36,907 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-36913, Keymultidelete/f4. Exception:{}
om1_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om1_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:130)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:40,520 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:52,954 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om1_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om1_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om1_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om1_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om1_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om1_1       | 2020-05-27 13:59:59,130 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-0-22017 for user:hadoop
om1_1       | 2020-05-27 13:59:59,196 [IPC Server handler 71 on default port 9862] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-235_294 to index:294
om1_1       | 2020-05-27 13:59:59,196 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_235-294
om1_1       | 2020-05-27 13:59:59,348 [om1@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_295
om1_1       | 2020-05-27 14:00:06,182 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:17,196 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-183_234 to index:234
om3_1       | 2020-05-27 13:59:17,198 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_183 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_183-234
om3_1       | 2020-05-27 13:59:17,241 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235
om3_1       | 2020-05-27 13:59:33,628 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:36,930 [OM StateMachine ApplyTransaction Thread - 0] ERROR key.OMKeyDeleteRequest: Key delete failed. Volume:s3v, Bucket:bucket-36913, Keymultidelete/f4. Exception:{}
om3_1       | KEY_NOT_FOUND org.apache.hadoop.ozone.om.exceptions.OMException: Key not found
om3_1       | 	at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:130)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:40,527 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:52,965 [OM StateMachine ApplyTransaction Thread - 0] ERROR volume.OMVolumeCreateRequest: Volume creation failed for user:hadoop volume:s3v
om3_1       | VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists
om3_1       | 	at org.apache.hadoop.ozone.om.request.volume.OMVolumeCreateRequest.validateAndUpdateCache(OMVolumeCreateRequest.java:174)
om3_1       | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:226)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)
om3_1       | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:242)
om3_1       | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | 2020-05-27 13:59:59,140 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-0-22017 for user:hadoop
om3_1       | 2020-05-27 13:59:59,198 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolling segment log-235_294 to index:294
om3_1       | 2020-05-27 13:59:59,198 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_235 to /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_235-294
om3_1       | 2020-05-27 13:59:59,354 [om3@group-562213E44849-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-562213E44849-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fafe1b60-c241-37cc-98f4-562213e44849/current/log_inprogress_295
om3_1       | 2020-05-27 14:00:06,188 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:31,616 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:31,653 [qtp1804379080-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey3
s3g_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:32,124 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:33,193 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:34,033 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:37,214 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:37,239 [qtp1804379080-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey3
s3g_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag1, 1}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769410400269
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
scm_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2020-05-27 13:56:33,783 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = a5038335e5ea/172.21.0.6
scm_1       | STARTUP_MSG:   args = [--init]
scm_1       | STARTUP_MSG:   version = 3.2.1
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
scm_1       | STARTUP_MSG:   java = 11.0.6
scm_1       | ************************************************************/
scm_1       | 2020-05-27 13:56:33,810 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-05-27 13:56:34,251 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-05-27 13:56:34,594 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-b464b64b-2e94-4301-9394-a54c0dc63980
scm_1       | 2020-05-27 13:56:34,655 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at a5038335e5ea/172.21.0.6
scm_1       | ************************************************************/
scm_1       | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2020-05-27 13:56:40,736 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = a5038335e5ea/172.21.0.6
scm_1       | STARTUP_MSG:   args = []
scm_1       | STARTUP_MSG:   version = 3.2.1
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-cac3336-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
scm_1       | STARTUP_MSG:   java = 11.0.6
scm_1       | ************************************************************/
scm_1       | 2020-05-27 13:56:40,757 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-05-27 13:56:40,996 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-05-27 13:56:41,189 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-05-27 13:56:41,518 [main] INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@210ab13f
scm_1       | 2020-05-27 13:56:41,519 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1       | 2020-05-27 13:56:41,619 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1       | 2020-05-27 13:56:41,713 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1       | 2020-05-27 13:56:41,723 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1       | 2020-05-27 13:56:41,752 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2020-05-27 13:56:41,753 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1       | 2020-05-27 13:56:41,813 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 0 nodes. Healthy nodes 0
scm_1       | 2020-05-27 13:56:42,227 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-05-27 13:56:42,247 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1       | 2020-05-27 13:56:42,278 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-05-27 13:56:42,278 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1       | 2020-05-27 13:56:42,286 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-05-27 13:56:42,290 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1       | 2020-05-27 13:56:42,306 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1       | 2020-05-27 13:56:42,306 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1       | 2020-05-27 13:56:42,322 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @6425ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1       | 2020-05-27 13:56:42,444 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2020-05-27 13:56:42,456 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1       | 2020-05-27 13:56:42,460 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1       | 2020-05-27 13:56:42,461 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context scm
scm_1       | 2020-05-27 13:56:42,461 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs
scm_1       | 2020-05-27 13:56:42,461 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static
scm_1       | 2020-05-27 13:56:42,584 [Listener at 0.0.0.0/9860] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
scm_1       | 2020-05-27 13:56:42,609 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1       | 2020-05-27 13:56:42,656 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1       | 2020-05-27 13:56:42,714 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1       | 2020-05-27 13:56:42,714 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1       | 2020-05-27 13:56:42,890 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1       | 2020-05-27 13:56:42,891 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-05-27 13:56:42,921 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1       | 2020-05-27 13:56:42,979 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1       | 2020-05-27 13:56:42,980 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1       | 2020-05-27 13:56:42,981 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-05-27 13:56:42,982 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1       | 2020-05-27 13:56:43,014 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1       | 2020-05-27 13:56:43,015 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1       | 2020-05-27 13:56:43,016 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-05-27 13:56:43,017 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1       | 2020-05-27 13:56:43,052 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1       | 2020-05-27 13:56:43,057 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
scm_1       | 2020-05-27 13:56:43,194 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1       | 2020-05-27 13:56:43,206 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1       | 2020-05-27 13:56:43,207 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1       | 2020-05-27 13:56:43,244 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2020-05-27 13:56:43,257 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2bf94401{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1       | 2020-05-27 13:56:43,257 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c298dc{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1       | 2020-05-27 13:56:44,034 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopology: Added a new node: /default-rack/a22687bf-1be9-4837-b2b6-cbf11c380a1a
scm_1       | 2020-05-27 13:56:44,035 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}
scm_1       | 2020-05-27 13:56:44,071 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1       | 2020-05-27 13:56:44,071 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-05-27 13:56:44,203 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b7e70a29-c087-4293-8da3-66e4e1ac37e8 to datanode:a22687bf-1be9-4837-b2b6-cbf11c380a1a
scm_1       | 2020-05-27 13:56:44,244 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: b7e70a29-c087-4293-8da3-66e4e1ac37e8, Nodes: a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-05-27T13:56:44.197972Z]
scm_1       | 2020-05-27 13:56:44,283 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 1 nodes. Healthy nodes 1
scm_1       | 2020-05-27 13:56:44,564 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopology: Added a new node: /default-rack/bbe1b503-f6e3-42f6-9e2c-66477e765538
scm_1       | 2020-05-27 13:56:44,566 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : bbe1b503-f6e3-42f6-9e2c-66477e765538{ip: 172.21.0.4, host: ozone-om-ha-s3_datanode_3.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}
scm_1       | 2020-05-27 13:56:44,566 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1       | 2020-05-27 13:56:44,566 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-05-27 13:56:44,567 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2ca17910-3619-4b83-ad3b-817ecca4b57c to datanode:bbe1b503-f6e3-42f6-9e2c-66477e765538
scm_1       | 2020-05-27 13:56:44,585 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2ca17910-3619-4b83-ad3b-817ecca4b57c, Nodes: bbe1b503-f6e3-42f6-9e2c-66477e765538{ip: 172.21.0.4, host: ozone-om-ha-s3_datanode_3.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-05-27T13:56:44.567752Z]
scm_1       | 2020-05-27 13:56:44,587 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 2 nodes. Healthy nodes 2
scm_1       | 2020-05-27 13:56:44,599 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5e1d03d7{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-0_6_0-SNAPSHOT_jar-_-any-2932892089581607645.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/scm}
scm_1       | 2020-05-27 13:56:44,615 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@2d5f7182{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1       | 2020-05-27 13:56:44,615 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8719ms
scm_1       | 2020-05-27 13:56:44,642 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1       | 2020-05-27 13:56:44,650 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1       | 2020-05-27 13:56:44,659 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1       | 2020-05-27 13:56:44,672 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2849434b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1       | 2020-05-27 13:56:45,673 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopology: Added a new node: /default-rack/64aa2774-d5fb-40de-bd90-35cfd81f6839
scm_1       | 2020-05-27 13:56:45,674 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 64aa2774-d5fb-40de-bd90-35cfd81f6839{ip: 172.21.0.2, host: ozone-om-ha-s3_datanode_1.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}
scm_1       | 2020-05-27 13:56:45,674 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a0748133-47ae-4a27-85f0-b3a618137565 to datanode:64aa2774-d5fb-40de-bd90-35cfd81f6839
scm_1       | 2020-05-27 13:56:45,675 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a0748133-47ae-4a27-85f0-b3a618137565, Nodes: 64aa2774-d5fb-40de-bd90-35cfd81f6839{ip: 172.21.0.2, host: ozone-om-ha-s3_datanode_1.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-05-27T13:56:45.674352Z]
scm_1       | 2020-05-27 13:56:45,675 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-05-27 13:56:45,676 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-05-27 13:56:45,675 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1       | 2020-05-27 13:56:45,677 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:37,631 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:37,652 [qtp1804379080-21] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey3
s3g_1       | INVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3. Provided Part info is { etag2, 2}, where as OM has partName /s3v/bucket-49783/multipartKey3104240769480065038
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
scm_1       | 2020-05-27 13:56:45,677 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1       | 2020-05-27 13:56:45,678 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-05-27 13:56:45,682 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b321e0da-40fa-404a-8ad9-330873a83a56 to datanode:a22687bf-1be9-4837-b2b6-cbf11c380a1a
scm_1       | 2020-05-27 13:56:45,689 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b321e0da-40fa-404a-8ad9-330873a83a56 to datanode:bbe1b503-f6e3-42f6-9e2c-66477e765538
scm_1       | 2020-05-27 13:56:45,690 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b321e0da-40fa-404a-8ad9-330873a83a56 to datanode:64aa2774-d5fb-40de-bd90-35cfd81f6839
scm_1       | 2020-05-27 13:56:45,691 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: b321e0da-40fa-404a-8ad9-330873a83a56, Nodes: a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}bbe1b503-f6e3-42f6-9e2c-66477e765538{ip: 172.21.0.4, host: ozone-om-ha-s3_datanode_3.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}64aa2774-d5fb-40de-bd90-35cfd81f6839{ip: 172.21.0.2, host: ozone-om-ha-s3_datanode_1.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-05-27T13:56:45.682090Z]
scm_1       | 2020-05-27 13:56:45,691 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-05-27 13:56:47,788 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: b7e70a29-c087-4293-8da3-66e4e1ac37e8, Nodes: a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:a22687bf-1be9-4837-b2b6-cbf11c380a1a, CreationTimestamp2020-05-27T13:56:44.197972Z] moved to OPEN state
scm_1       | 2020-05-27 13:56:47,796 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-05-27 13:56:47,807 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-05-27 13:56:48,565 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2ca17910-3619-4b83-ad3b-817ecca4b57c, Nodes: bbe1b503-f6e3-42f6-9e2c-66477e765538{ip: 172.21.0.4, host: ozone-om-ha-s3_datanode_3.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:bbe1b503-f6e3-42f6-9e2c-66477e765538, CreationTimestamp2020-05-27T13:56:44.567752Z] moved to OPEN state
scm_1       | 2020-05-27 13:56:48,571 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-05-27 13:56:48,571 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-05-27 13:56:49,529 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a0748133-47ae-4a27-85f0-b3a618137565, Nodes: 64aa2774-d5fb-40de-bd90-35cfd81f6839{ip: 172.21.0.2, host: ozone-om-ha-s3_datanode_1.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:64aa2774-d5fb-40de-bd90-35cfd81f6839, CreationTimestamp2020-05-27T13:56:45.674352Z] moved to OPEN state
scm_1       | 2020-05-27 13:56:49,532 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-05-27 13:56:49,532 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-05-27 13:56:53,547 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: b321e0da-40fa-404a-8ad9-330873a83a56, Nodes: a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}bbe1b503-f6e3-42f6-9e2c-66477e765538{ip: 172.21.0.4, host: ozone-om-ha-s3_datanode_3.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}64aa2774-d5fb-40de-bd90-35cfd81f6839{ip: 172.21.0.2, host: ozone-om-ha-s3_datanode_1.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:a22687bf-1be9-4837-b2b6-cbf11c380a1a, CreationTimestamp2020-05-27T13:56:45.682090Z] moved to OPEN state
scm_1       | 2020-05-27 13:56:53,548 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-05-27 13:56:53,548 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1       | 2020-05-27 13:56:53,552 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1       | 2020-05-27 13:56:53,552 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1       | 2020-05-27 13:56:53,553 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1       | 2020-05-27 13:58:41,822 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-05-27 13:58:41,822 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-05-27 13:58:58,376 [IPC Server handler 65 on default port 9863] INFO server.SCMBlockProtocolServer: SCM is informed by OM to delete 5 blocks
scm_1       | 2020-05-27 13:58:58,377 [IPC Server handler 65 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240770478637071 bcsId: 0,conID: 1 locID: 104240770477457422 bcsId: 0,conID: 1 locID: 104240770475556877 bcsId: 0
scm_1       | 2020-05-27 13:58:58,392 [IPC Server handler 65 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240768754319362 bcsId: 0
scm_1       | 2020-05-27 13:58:58,392 [IPC Server handler 65 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240769481572361 bcsId: 0
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:38,068 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:38,113 [qtp1804379080-17] ERROR endpoint.ObjectEndpoint: Error in Complete Multipart Upload Request for bucket: bucket-49783, , key: multipartKey3
s3g_1       | INVALID_PART_ORDER org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3vbucket: bucket-49783key: multipartKey3because parts are in Invalid order.
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:589)
s3g_1       | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.completeMultipartUpload(OzoneManagerProtocolClientSideTranslatorPB.java:884)
s3g_1       | 	at org.apache.hadoop.ozone.client.rpc.RpcClient.completeMultipartUpload(RpcClient.java:901)
s3g_1       | 	at org.apache.hadoop.ozone.client.OzoneBucket.completeMultipartUpload(OzoneBucket.java:446)
s3g_1       | 	at org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.completeMultipartUpload(ObjectEndpoint.java:476)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
s3g_1       | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
s3g_1       | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
s3g_1       | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)
s3g_1       | 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)
s3g_1       | 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
s3g_1       | 	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:316)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:298)
s3g_1       | 	at org.glassfish.jersey.internal.Errors.process(Errors.java:268)
s3g_1       | 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)
s3g_1       | 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)
s3g_1       | 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)
s3g_1       | 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)
s3g_1       | 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder$NotAsyncServlet.service(ServletHolder.java:1395)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:755)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1617)
s3g_1       | 	at org.apache.hadoop.ozone.s3.RootPageDisplayFilter.doFilter(RootPageDisplayFilter.java:53)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1640)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
s3g_1       | 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
scm_1       | 2020-05-27 13:58:58,392 [IPC Server handler 65 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240770058354699 bcsId: 0
scm_1       | 2020-05-27 13:58:58,393 [IPC Server handler 65 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240770275147788 bcsId: 0
scm_1       | 2020-05-27 13:59:58,429 [IPC Server handler 50 on default port 9863] INFO server.SCMBlockProtocolServer: SCM is informed by OM to delete 5 blocks
scm_1       | 2020-05-27 13:59:58,429 [IPC Server handler 50 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240772356309016 bcsId: 0
scm_1       | 2020-05-27 13:59:58,430 [IPC Server handler 50 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240772568121369 bcsId: 0
scm_1       | 2020-05-27 13:59:58,431 [IPC Server handler 50 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240772974903322 bcsId: 0
scm_1       | 2020-05-27 13:59:58,432 [IPC Server handler 50 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240773506727963 bcsId: 0
scm_1       | 2020-05-27 13:59:58,432 [IPC Server handler 50 on default port 9863] INFO block.BlockManagerImpl: Deleting blocks conID: 1 locID: 104240773546246172 bcsId: 0
scm_1       | 2020-05-27 13:59:59,193 [IPC Server handler 6 on default port 9863] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: e9339f91-70d0-424f-8151-d2db21404974, Nodes: a22687bf-1be9-4837-b2b6-cbf11c380a1a{ip: 172.21.0.5, host: ozone-om-ha-s3_datanode_2.ozone-om-ha-s3_default, networkLocation: /default-rack, certSerialId: null}, Type:STAND_ALONE, Factor:ONE, State:OPEN, leaderId:null, CreationTimestamp2020-05-27T13:59:59.193239Z]
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
s3g_1       | 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
s3g_1       | 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
s3g_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
s3g_1       | 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)
s3g_1       | 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
s3g_1       | 	at org.eclipse.jetty.server.Server.handle(Server.java:500)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
s3g_1       | 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
s3g_1       | 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
s3g_1       | 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
s3g_1       | 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
s3g_1       | 	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
s3g_1       | 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
s3g_1       | 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
s3g_1       | 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
s3g_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
s3g_1       | 2020-05-27 13:58:38,512 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:38,934 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:39,522 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:40,011 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:40,522 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:40,970 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:41,421 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:41,927 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:45,309 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:45,933 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:46,436 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:46,918 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:47,349 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:48,032 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:48,345 [qtp1804379080-17] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:48,360 [qtp1804379080-16] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:48,360 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:49,461 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:49,958 [qtp1804379080-16] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:50,090 [qtp1804379080-18] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:50,094 [qtp1804379080-16] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:50,096 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:50,981 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:51,756 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:55,430 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:55,884 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:56,775 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:57,233 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:57,912 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:58,872 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:58:59,336 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:00,730 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:01,415 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:01,857 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:02,543 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:03,073 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:03,529 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:07,467 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:07,474 [qtp1804379080-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-84959, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:07,486 [qtp1804379080-20] INFO endpoint.BucketEndpoint: Location is /bucket-84959
s3g_1       | 2020-05-27 13:59:07,965 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:07,974 [qtp1804379080-21] INFO rpc.RpcClient: Creating Bucket: s3v/destbucket-26685, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:07,987 [qtp1804379080-21] INFO endpoint.BucketEndpoint: Location is /destbucket-26685
s3g_1       | 2020-05-27 13:59:08,487 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:08,979 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:09,406 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:10,113 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:10,534 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:11,070 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:11,494 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:11,918 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:12,376 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:12,797 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:16,652 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:16,679 [qtp1804379080-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-14064, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:16,692 [qtp1804379080-20] INFO endpoint.BucketEndpoint: Location is /bucket-14064
s3g_1       | 2020-05-27 13:59:17,084 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:17,769 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:18,197 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:18,597 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:18,978 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:19,363 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:19,900 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:20,315 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:24,450 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:24,852 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:25,337 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:25,736 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:26,521 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:27,163 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:27,582 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:28,067 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:28,488 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:29,016 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:34,102 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:34,119 [qtp1804379080-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-36913, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:34,131 [qtp1804379080-20] INFO endpoint.BucketEndpoint: Location is /bucket-36913
s3g_1       | 2020-05-27 13:59:34,633 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:35,237 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:35,821 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:36,432 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:36,852 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:37,323 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:40,983 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:40,993 [qtp1804379080-21] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-75918, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:41,005 [qtp1804379080-21] INFO endpoint.BucketEndpoint: Location is /bucket-75918
s3g_1       | 2020-05-27 13:59:41,387 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:42,009 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:42,410 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:42,941 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:43,335 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:43,761 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:44,207 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:44,653 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:45,122 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:45,534 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:45,947 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:46,381 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:46,866 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:47,276 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:47,727 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:48,183 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:48,623 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:49,042 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:49,424 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:49,823 [qtp1804379080-21] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:53,439 [qtp1804379080-20] INFO s3.AWSV4SignatureProcessor: Initializing request header parser
s3g_1       | 2020-05-27 13:59:53,451 [qtp1804379080-20] INFO rpc.RpcClient: Creating Bucket: s3v/bucket-88255, with Versioning false and Storage Type set to DISK and Encryption set to false 
s3g_1       | 2020-05-27 13:59:53,462 [qtp1804379080-20] INFO endpoint.BucketEndpoint: Location is /bucket-88255
