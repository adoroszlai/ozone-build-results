Attaching to ozone-om-ha_datanode_3, ozone-om-ha_datanode_2, ozone-om-ha_scm_1, ozone-om-ha_datanode_1, ozone-om-ha_om2_1, ozone-om-ha_om3_1, ozone-om-ha_om1_1
datanode_2  | Enabled profiling in kernel
datanode_2  | 2020-04-01 13:00:18 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_2  | /************************************************************
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2  | STARTUP_MSG:   host = ad68cea49847/172.21.0.7
datanode_2  | STARTUP_MSG:   args = []
datanode_2  | STARTUP_MSG:   version = 3.2.0
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2  | STARTUP_MSG:   java = 11.0.3
datanode_2  | ************************************************************/
datanode_2  | 2020-04-01 13:00:19 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2020-04-01 13:00:20 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2020-04-01 13:00:21 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2  | 2020-04-01 13:00:22 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 2020-04-01 13:00:22 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2  | 2020-04-01 13:00:23 INFO  HddsDatanodeService:204 - HddsDatanodeService host:ad68cea49847 ip:172.21.0.7
datanode_2  | 2020-04-01 13:00:24 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 2020-04-01 13:00:24 INFO  HddsVolume:177 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_2  | 2020-04-01 13:00:24 INFO  MutableVolumeSet:188 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2020-04-01 13:00:24 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2  | 2020-04-01 13:00:24 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2020-04-01 13:00:29 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2020-04-01 13:00:30 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
datanode_2  | 2020-04-01 13:00:30 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
datanode_2  | 2020-04-01 13:00:30 INFO  GrpcService:43 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2  | 2020-04-01 13:00:30 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-01 13:00:30 INFO  GrpcService:43 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2  | 2020-04-01 13:00:30 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-01 13:00:31 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-01 13:00:31 INFO  BaseHttpServer:170 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2  | 2020-04-01 13:00:31 INFO  log:169 - Logging initialized @18638ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | 2020-04-01 13:00:32 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2020-04-01 13:00:32 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2020-04-01 13:00:32 INFO  HttpServer2:970 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2020-04-01 13:00:32 INFO  HttpServer2:946 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2020-04-01 13:00:32 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2020-04-01 13:00:32 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2020-04-01 13:00:32 WARN  BaseHttpServer:123 - /prof java profiling servlet is activated. Not safe for production!
datanode_2  | 2020-04-01 13:00:32 INFO  HttpServer2:1188 - Jetty bound to port 9882
datanode_2  | 2020-04-01 13:00:32 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2  | 2020-04-01 13:00:32 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_2  | 2020-04-01 13:00:32 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2  | 2020-04-01 13:00:32 INFO  session:140 - node0 Scavenging every 600000ms
datanode_2  | 2020-04-01 13:00:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@51d719bc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2020-04-01 13:00:32 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@4d266391{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2020-04-01 13:00:32 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@59c70ceb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-17888097017489951894.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2  | 2020-04-01 13:00:33 INFO  AbstractConnector:330 - Started ServerConnector@49c17ba4{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2020-04-01 13:00:33 INFO  Server:399 - Started @19788ms
datanode_2  | 2020-04-01 13:00:33 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2  | 2020-04-01 13:00:33 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2  | 2020-04-01 13:00:33 INFO  BaseHttpServer:284 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2020-04-01 13:00:33 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2  | 2020-04-01 13:00:33 INFO  InitDatanodeState:147 - DatanodeDetails is persisted to /data/datanode.id
datanode_2  | 2020-04-01 13:00:36 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2  | java.net.SocketTimeoutException: Call From ad68cea49847/172.21.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.7:55858 remote=scm/172.21.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
datanode_2  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)
datanode_2  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
datanode_2  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
datanode_2  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
datanode_2  | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1  | Enabled profiling in kernel
datanode_1  | 2020-04-01 13:00:17 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = 3fb36f21e5e7/172.21.0.6
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 3.2.0
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1  | STARTUP_MSG:   java = 11.0.3
datanode_1  | ************************************************************/
datanode_1  | 2020-04-01 13:00:17 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2020-04-01 13:00:18 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2020-04-01 13:00:19 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1  | 2020-04-01 13:00:21 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2020-04-01 13:00:21 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1  | 2020-04-01 13:00:22 INFO  HddsDatanodeService:204 - HddsDatanodeService host:3fb36f21e5e7 ip:172.21.0.6
datanode_1  | 2020-04-01 13:00:22 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1  | 2020-04-01 13:00:22 INFO  HddsVolume:177 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_1  | 2020-04-01 13:00:23 INFO  MutableVolumeSet:188 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1  | 2020-04-01 13:00:23 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1  | 2020-04-01 13:00:23 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2020-04-01 13:00:28 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2020-04-01 13:00:28 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
datanode_1  | 2020-04-01 13:00:29 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
datanode_1  | 2020-04-01 13:00:29 INFO  GrpcService:43 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1  | 2020-04-01 13:00:29 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-01 13:00:29 INFO  GrpcService:43 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1  | 2020-04-01 13:00:29 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-01 13:00:29 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-01 13:00:30 INFO  BaseHttpServer:170 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1  | 2020-04-01 13:00:30 INFO  log:169 - Logging initialized @18706ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1  | 2020-04-01 13:00:31 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1  | 2020-04-01 13:00:31 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2020-04-01 13:00:31 INFO  HttpServer2:970 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2020-04-01 13:00:31 INFO  HttpServer2:946 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1  | 2020-04-01 13:00:31 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2020-04-01 13:00:31 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2020-04-01 13:00:31 WARN  BaseHttpServer:123 - /prof java profiling servlet is activated. Not safe for production!
datanode_1  | 2020-04-01 13:00:31 INFO  HttpServer2:1188 - Jetty bound to port 9882
datanode_1  | 2020-04-01 13:00:31 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1  | 2020-04-01 13:00:31 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1  | 2020-04-01 13:00:31 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1  | 2020-04-01 13:00:31 INFO  session:140 - node0 Scavenging every 600000ms
datanode_1  | 2020-04-01 13:00:31 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@51d719bc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2020-04-01 13:00:31 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@4d266391{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2020-04-01 13:00:31 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@59c70ceb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-15921829428711709504.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2020-04-01 13:00:32 INFO  AbstractConnector:330 - Started ServerConnector@49c17ba4{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1  | 2020-04-01 13:00:32 INFO  Server:399 - Started @20154ms
datanode_1  | 2020-04-01 13:00:32 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1  | 2020-04-01 13:00:32 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1  | 2020-04-01 13:00:32 INFO  BaseHttpServer:284 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1  | 2020-04-01 13:00:32 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1  | 2020-04-01 13:00:32 INFO  InitDatanodeState:147 - DatanodeDetails is persisted to /data/datanode.id
datanode_1  | 2020-04-01 13:00:35 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:36 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1  | java.net.SocketTimeoutException: Call From 3fb36f21e5e7/172.21.0.6 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.6:58534 remote=scm/172.21.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
datanode_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)
datanode_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
datanode_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
datanode_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
datanode_1  | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
om1_1       | 2020-04-01 13:00:16 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
om1_1       | /************************************************************
om1_1       | STARTUP_MSG: Starting OzoneManager
om1_1       | STARTUP_MSG:   host = c778a8104d58/172.21.0.2
om1_1       | STARTUP_MSG:   args = [--init]
om1_1       | STARTUP_MSG:   version = 3.2.0
om1_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-tools-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om1_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om1_1       | STARTUP_MSG:   java = 11.0.3
om1_1       | ************************************************************/
om1_1       | 2020-04-01 13:00:16 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
om1_1       | 2020-04-01 13:00:22 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1       | 2020-04-01 13:00:22 INFO  OMHANodeDetails:182 - Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1       | 2020-04-01 13:00:22 INFO  OMHANodeDetails:315 - Setting configuration key ozone.om.address with value of key ozone.om.address.om1: om1
om1_1       | 2020-04-01 13:00:22 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1       | 2020-04-01 13:00:25 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:26 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:27 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:28 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:29 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:30 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:31 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:32 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:33 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:34 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om1_1       | 2020-04-01 13:00:34 INFO  RetriableTask:62 - Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2df71362-8533-41d2-b5ae-3773a97c69f8
om1_1       | 2020-04-01 13:00:39 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
om1_1       | /************************************************************
om1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at c778a8104d58/172.21.0.2
om1_1       | ************************************************************/
om1_1       | Enabled profiling in kernel
scm_1       | 2020-04-01 13:00:26 INFO  StorageContainerManagerStarter:51 - STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = 852763d798a5/172.21.0.5
scm_1       | STARTUP_MSG:   args = [--init]
scm_1       | STARTUP_MSG:   version = 3.2.0
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | STARTUP_MSG:   java = 11.0.3
scm_1       | ************************************************************/
scm_1       | 2020-04-01 13:00:26 INFO  StorageContainerManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-04-01 13:00:27 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-01 13:00:27 INFO  StorageContainerManager:635 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-2df71362-8533-41d2-b5ae-3773a97c69f8
scm_1       | 2020-04-01 13:00:28 INFO  StorageContainerManagerStarter:51 - SHUTDOWN_MSG: 
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 852763d798a5/172.21.0.5
scm_1       | ************************************************************/
scm_1       | Enabled profiling in kernel
scm_1       | 2020-04-01 13:00:33 INFO  StorageContainerManagerStarter:51 - STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = 852763d798a5/172.21.0.5
scm_1       | STARTUP_MSG:   args = []
scm_1       | STARTUP_MSG:   version = 3.2.0
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | STARTUP_MSG:   java = 11.0.3
scm_1       | ************************************************************/
scm_1       | 2020-04-01 13:00:33 INFO  StorageContainerManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-04-01 13:00:33 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-01 13:00:33 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-01 13:00:34 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@27912e3
scm_1       | 2020-04-01 13:00:34 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1       | 2020-04-01 13:00:34 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1       | 2020-04-01 13:00:34 INFO  ContainerPlacementPolicyFactory:59 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1       | 2020-04-01 13:00:34 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-01 13:00:34 INFO  SCMPipelineManager:151 - No pipeline exists in current db
scm_1       | 2020-04-01 13:00:34 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-01 13:00:34 INFO  HealthyPipelineSafeModeRule:88 - Total pipeline count is 0, healthy pipeline threshold count is 0
scm_1       | 2020-04-01 13:00:34 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1       | 2020-04-01 13:00:34 WARN  PipelinePlacementPolicy:150 - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.
scm_1       | 2020-04-01 13:00:34 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-01 13:00:34 INFO  Server:1074 - Starting Socket Reader #1 for port 9861
scm_1       | 2020-04-01 13:00:34 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-01 13:00:35 INFO  Server:1074 - Starting Socket Reader #1 for port 9863
scm_1       | 2020-04-01 13:00:35 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-01 13:00:35 INFO  Server:1074 - Starting Socket Reader #1 for port 9860
scm_1       | 2020-04-01 13:00:35 INFO  BaseHttpServer:170 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1       | 2020-04-01 13:00:35 INFO  log:169 - Logging initialized @5113ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1       | 2020-04-01 13:00:35 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2020-04-01 13:00:35 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1       | 2020-04-01 13:00:35 INFO  HttpServer2:970 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1       | 2020-04-01 13:00:35 INFO  HttpServer2:946 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1       | 2020-04-01 13:00:35 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2020-04-01 13:00:35 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2020-04-01 13:00:35 WARN  BaseHttpServer:123 - /prof java profiling servlet is activated. Not safe for production!
scm_1       | 2020-04-01 13:00:35 INFO  StorageContainerManager:773 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1       | 2020-04-01 13:00:35 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1       | 2020-04-01 13:00:35 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1       | 2020-04-01 13:00:35 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1       | 2020-04-01 13:00:35 INFO  SCMClientProtocolServer:162 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1       | 2020-04-01 13:00:35 INFO  Server:1314 - IPC Server Responder: starting
scm_1       | 2020-04-01 13:00:35 INFO  Server:1153 - IPC Server listener on 9860: starting
scm_1       | 2020-04-01 13:00:35 INFO  StorageContainerManager:785 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1       | 2020-04-01 13:00:35 INFO  SCMBlockProtocolServer:147 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1       | 2020-04-01 13:00:35 INFO  Server:1314 - IPC Server Responder: starting
scm_1       | 2020-04-01 13:00:35 INFO  Server:1153 - IPC Server listener on 9863: starting
scm_1       | 2020-04-01 13:00:35 INFO  StorageContainerManager:791 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1       | 2020-04-01 13:00:35 INFO  SCMDatanodeProtocolServer:176 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1       | 2020-04-01 13:00:35 INFO  Server:1314 - IPC Server Responder: starting
scm_1       | 2020-04-01 13:00:35 INFO  Server:1153 - IPC Server listener on 9861: starting
scm_1       | 2020-04-01 13:00:36 INFO  HttpServer2:1188 - Jetty bound to port 9876
scm_1       | 2020-04-01 13:00:36 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1       | 2020-04-01 13:00:36 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1       | 2020-04-01 13:00:36 INFO  session:338 - No SessionScavenger set, using defaults
scm_1       | 2020-04-01 13:00:36 INFO  session:140 - node0 Scavenging every 660000ms
scm_1       | 2020-04-01 13:00:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7965a51c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1       | 2020-04-01 13:00:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@528f8f8b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1       | 2020-04-01 13:00:36 WARN  Server:1523 - IPC Server handler 0 on 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.21.0.7:55858: output error
scm_1       | 2020-04-01 13:00:36 INFO  Server:2695 - IPC Server handler 0 on 9861 caught an exception
scm_1       | java.nio.channels.AsynchronousCloseException
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3250)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:137)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1473)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1543)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2593)
om2_1       | 2020-04-01 13:00:20 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
om2_1       | /************************************************************
om2_1       | STARTUP_MSG: Starting OzoneManager
om2_1       | STARTUP_MSG:   host = 70a283850dba/172.21.0.3
om2_1       | STARTUP_MSG:   args = [--init]
om2_1       | STARTUP_MSG:   version = 3.2.0
datanode_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.6:58534 remote=scm/172.21.0.5:9861]
datanode_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:557)
datanode_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1816)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)
datanode_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)
datanode_1  | 2020-04-01 13:00:36 INFO  OzoneContainer:231 - Attempting to start container services.
datanode_1  | 2020-04-01 13:00:36 INFO  OzoneContainer:195 - Background container scanner has been disabled.
datanode_1  | 2020-04-01 13:00:36 INFO  XceiverServerRatis:406 - Starting XceiverServerRatis 42eecb8e-afca-4afa-8236-1d066831b530 at port 9858
datanode_1  | 2020-04-01 13:00:36 INFO  RaftServerProxy:299 - 42eecb8e-afca-4afa-8236-1d066831b530: start RPC server
datanode_1  | 2020-04-01 13:00:36 INFO  GrpcService:158 - 42eecb8e-afca-4afa-8236-1d066831b530: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerProxy:89 - 42eecb8e-afca-4afa-8236-1d066831b530: addNew group-EA9A370CAE09:[42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] returns group-EA9A370CAE09:java.util.concurrent.CompletableFuture@189240aa[Not completed]
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:97 - 42eecb8e-afca-4afa-8236-1d066831b530: new RaftServerImpl for group-EA9A370CAE09:[42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:103 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: ConfigurationManager, init=-1: [42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/71a7bebb-be7a-4875-884b-ea9a370cae09 does not exist. Creating ...
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/71a7bebb-be7a-4875-884b-ea9a370cae09/in_use.lock acquired by nodename 21@3fb36f21e5e7
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/71a7bebb-be7a-4875-884b-ea9a370cae09 has been successfully formatted.
datanode_1  | 2020-04-01 13:00:41 INFO  ContainerStateMachine:233 - group-EA9A370CAE09: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_worker.42eecb8e-afca-4afa-8236-1d066831b530
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:176 - new 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/71a7bebb-be7a-4875-884b-ea9a370cae09
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:129 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-tools-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1615)
om2_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | Enabled profiling in kernel
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om3_1       | 2020-04-01 13:00:21 INFO  OzoneManagerStarter:51 - STARTUP_MSG: 
datanode_2  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om2_1       | STARTUP_MSG:   java = 11.0.3
datanode_3  | 2020-04-01 13:00:17 INFO  HddsDatanodeService:51 - STARTUP_MSG: 
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
om3_1       | /************************************************************
datanode_2  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:940)
om2_1       | ************************************************************/
datanode_3  | /************************************************************
om3_1       | STARTUP_MSG: Starting OzoneManager
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:774)
om2_1       | 2020-04-01 13:00:20 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
om3_1       | STARTUP_MSG:   host = 493ad57775d3/172.21.0.4
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:885)
om2_1       | 2020-04-01 13:00:25 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1       | STARTUP_MSG:   args = [--init]
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
om2_1       | 2020-04-01 13:00:25 INFO  OMHANodeDetails:182 - Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
datanode_3  | STARTUP_MSG:   host = ad5f8f82f10c/172.21.0.8
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
om3_1       | STARTUP_MSG:   version = 3.2.0
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_3  | STARTUP_MSG:   args = []
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:183 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: start as a follower, conf=-1: [42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
datanode_2  | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.7:55858 remote=scm/172.21.0.5:9861]
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
om2_1       | 2020-04-01 13:00:25 INFO  OMHANodeDetails:315 - Setting configuration key ozone.om.address with value of key ozone.om.address.om2: om2
om3_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-tools-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   version = 3.2.0
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
om2_1       | 2020-04-01 13:00:25 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1  | 2020-04-01 13:00:41 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start FollowerState
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jline-0.9.94.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/curator-framework-2.12.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/curator-recipes-2.12.0.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/curator-client-2.12.0.jar:/opt/hadoop/share/ozone/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-a320ae0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
om2_1       | 2020-04-01 13:00:28 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | STARTUP_MSG:   java = 11.0.3
datanode_1  | 2020-04-01 13:00:41 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EA9A370CAE09,id=42eecb8e-afca-4afa-8236-1d066831b530
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm_1       | 2020-04-01 13:00:36 WARN  Server:1523 - IPC Server handler 2 on 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.21.0.6:58534: output error
om2_1       | 2020-04-01 13:00:29 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | ************************************************************/
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09
datanode_3  | STARTUP_MSG:   java = 11.0.3
datanode_2  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
scm_1       | 2020-04-01 13:00:36 INFO  Server:2695 - IPC Server handler 2 on 9861 caught an exception
om2_1       | 2020-04-01 13:00:30 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:21 INFO  OzoneManagerStarter:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2020-04-01 13:00:41 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS ONE #id: "71a7bebb-be7a-4875-884b-ea9a370cae09"
datanode_3  | ************************************************************/
datanode_2  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm_1       | java.nio.channels.AsynchronousCloseException
om2_1       | 2020-04-01 13:00:31 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:27 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
datanode_1  | .
datanode_3  | 2020-04-01 13:00:17 INFO  HddsDatanodeService:51 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm_1       | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
om2_1       | 2020-04-01 13:00:32 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:27 INFO  OMHANodeDetails:182 - Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerProxy:89 - 42eecb8e-afca-4afa-8236-1d066831b530: addNew group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] returns group-D42469200153:java.util.concurrent.CompletableFuture@67a6e866[Not completed]
datanode_3  | 2020-04-01 13:00:18 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm_1       | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3250)
om2_1       | 2020-04-01 13:00:33 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:27 INFO  OMHANodeDetails:315 - Setting configuration key ozone.om.address with value of key ozone.om.address.om3: om3
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:97 - 42eecb8e-afca-4afa-8236-1d066831b530: new RaftServerImpl for group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-01 13:00:19 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:557)
scm_1       | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:137)
om2_1       | 2020-04-01 13:00:34 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:27 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-01 13:00:21 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1473)
om2_1       | 2020-04-01 13:00:35 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om3_1       | 2020-04-01 13:00:29 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-01 13:00:21 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1816)
scm_1       | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1543)
om2_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2df71362-8533-41d2-b5ae-3773a97c69f8
om3_1       | 2020-04-01 13:00:30 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-01 13:00:22 INFO  HddsDatanodeService:204 - HddsDatanodeService host:ad5f8f82f10c ip:172.21.0.8
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2593)
om2_1       | 2020-04-01 13:00:36 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
om3_1       | 2020-04-01 13:00:31 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-01 13:00:22 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)
scm_1       | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1615)
om2_1       | /************************************************************
om3_1       | 2020-04-01 13:00:32 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-01 13:00:22 INFO  HddsVolume:177 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_2  | 2020-04-01 13:00:37 INFO  OzoneContainer:231 - Attempting to start container services.
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:940)
om2_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 70a283850dba/172.21.0.3
om3_1       | 2020-04-01 13:00:33 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:103 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: ConfigurationManager, init=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-01 13:00:22 INFO  MutableVolumeSet:188 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2020-04-01 13:00:37 INFO  OzoneContainer:195 - Background container scanner has been disabled.
scm_1       | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:774)
datanode_3  | 2020-04-01 13:00:23 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:23 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:885)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-01 13:00:37 INFO  XceiverServerRatis:406 - Starting XceiverServerRatis a11acf3f-11a5-49c6-b00f-84852dd9f928 at port 9858
datanode_3  | 2020-04-01 13:00:28 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1       | ************************************************************/
om3_1       | 2020-04-01 13:00:34 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-01 13:00:37 INFO  RaftServerProxy:299 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start RPC server
datanode_3  | 2020-04-01 13:00:29 INFO  RaftServerProxy:43 - raft.rpc.type = GRPC (default)
om2_1       | Enabled profiling in kernel
om3_1       | 2020-04-01 13:00:35 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 does not exist. Creating ...
datanode_2  | 2020-04-01 13:00:37 INFO  GrpcService:158 - a11acf3f-11a5-49c6-b00f-84852dd9f928: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3  | 2020-04-01 13:00:29 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.port = 9858 (custom)
om3_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2df71362-8533-41d2-b5ae-3773a97c69f8
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/in_use.lock acquired by nodename 21@3fb36f21e5e7
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerProxy:89 - a11acf3f-11a5-49c6-b00f-84852dd9f928: addNew group-7A6CA804F5E2:[a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858] returns group-7A6CA804F5E2:java.util.concurrent.CompletableFuture@189240aa[Not completed]
datanode_3  | 2020-04-01 13:00:29 INFO  GrpcService:43 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
om3_1       | 2020-04-01 13:00:36 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG: 
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 has been successfully formatted.
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerImpl:97 - a11acf3f-11a5-49c6-b00f-84852dd9f928: new RaftServerImpl for group-7A6CA804F5E2:[a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-01 13:00:29 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1       | /************************************************************
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
datanode_1  | 2020-04-01 13:00:41 INFO  ContainerStateMachine:233 - group-D42469200153: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-01 13:00:29 INFO  GrpcService:43 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
om3_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 493ad57775d3/172.21.0.4
scm_1       | 2020-04-01 13:00:37 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@b52b755{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-0_6_0-SNAPSHOT_jar-_-any-15030314013220017124.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/scm}
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-01 13:00:29 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 60s (custom)
om3_1       | ************************************************************/
scm_1       | 2020-04-01 13:00:37 INFO  AbstractConnector:330 - Started ServerConnector@6fd5717c{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-01 13:00:30 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1       | Enabled profiling in kernel
scm_1       | 2020-04-01 13:00:37 INFO  Server:399 - Started @7123ms
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-01 13:00:30 INFO  BaseHttpServer:170 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
scm_1       | 2020-04-01 13:00:37 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-01 13:00:30 INFO  log:169 - Logging initialized @19048ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1       | 2020-04-01 13:00:37 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerImpl:103 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: ConfigurationManager, init=-1: [a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-01 13:00:31 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2020-04-01 13:00:37 INFO  BaseHttpServer:284 - HTTP server of scm listening at http://0.0.0.0:9876
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-01 13:00:31 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:176 - new 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
scm_1       | 2020-04-01 13:00:37 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3  | 2020-04-01 13:00:31 INFO  HttpServer2:970 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 does not exist. Creating ...
scm_1       | 2020-04-01 13:00:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/42eecb8e-afca-4afa-8236-1d066831b530
datanode_3  | 2020-04-01 13:00:31 INFO  HttpServer2:946 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2/in_use.lock acquired by nodename 21@ad68cea49847
scm_1       | 2020-04-01 13:00:38 INFO  SCMNodeManager:268 - Registered Data node : 42eecb8e-afca-4afa-8236-1d066831b530{ip: 172.21.0.6, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2020-04-01 13:00:31 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 has been successfully formatted.
scm_1       | 2020-04-01 13:00:38 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=71a7bebb-be7a-4875-884b-ea9a370cae09 to datanode:42eecb8e-afca-4afa-8236-1d066831b530
datanode_3  | 2020-04-01 13:00:31 INFO  HttpServer2:954 - Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2020-04-01 13:00:38 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 71a7bebb-be7a-4875-884b-ea9a370cae09, Nodes: 42eecb8e-afca-4afa-8236-1d066831b530{ip: 172.21.0.6, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-01T13:00:38.187528Z]
datanode_2  | 2020-04-01 13:00:42 INFO  ContainerStateMachine:233 - group-7A6CA804F5E2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-01 13:00:31 WARN  BaseHttpServer:123 - /prof java profiling servlet is activated. Not safe for production!
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
scm_1       | 2020-04-01 13:00:38 INFO  SCMSafeModeManager:175 - ContainerSafeModeRule rule is successfully validated
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-01 13:00:31 INFO  HttpServer2:1188 - Jetty bound to port 9882
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
scm_1       | 2020-04-01 13:00:38 WARN  PipelinePlacementPolicy:150 - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1.
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-01 13:00:31 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
scm_1       | 2020-04-01 13:00:38 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 1 required.
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-01 13:00:31 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-01 13:00:31 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       | 2020-04-01 13:00:38 INFO  SCMSafeModeManager:175 - DataNodeSafeModeRule rule is successfully validated
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-01 13:00:31 INFO  session:140 - node0 Scavenging every 660000ms
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2020-04-01 13:00:38 INFO  NetworkTopology:111 - Added a new node: /default-rack/47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_2  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_worker.a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 2020-04-01 13:00:31 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@51d719bc{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:129 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       | 2020-04-01 13:00:38 INFO  SCMNodeManager:268 - Registered Data node : 47e83d87-bee9-4b63-9030-2e7b8e2ea910{ip: 172.21.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-01 13:00:31 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@4d266391{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | 2020-04-01 13:00:38 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=9f1cfc58-347b-4fbf-b931-b86dfdbb2c22 to datanode:47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_2  | 2020-04-01 13:00:42 INFO  SegmentedRaftLogWorker:176 - new a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2
datanode_3  | 2020-04-01 13:00:32 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@59c70ceb{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-16538801831489894571.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       | 2020-04-01 13:00:38 INFO  SCMSafeModeManager:175 - ContainerSafeModeRule rule is successfully validated
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-01 13:00:32 INFO  AbstractConnector:330 - Started ServerConnector@49c17ba4{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 2020-04-01 13:00:38 INFO  SCMSafeModeManager:175 - DataNodeSafeModeRule rule is successfully validated
datanode_3  | 2020-04-01 13:00:32 INFO  Server:399 - Started @20283ms
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153
scm_1       | 2020-04-01 13:00:38 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 9f1cfc58-347b-4fbf-b931-b86dfdbb2c22, Nodes: 47e83d87-bee9-4b63-9030-2e7b8e2ea910{ip: 172.21.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-01T13:00:38.353686Z]
datanode_3  | 2020-04-01 13:00:32 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153
scm_1       | 2020-04-01 13:00:38 WARN  PipelinePlacementPolicy:150 - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.
datanode_3  | 2020-04-01 13:00:32 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:183 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: start as a follower, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 2020-04-01 13:00:39 INFO  NetworkTopology:111 - Added a new node: /default-rack/a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 2020-04-01 13:00:32 INFO  BaseHttpServer:284 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 2020-04-01 13:00:39 INFO  SCMNodeManager:268 - Registered Data node : a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}
datanode_3  | 2020-04-01 13:00:32 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start FollowerState
scm_1       | 2020-04-01 13:00:39 INFO  SCMSafeModeManager:175 - ContainerSafeModeRule rule is successfully validated
datanode_3  | 2020-04-01 13:00:32 INFO  InitDatanodeState:147 - DatanodeDetails is persisted to /data/datanode.id
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-01 13:00:41 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D42469200153,id=42eecb8e-afca-4afa-8236-1d066831b530
scm_1       | 2020-04-01 13:00:39 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 to datanode:a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 2020-04-01 13:00:35 INFO  Client:948 - Retrying connect to server: scm/172.21.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153
scm_1       | 2020-04-01 13:00:39 INFO  SCMSafeModeManager:175 - DataNodeSafeModeRule rule is successfully validated
datanode_3  | 2020-04-01 13:00:36 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2020-04-01 13:00:39 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_3  | java.net.SocketTimeoutException: Call From ad5f8f82f10c/172.21.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.8:53870 remote=scm/172.21.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1  | 2020-04-01 13:00:43 WARN  RaftServerProxy:390 - 42eecb8e-afca-4afa-8236-1d066831b530: Failed groupAdd* GroupManagementRequest:client-5DA1493D1E89->42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153, cid=0, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
datanode_2  | 2020-04-01 13:00:42 INFO  SegmentedRaftLogWorker:129 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 42eecb8e-afca-4afa-8236-1d066831b530: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 2020-04-01 13:00:39 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=d403079b-76d6-4ae3-b1b6-d42469200153 to datanode:42eecb8e-afca-4afa-8236-1d066831b530
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm_1       | 2020-04-01 13:00:39 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=d403079b-76d6-4ae3-b1b6-d42469200153 to datanode:a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
scm_1       | 2020-04-01 13:00:39 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=d403079b-76d6-4ae3-b1b6-d42469200153 to datanode:47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
scm_1       | 2020-04-01 13:00:39 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: d403079b-76d6-4ae3-b1b6-d42469200153, Nodes: 42eecb8e-afca-4afa-8236-1d066831b530{ip: 172.21.0.6, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}47e83d87-bee9-4b63-9030-2e7b8e2ea910{ip: 172.21.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-01T13:00:39.083506Z]
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
scm_1       | 2020-04-01 13:00:41 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 71a7bebb-be7a-4875-884b-ea9a370cae09, Nodes: 42eecb8e-afca-4afa-8236-1d066831b530{ip: 172.21.0.6, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:42eecb8e-afca-4afa-8236-1d066831b530, CreationTimestamp2020-04-01T13:00:38.187528Z] moved to OPEN state
datanode_2  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2
datanode_3  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
scm_1       | 2020-04-01 13:00:41 INFO  SCMSafeModeManager:175 - AtleastOneDatanodeReportedRule rule is successfully validated
datanode_2  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2
datanode_3  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
scm_1       | 2020-04-01 13:00:41 INFO  SCMSafeModeManager:175 - HealthyPipelineSafeModeRule rule is successfully validated
datanode_3  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerImpl:183 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: start as a follower, conf=-1: [a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858], old=null
scm_1       | 2020-04-01 13:00:41 INFO  SCMSafeModeManager:184 - ScmSafeModeManager, all rules are successfully validated
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 2020-04-01 13:00:42 INFO  RaftServerImpl:172 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-01 13:00:42 INFO  RoleInfo:143 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start FollowerState
datanode_3  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 2020-04-01 13:00:42 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7A6CA804F5E2,id=a11acf3f-11a5-49c6-b00f-84852dd9f928
scm_1       | 2020-04-01 13:00:41 INFO  SCMSafeModeManager:200 - SCM exiting safe mode.
datanode_3  | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2
scm_1       | 2020-04-01 13:00:41 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 9f1cfc58-347b-4fbf-b931-b86dfdbb2c22, Nodes: 47e83d87-bee9-4b63-9030-2e7b8e2ea910{ip: 172.21.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:47e83d87-bee9-4b63-9030-2e7b8e2ea910, CreationTimestamp2020-04-01T13:00:38.353686Z] moved to OPEN state
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
scm_1       | 2020-04-01 13:00:42 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] moved to OPEN state
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 2020-04-01 13:00:43 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS ONE #id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_3  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
scm_1       | 2020-04-01 13:00:47 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: d403079b-76d6-4ae3-b1b6-d42469200153, Nodes: 42eecb8e-afca-4afa-8236-1d066831b530{ip: 172.21.0.6, host: ozone-om-ha_datanode_1.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}47e83d87-bee9-4b63-9030-2e7b8e2ea910{ip: 172.21.0.8, host: ozone-om-ha_datanode_3.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:42eecb8e-afca-4afa-8236-1d066831b530, CreationTimestamp2020-04-01T13:00:39.083506Z] moved to OPEN state
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | .
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
scm_1       | 2020-04-01 13:04:00 ERROR PipelineActionHandler:60 - Received pipeline action CLOSE for Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] from datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}. Reason : ContainerID 9 creation failed
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerProxy:89 - a11acf3f-11a5-49c6-b00f-84852dd9f928: addNew group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] returns group-D42469200153:java.util.concurrent.CompletableFuture@67a6e866[Not completed]
datanode_3  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
scm_1       | 2020-04-01 13:04:00 INFO  SCMPipelineManager:391 - Destroying pipeline:Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerImpl:97 - a11acf3f-11a5-49c6-b00f-84852dd9f928: new RaftServerImpl for group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 2020-04-01 13:04:00 INFO  PipelineStateManager:120 - Pipeline Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] moved to CLOSED state
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
scm_1       | 2020-04-01 13:04:00 INFO  CloseContainerEventHandler:61 - Close container Event triggered for container : #1
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1       | 2020-04-01 13:04:00 INFO  CloseContainerEventHandler:61 - Close container Event triggered for container : #5
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 42eecb8e-afca-4afa-8236-1d066831b530: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerImpl:103 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: ConfigurationManager, init=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 2020-04-01 13:04:00 INFO  CloseContainerEventHandler:61 - Close container Event triggered for container : #6
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 2020-04-01 13:04:00 INFO  CloseContainerEventHandler:61 - Close container Event triggered for container : #7
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-01 13:04:00 INFO  CloseContainerEventHandler:61 - Close container Event triggered for container : #9
datanode_1  | 	... 13 more
datanode_2  | 2020-04-01 13:00:43 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 does not exist. Creating ...
datanode_3  | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.21.0.8:53870 remote=scm/172.21.0.5:9861]
scm_1       | 2020-04-01 13:04:00 ERROR PipelineActionHandler:60 - Received pipeline action CLOSE for Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] from datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}. Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_1  | 2020-04-01 13:00:44 WARN  RaftServerProxy:390 - 42eecb8e-afca-4afa-8236-1d066831b530: Failed groupAdd* GroupManagementRequest:client-3CA02E6CC6A6->42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153, cid=0, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
datanode_2  | 2020-04-01 13:00:43 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/in_use.lock acquired by nodename 21@ad68cea49847
datanode_3  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 42eecb8e-afca-4afa-8236-1d066831b530: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
datanode_2  | 2020-04-01 13:00:43 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 has been successfully formatted.
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm_1       | 2020-04-01 13:04:00 INFO  SCMPipelineManager:391 - Destroying pipeline:Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 2020-04-01 13:00:43 INFO  ContainerStateMachine:233 - group-D42469200153: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm_1       | 2020-04-01 13:04:08 ERROR PipelineActionHandler:60 - Received pipeline action CLOSE for Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] from datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}. Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
scm_1       | 2020-04-01 13:04:08 INFO  SCMPipelineManager:391 - Destroying pipeline:Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_3  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
scm_1       | 2020-04-01 13:04:34 INFO  RatisPipelineProvider:170 - Sending CreatePipelineCommand for pipeline:PipelineID=de5fe4b5-a09e-4e05-80c5-481ae7b53f03 to datanode:a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1       | 2020-04-01 13:04:34 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: de5fe4b5-a09e-4e05-80c5-481ae7b53f03, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-01T13:04:34.456969Z]
datanode_3  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2020-04-01 13:05:06 INFO  RatisPipelineProvider:208 - Send pipeline:PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 close command to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:557)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 2020-04-01 13:05:06 INFO  PipelineStateManager:105 - Pipeline Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z] removed from db
datanode_3  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 2020-04-01 13:00:43 INFO  SegmentedRaftLogWorker:176 - new a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153
scm_1       | 2020-04-01 13:05:06 INFO  RatisPipelineProvider:208 - Send pipeline:PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 close command to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1816)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
scm_1       | 2020-04-01 13:05:06 ERROR SCMPipelineManager:72 - Destroy pipeline failed for pipeline:Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_3  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-01 13:00:36 INFO  OzoneContainer:231 - Attempting to start container services.
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-01 13:00:36 INFO  OzoneContainer:195 - Background container scanner has been disabled.
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-01 13:00:36 INFO  XceiverServerRatis:406 - Starting XceiverServerRatis 47e83d87-bee9-4b63-9030-2e7b8e2ea910 at port 9858
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-01 13:00:36 INFO  RaftServerProxy:299 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start RPC server
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:571)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-01 13:00:36 INFO  GrpcService:158 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:555)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerProxy:89 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: addNew group-B86DFDBB2C22:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858] returns group-B86DFDBB2C22:java.util.concurrent.CompletableFuture@47fdccdc[Not completed]
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 42eecb8e-afca-4afa-8236-1d066831b530: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:398)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:97 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: new RaftServerImpl for group-B86DFDBB2C22:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858] with ContainerStateMachine:uninitialized
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
datanode_2  | 2020-04-01 13:00:43 INFO  SegmentedRaftLogWorker:129 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 	... 13 more
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-01 13:00:44 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS THREE #id: "d403079b-76d6-4ae3-b1b6-d42469200153"
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | .
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2020-04-01 13:00:43 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-01 13:00:47 INFO  FollowerState:108 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-FollowerState: change to CANDIDATE, lastRpcTime:5214ms, electionTimeout:5176ms
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 2020-04-01 13:00:43 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:121 - 42eecb8e-afca-4afa-8236-1d066831b530: shutdown FollowerState
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerImpl:183 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: start as a follower, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:103 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: ConfigurationManager, init=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2020-04-01 13:05:14 INFO  RatisPipelineProvider:208 - Send pipeline:PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 close command to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_2  | 2020-04-01 13:00:43 INFO  RaftServerImpl:172 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start LeaderElection
scm_1       | 2020-04-01 13:05:14 ERROR SCMPipelineManager:72 - Destroy pipeline failed for pipeline:Pipeline[ Id: f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2, Nodes: a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:a11acf3f-11a5-49c6-b00f-84852dd9f928, CreationTimestamp2020-04-01T13:00:39.076596Z]
datanode_2  | 2020-04-01 13:00:43 INFO  RoleInfo:143 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start FollowerState
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-01 13:00:47 INFO  FollowerState:108 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-FollowerState: change to CANDIDATE, lastRpcTime:5103ms, electionTimeout:5078ms
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_2  | 2020-04-01 13:00:43 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D42469200153,id=a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/9f1cfc58-347b-4fbf-b931-b86dfdbb2c22 does not exist. Creating ...
datanode_1  | 2020-04-01 13:00:47 INFO  LeaderElection:206 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-LeaderElection1: begin an election at term 1 for -1: [42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_2  | 2020-04-01 13:00:43 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/9f1cfc58-347b-4fbf-b931-b86dfdbb2c22/in_use.lock acquired by nodename 22@ad5f8f82f10c
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:121 - 42eecb8e-afca-4afa-8236-1d066831b530: shutdown FollowerState
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
datanode_2  | 2020-04-01 13:00:44 WARN  RaftServerProxy:390 - a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed groupAdd* GroupManagementRequest:client-EDA35A651706->a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153, cid=1, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/9f1cfc58-347b-4fbf-b931-b86dfdbb2c22 has been successfully formatted.
datanode_3  | 2020-04-01 13:00:41 INFO  ContainerStateMachine:233 - group-B86DFDBB2C22: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:571)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start LeaderElection
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:555)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:134 - 42eecb8e-afca-4afa-8236-1d066831b530: shutdown LeaderElection
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:398)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1  | 2020-04-01 13:00:47 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-EA9A370CAE09 with new leaderId: 42eecb8e-afca-4afa-8236-1d066831b530
datanode_3  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_worker.47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:255 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: change Leader from null to 42eecb8e-afca-4afa-8236-1d066831b530 at term 1 for becomeLeader, leader elected after 5663ms
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
datanode_1  | 2020-04-01 13:00:47 INFO  LeaderElection:206 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-LeaderElection2: begin an election at term 1 for -1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
datanode_3  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:176 - new 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/9f1cfc58-347b-4fbf-b931-b86dfdbb2c22
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 2020-04-01 13:00:47 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_appender.42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:165 - Starting Replication Monitor Thread.
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.byte-limit = 1073741824 (custom)
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 180s (custom)
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start LeaderState
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 2020-04-01 13:00:47 INFO  LeaderElection:61 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-LeaderElection2: Election PASSED; received 1 response(s) [42eecb8e-afca-4afa-8236-1d066831b530<-a11acf3f-11a5-49c6-b00f-84852dd9f928#0:OK-t1] and 0 exception(s); 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153:t1, leader=null, voted=42eecb8e-afca-4afa-8236-1d066831b530, raftlog=42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 2020-04-01 13:05:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 6 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:134 - 42eecb8e-afca-4afa-8236-1d066831b530: shutdown LeaderElection
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm_1       | 2020-04-01 13:10:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 2020-04-01 13:00:47 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-D42469200153 with new leaderId: 42eecb8e-afca-4afa-8236-1d066831b530
scm_1       | 2020-04-01 13:10:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 2020-04-01 13:00:41 INFO  SegmentedRaftLogWorker:129 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:255 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: change Leader from null to 42eecb8e-afca-4afa-8236-1d066831b530 at term 1 for becomeLeader, leader elected after 5447ms
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
scm_1       | 2020-04-01 13:10:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
scm_1       | 2020-04-01 13:10:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 	... 13 more
scm_1       | 2020-04-01 13:10:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 2 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22
datanode_1  | 2020-04-01 13:00:47 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_appender.42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153
datanode_2  | 2020-04-01 13:00:44 WARN  RaftServerProxy:390 - a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed groupAdd* GroupManagementRequest:client-90A5A752BEA7->a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153, cid=0, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
scm_1       | 2020-04-01 13:14:30 ERROR PipelineActionHandler:66 - Could not execute pipeline action=action: CLOSE
datanode_3  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 1024 (custom)
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | closePipeline {
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:183 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: start as a follower, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858], old=null
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm_1       |   pipelineID {
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 180s (custom)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
scm_1       |     id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_3  | 2020-04-01 13:00:41 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start FollowerState
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
scm_1       |   }
datanode_3  | 2020-04-01 13:00:41 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B86DFDBB2C22,id=47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
scm_1       |   reason: PIPELINE_LOG_FAILED
datanode_3  | 2020-04-01 13:00:41 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
scm_1       |   detailedReason: "Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1\n\t State Machine: cmdType: WriteChunk traceID: \"4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0\" containerID: 9 datanodeUuid: \"a11acf3f-11a5-49c6-b00f-84852dd9f928\" pipelineID: \"f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2\" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: \"103923465509863432_chunk_1\" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: \" wi\\267\" } } }, container path=nonexistent"
datanode_3  | 2020-04-01 13:00:41 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS ONE #id: "9f1cfc58-347b-4fbf-b931-b86dfdbb2c22"
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
scm_1       | }
datanode_3  | .
datanode_1  | 2020-04-01 13:00:47 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
scm_1       |  pipeline=PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 {}
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerProxy:89 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: addNew group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] returns group-D42469200153:java.util.concurrent.CompletableFuture@36b20c98[Not completed]
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:97 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: new RaftServerImpl for group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:243)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:59)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:35)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 2020-04-01 13:00:47 INFO  GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm_1       | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 60s (custom)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerImpl:103 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: ConfigurationManager, init=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:391 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-01 13:00:41 INFO  RaftServerConfigKeys:43 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: a11acf3f-11a5-49c6-b00f-84852dd9f928: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:253 - The storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 does not exist. Creating ...
datanode_1  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 42eecb8e-afca-4afa-8236-1d066831b530: start LeaderState
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorageDirectory:335 - Lock on /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/in_use.lock acquired by nodename 22@ad5f8f82f10c
datanode_1  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:391 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_3  | 2020-04-01 13:00:41 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153 has been successfully formatted.
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:356 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09: set configuration 0: [42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null at 0
datanode_2  | 2020-04-01 13:00:44 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS THREE #id: "d403079b-76d6-4ae3-b1b6-d42469200153"
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:41 INFO  ContainerStateMachine:233 - group-D42469200153: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:356 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153: set configuration 0: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null at 0
datanode_2  | .
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-01 13:00:47 INFO  RaftServerImpl:806 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-   LEADER: Withhold vote from candidate 47e83d87-bee9-4b63-9030-2e7b8e2ea910 with term 1. State: leader=42eecb8e-afca-4afa-8236-1d066831b530, term=1, lastRpcElapsed=null
datanode_2  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:42eecb8e-afca-4afa-8236-1d066831b530
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:583 - 42eecb8e-afca-4afa-8236-1d066831b530@group-D42469200153-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/current/log_inprogress_0
datanode_2  | 2020-04-01 13:00:47 INFO  RoleInfo:121 - a11acf3f-11a5-49c6-b00f-84852dd9f928: shutdown FollowerState
scm_1       | 2020-04-01 13:15:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:583 - 42eecb8e-afca-4afa-8236-1d066831b530@group-EA9A370CAE09-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/71a7bebb-be7a-4875-884b-ea9a370cae09/current/log_inprogress_0
datanode_2  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start FollowerState
datanode_2  | 2020-04-01 13:00:47 INFO  FollowerState:117 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-01 13:00:47 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-D42469200153 with new leaderId: 42eecb8e-afca-4afa-8236-1d066831b530
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_1  | 2020-04-01 13:30:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-01 13:00:47 INFO  RaftServerImpl:255 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: change Leader from null to 42eecb8e-afca-4afa-8236-1d066831b530 at term 1 for appendEntries, leader elected after 4443ms
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_1  | 2020-04-01 13:30:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-01 13:00:47 INFO  RaftServerImpl:356 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153: set configuration 0: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null at 0
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_1  | 2020-04-01 13:45:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  SegmentedRaftLogWorker:176 - new 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153
datanode_2  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:391 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_1  | 2020-04-01 13:45:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.byte-limit = 2147483647 (custom)
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:00:48 INFO  FollowerState:108 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-FollowerState: change to CANDIDATE, lastRpcTime:5150ms, electionTimeout:5095ms
datanode_1  | 2020-04-01 14:00:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.queue.element-limit = 1024 (custom)
scm_1       | 2020-04-01 13:20:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 2 milliseconds for processing 9 containers.
datanode_2  | 2020-04-01 13:00:48 INFO  RoleInfo:121 - a11acf3f-11a5-49c6-b00f-84852dd9f928: shutdown FollowerState
datanode_1  | 2020-04-01 14:00:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.segment.size.max = 1048576 (custom)
scm_1       | 2020-04-01 13:25:01 ERROR PipelineActionHandler:66 - Could not execute pipeline action=action: CLOSE
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerImpl:172 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-01 13:00:48 INFO  RoleInfo:143 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start LeaderElection
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.force.sync.num = 128 (default)
scm_1       | closePipeline {
datanode_2  | 2020-04-01 13:00:48 INFO  SegmentedRaftLogWorker:583 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-D42469200153-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/current/log_inprogress_0
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync = true (default)
scm_1       |   pipelineID {
datanode_2  | 2020-04-01 13:00:48 INFO  LeaderElection:206 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-LeaderElection1: begin an election at term 1 for -1: [a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858], old=null
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1       |     id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_2  | 2020-04-01 13:00:48 INFO  RoleInfo:134 - a11acf3f-11a5-49c6-b00f-84852dd9f928: shutdown LeaderElection
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1       |   }
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerImpl:172 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       |   reason: PIPELINE_LOG_FAILED
datanode_2  | 2020-04-01 13:00:48 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-7A6CA804F5E2 with new leaderId: a11acf3f-11a5-49c6-b00f-84852dd9f928
datanode_3  | 2020-04-01 13:00:42 INFO  SegmentedRaftLogWorker:129 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1       |   detailedReason: "Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1\n\t State Machine: cmdType: WriteChunk traceID: \"4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0\" containerID: 9 datanodeUuid: \"a11acf3f-11a5-49c6-b00f-84852dd9f928\" pipelineID: \"f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2\" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: \"103923465509863432_chunk_1\" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: \" wi\\267\" } } }, container path=nonexistent"
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerImpl:255 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: change Leader from null to a11acf3f-11a5-49c6-b00f-84852dd9f928 at term 1 for becomeLeader, leader elected after 5773ms
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1       | }
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       |  pipeline=PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 {}
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.snapshot.retention.file.num = 5 (custom)
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_2  | 2020-04-01 13:00:48 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_appender.a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerConfigKeys:43 - raft.server.retrycache.expirytime = 600000ms (custom)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.leader_election.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.server.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:243)
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 180s (custom)
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerImpl:183 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: start as a follower, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:59)
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2020-04-01 13:00:42 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:35)
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2020-04-01 13:00:42 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start FollowerState
scm_1       | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
datanode_2  | 2020-04-01 13:00:48 INFO  RoleInfo:143 - a11acf3f-11a5-49c6-b00f-84852dd9f928: start LeaderState
datanode_3  | 2020-04-01 13:00:42 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D42469200153,id=47e83d87-bee9-4b63-9030-2e7b8e2ea910
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 2020-04-01 13:00:48 INFO  SegmentedRaftLogWorker:391 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-01 13:00:42 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.state_machine.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 2020-04-01 13:00:48 INFO  SegmentedRaftLogWorker:583 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2/current/log_inprogress_0
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-01 13:00:44 INFO  CreatePipelineCommandHandler:112 - Created Pipeline RATIS THREE #id: "d403079b-76d6-4ae3-b1b6-d42469200153"
datanode_2  | 2020-04-01 13:00:48 INFO  RaftServerImpl:356 - a11acf3f-11a5-49c6-b00f-84852dd9f928@group-7A6CA804F5E2: set configuration 0: [a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858], old=null at 0
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | .
datanode_3  | 2020-04-01 13:00:44 WARN  RaftServerProxy:390 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed groupAdd* GroupManagementRequest:client-CEE2D609EA70->47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153, cid=1, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:04:00 INFO  KeyValueHandler:76 - Operation: CreateContainer , Trace ID: 4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:243)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
scm_1       | 2020-04-01 13:25:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:161)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:411)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:247)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
scm_1       | 2020-04-01 13:30:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
scm_1       | 2020-04-01 13:35:31 ERROR PipelineActionHandler:66 - Could not execute pipeline action=action: CLOSE
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
scm_1       | closePipeline {
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
scm_1       |   pipelineID {
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
scm_1       |     id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm_1       |   }
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=1050821136 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
scm_1       |   reason: PIPELINE_LOG_FAILED
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       |   detailedReason: "Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1\n\t State Machine: cmdType: WriteChunk traceID: \"4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0\" containerID: 9 datanodeUuid: \"a11acf3f-11a5-49c6-b00f-84852dd9f928\" pipelineID: \"f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2\" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: \"103923465509863432_chunk_1\" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: \" wi\\267\" } } }, container path=nonexistent"
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | }
datanode_2  | 	... 12 more
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       |  pipeline=PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 {}
datanode_2  | 2020-04-01 13:04:00 INFO  HddsDispatcher:76 - Operation: WriteChunk , Trace ID: 4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0 , Message: ContainerID 9 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 9 creation failed
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:251)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	... 13 more
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:243)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 2020-04-01 13:00:44 WARN  RaftServerProxy:390 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed groupAdd* GroupManagementRequest:client-289443363545->47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153, cid=1, seq=0, RW, null, Add:group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858]
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:59)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:35)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
scm_1       | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:04:00 ERROR ContainerStateMachine:475 - group-7A6CA804F5E2: writeChunk writeStateMachineData failed: blockIdcontainerID: 9
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | localID: 103923465509863432
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | blockCommitSequenceId: 0
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  |  logIndex 13 chunkName 103923465509863432_chunk_1 Error message: ContainerID 9 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:04:00 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : ContainerID 9 creation failed
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
scm_1       | 2020-04-01 13:35:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 2 milliseconds for processing 9 containers.
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 2020-04-01 13:04:00 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 2020-04-01 13:04:08 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:14:30 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-01 13:40:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_2  | 2020-04-01 13:25:01 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 47e83d87-bee9-4b63-9030-2e7b8e2ea910: Failed to add group-D42469200153:[47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858] since the group already exists in the map.
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:25:01 WARN  ChunkUtils:224 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='103923465509863432_chunk_1, offset=0, len=17540}
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:30:24 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 	... 13 more
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_2  | 2020-04-01 13:30:24 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:46 INFO  FollowerState:108 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-FollowerState: change to CANDIDATE, lastRpcTime:5075ms, electionTimeout:5051ms
datanode_2  | 2020-04-01 13:35:31 WARN  ChunkUtils:224 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='103923465509863432_chunk_1, offset=0, len=17540}
scm_1       | 2020-04-01 13:45:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:46 INFO  RoleInfo:121 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: shutdown FollowerState
datanode_2  | 2020-04-01 13:35:31 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
scm_1       | 2020-04-01 13:46:01 ERROR PipelineActionHandler:66 - Could not execute pipeline action=action: CLOSE
datanode_3  | 2020-04-01 13:00:46 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
scm_1       | closePipeline {
datanode_3  | 2020-04-01 13:00:46 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start LeaderElection
datanode_2  | 2020-04-01 13:45:24 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       |   pipelineID {
datanode_3  | 2020-04-01 13:00:46 INFO  LeaderElection:206 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-LeaderElection1: begin an election at term 1 for -1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858], old=null
datanode_2  | 2020-04-01 13:45:24 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
scm_1       |     id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_3  | 2020-04-01 13:00:46 INFO  RoleInfo:134 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: shutdown LeaderElection
datanode_2  | 2020-04-01 13:46:01 WARN  ChunkUtils:224 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='103923465509863432_chunk_1, offset=0, len=17540}
scm_1       |   }
datanode_2  | 2020-04-01 13:46:01 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1       |   reason: PIPELINE_LOG_FAILED
scm_1       |   detailedReason: "Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1\n\t State Machine: cmdType: WriteChunk traceID: \"4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0\" containerID: 9 datanodeUuid: \"a11acf3f-11a5-49c6-b00f-84852dd9f928\" pipelineID: \"f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2\" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: \"103923465509863432_chunk_1\" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: \" wi\\267\" } } }, container path=nonexistent"
datanode_3  | 2020-04-01 13:00:47 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-B86DFDBB2C22 with new leaderId: 47e83d87-bee9-4b63-9030-2e7b8e2ea910
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:255 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: change Leader from null to 47e83d87-bee9-4b63-9030-2e7b8e2ea910 at term 1 for becomeLeader, leader elected after 5436ms
scm_1       | }
datanode_2  | 2020-04-01 13:56:31 ERROR XceiverServerRatis:573 - pipeline Action CLOSE on pipeline PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2.Reason : Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)
scm_1       |  pipeline=PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 {}
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0" containerID: 9 datanodeUuid: "a11acf3f-11a5-49c6-b00f-84852dd9f928" pipelineID: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: "103923465509863432_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } }, container path=nonexistent
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
datanode_2  | 2020-04-01 13:56:31 WARN  ChunkUtils:224 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='103923465509863432_chunk_1, offset=0, len=17540}
datanode_3  | 2020-04-01 13:00:47 INFO  RatisMetrics:39 - Creating Metrics Registry : ratis.log_appender.47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_2  | 2020-04-01 14:00:24 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.element-limit = 1024 (custom)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
datanode_2  | 2020-04-01 14:00:24 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.write.byte-limit = 1073741824 (custom)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:243)
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout = 180s (custom)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:59)
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:35)
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerConfigKeys:43 - raft.server.watch.element-limit = 65536 (default)
scm_1       | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
datanode_3  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start LeaderState
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:391 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  FollowerState:108 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-FollowerState: change to CANDIDATE, lastRpcTime:5183ms, electionTimeout:5175ms
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RoleInfo:121 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: shutdown FollowerState
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start LeaderElection
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:356 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22: set configuration 0: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858], old=null at 0
scm_1       | 2020-04-01 13:50:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 2 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:47 INFO  LeaderElection:206 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-LeaderElection2: begin an election at term 1 for -1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  LeaderElection:61 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-LeaderElection2: Election REJECTED; received 2 response(s) [47e83d87-bee9-4b63-9030-2e7b8e2ea910<-a11acf3f-11a5-49c6-b00f-84852dd9f928#0:FAIL-t1, 47e83d87-bee9-4b63-9030-2e7b8e2ea910<-42eecb8e-afca-4afa-8236-1d066831b530#0:FAIL-t1] and 0 exception(s); 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153:t1, leader=null, voted=47e83d87-bee9-4b63-9030-2e7b8e2ea910, raftlog=47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:172 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RoleInfo:134 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: shutdown LeaderElection
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  RoleInfo:143 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910: start FollowerState
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
datanode_3  | 2020-04-01 13:00:47 INFO  XceiverServerRatis:739 - Leader change notification received for group: group-D42469200153 with new leaderId: 42eecb8e-afca-4afa-8236-1d066831b530
scm_1       | 2020-04-01 13:55:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:255 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: change Leader from null to 42eecb8e-afca-4afa-8236-1d066831b530 at term 1 for appendEntries, leader elected after 5705ms
scm_1       | 2020-04-01 13:56:31 ERROR PipelineActionHandler:66 - Could not execute pipeline action=action: CLOSE
datanode_3  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:583 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-B86DFDBB2C22-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9f1cfc58-347b-4fbf-b931-b86dfdbb2c22/current/log_inprogress_0
scm_1       | closePipeline {
datanode_3  | 2020-04-01 13:00:47 INFO  RaftServerImpl:356 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153: set configuration 0: [47e83d87-bee9-4b63-9030-2e7b8e2ea910:172.21.0.8:9858, a11acf3f-11a5-49c6-b00f-84852dd9f928:172.21.0.7:9858, 42eecb8e-afca-4afa-8236-1d066831b530:172.21.0.6:9858], old=null at 0
scm_1       |   pipelineID {
datanode_3  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:391 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-SegmentedRaftLogWorker: Starting segment from index:0
scm_1       |     id: "f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2"
datanode_3  | 2020-04-01 13:00:47 INFO  SegmentedRaftLogWorker:583 - 47e83d87-bee9-4b63-9030-2e7b8e2ea910@group-D42469200153-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d403079b-76d6-4ae3-b1b6-d42469200153/current/log_inprogress_0
scm_1       |   }
datanode_3  | 2020-04-01 13:30:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
scm_1       |   reason: PIPELINE_LOG_FAILED
datanode_3  | 2020-04-01 13:30:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 13:45:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 13:45:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-01 14:00:22 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-01 14:00:22 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
scm_1       |   detailedReason: "Log already failed at index 13 for task WriteLog:13: (t:1, i:13), STATEMACHINELOGENTRY, client-D7B5A3B93528, cid=1\n\t State Machine: cmdType: WriteChunk traceID: \"4d7f19eb3ae89036:9bcaf73cd7fe1766:4d7f19eb3ae89036:0\" containerID: 9 datanodeUuid: \"a11acf3f-11a5-49c6-b00f-84852dd9f928\" pipelineID: \"f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2\" writeChunk { blockID { containerID: 9 localID: 103923465509863432 blockCommitSequenceId: 0 } chunkData { chunkName: \"103923465509863432_chunk_1\" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: \" wi\\267\" } } }, container path=nonexistent"
scm_1       | }
scm_1       |  pipeline=PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 {}
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=f6bc2a0d-97bd-4b6c-8c2c-7a6ca804f5e2 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:243)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:59)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineActionHandler.onMessage(PipelineActionHandler.java:35)
scm_1       | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:00:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:664 - Sending close container command for container #5 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:664 - Sending close container command for container #6 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:664 - Sending close container command for container #7 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:664 - Sending close container command for container #9 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:664 - Sending close container command for container #1 to datanode a11acf3f-11a5-49c6-b00f-84852dd9f928{ip: 172.21.0.7, host: ozone-om-ha_datanode_2.ozone-om-ha_default, networkLocation: /default-rack, certSerialId: null}.
scm_1       | 2020-04-01 14:05:41 INFO  ReplicationManager:228 - Replication Monitor Thread took 1 milliseconds for processing 9 containers.
