Attaching to ozonesecure_datanode_3, ozonesecure_kms_1, ozonesecure_s3g_1, ozonesecure_datanode_1, ozonesecure_recon_1, ozonesecure_datanode_2, ozonesecure_om_1, ozonesecure_kdc_1, ozonesecure_scm_1
datanode_1  | Sleeping for 5 seconds
datanode_1  | Setting up kerberos!!
datanode_1  | KDC ISSUER_SERVER => kdc:8081
datanode_1  | Sleeping for 5 seconds
datanode_1  | Got 200, KDC service ready!!
datanode_1  | Download dn/d43c26201e9a@EXAMPLE.COM keytab file to /etc/security/keytabs/dn.keytab
datanode_1  | --2020-04-17 19:12:29--  http://kdc:8081/keytab/d43c26201e9a/dn
datanode_1  | Resolving kdc (kdc)... 172.24.0.4
datanode_1  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_1  | HTTP request sent, awaiting response... 200 OK
datanode_1  | Length: 158 [application/octet-stream]
datanode_1  | Saving to: '/etc/security/keytabs/dn.keytab'
datanode_1  | 
datanode_1  |      0K                                                       100% 30.8M=0s
datanode_1  | 
datanode_1  | 2020-04-17 19:12:30 (30.8 MB/s) - '/etc/security/keytabs/dn.keytab' saved [158/158]
datanode_1  | 
datanode_1  | Keytab name: FILE:/etc/security/keytabs/dn.keytab
datanode_1  | KVNO Timestamp         Principal
datanode_1  | ---- ----------------- --------------------------------------------------------
datanode_1  |    2 04/17/20 19:12:30 dn/d43c26201e9a@EXAMPLE.COM
datanode_1  |    2 04/17/20 19:12:30 dn/d43c26201e9a@EXAMPLE.COM
datanode_1  | Download HTTP/d43c26201e9a@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
datanode_1  | --2020-04-17 19:12:30--  http://kdc:8081/keytab/d43c26201e9a/HTTP
datanode_1  | Resolving kdc (kdc)... 172.24.0.4
datanode_1  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_1  | HTTP request sent, awaiting response... 200 OK
datanode_1  | Length: 162 [application/octet-stream]
datanode_1  | Saving to: '/etc/security/keytabs/HTTP.keytab'
datanode_1  | 
datanode_1  |      0K                                                       100% 17.0M=0s
datanode_1  | 
datanode_1  | 2020-04-17 19:12:30 (17.0 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [162/162]
datanode_1  | 
datanode_1  | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
datanode_1  | KVNO Timestamp         Principal
datanode_1  | ---- ----------------- --------------------------------------------------------
datanode_1  |    2 04/17/20 19:12:30 HTTP/d43c26201e9a@EXAMPLE.COM
datanode_1  |    2 04/17/20 19:12:30 HTTP/d43c26201e9a@EXAMPLE.COM
datanode_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1  | 2020-04-17 19:12:32,644 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1  | /************************************************************
datanode_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1  | STARTUP_MSG:   host = d43c26201e9a/172.24.0.3
datanode_1  | STARTUP_MSG:   args = []
datanode_1  | STARTUP_MSG:   version = 3.2.0
datanode_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1  | STARTUP_MSG:   java = 11.0.6
datanode_1  | ************************************************************/
datanode_1  | 2020-04-17 19:12:32,736 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1  | 2020-04-17 19:12:35,162 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1  | 2020-04-17 19:12:35,809 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1  | 2020-04-17 19:12:37,244 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1  | 2020-04-17 19:12:37,244 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1  | 2020-04-17 19:12:38,397 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d43c26201e9a ip:172.24.0.3
datanode_1  | 2020-04-17 19:12:40,631 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/_HOST@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode_1  | WARNING: An illegal reflective access operation has occurred
datanode_1  | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
datanode_1  | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
datanode_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_1  | WARNING: All illegal access operations will be denied in a future release
datanode_1  | 2020-04-17 19:12:41,683 [main] INFO security.UserGroupInformation: Login successful for user dn/d43c26201e9a@EXAMPLE.COM using keytab file /etc/security/keytabs/dn.keytab
datanode_1  | 2020-04-17 19:12:41,683 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode_1  | 2020-04-17 19:12:41,683 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode_1  | 2020-04-17 19:12:41,684 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode_1  | 2020-04-17 19:12:41,684 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode_1  | 2020-04-17 19:12:41,854 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode_1  | 2020-04-17 19:12:45,810 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode_1  | 2020-04-17 19:12:45,888 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.24.0.3,host:d43c26201e9a
datanode_1  | 2020-04-17 19:12:45,911 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode_1  | 2020-04-17 19:12:45,924 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@d43c26201e9a
datanode_1  | 2020-04-17 19:12:48,155 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:49,156 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:50,157 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:51,159 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:52,161 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:53,162 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:54,163 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:55,164 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:56,165 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:57,166 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1  | 2020-04-17 19:12:59,776 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode_1  | 2020-04-17 19:12:59,941 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2  | Sleeping for 5 seconds
datanode_2  | Setting up kerberos!!
datanode_2  | KDC ISSUER_SERVER => kdc:8081
datanode_2  | Sleeping for 5 seconds
datanode_2  | Got 200, KDC service ready!!
datanode_2  | Download dn/1877e902e348@EXAMPLE.COM keytab file to /etc/security/keytabs/dn.keytab
datanode_2  | --2020-04-17 19:12:31--  http://kdc:8081/keytab/1877e902e348/dn
datanode_2  | Resolving kdc (kdc)... 172.24.0.4
datanode_2  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_2  | HTTP request sent, awaiting response... 200 OK
datanode_2  | Length: 158 [application/octet-stream]
datanode_2  | Saving to: '/etc/security/keytabs/dn.keytab'
datanode_2  | 
datanode_2  |      0K                                                       100% 25.5M=0s
datanode_2  | 
datanode_2  | 2020-04-17 19:12:31 (25.5 MB/s) - '/etc/security/keytabs/dn.keytab' saved [158/158]
datanode_2  | 
datanode_2  | Keytab name: FILE:/etc/security/keytabs/dn.keytab
datanode_2  | KVNO Timestamp         Principal
datanode_2  | ---- ----------------- --------------------------------------------------------
datanode_2  |    2 04/17/20 19:12:31 dn/1877e902e348@EXAMPLE.COM
datanode_2  |    2 04/17/20 19:12:31 dn/1877e902e348@EXAMPLE.COM
datanode_2  | Download HTTP/1877e902e348@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
datanode_2  | --2020-04-17 19:12:31--  http://kdc:8081/keytab/1877e902e348/HTTP
datanode_2  | Resolving kdc (kdc)... 172.24.0.4
datanode_2  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_2  | HTTP request sent, awaiting response... 200 OK
datanode_2  | Length: 162 [application/octet-stream]
datanode_2  | Saving to: '/etc/security/keytabs/HTTP.keytab'
datanode_2  | 
datanode_2  |      0K                                                       100% 26.6M=0s
datanode_2  | 
datanode_2  | 2020-04-17 19:12:31 (26.6 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [162/162]
datanode_2  | 
datanode_2  | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
datanode_2  | KVNO Timestamp         Principal
datanode_2  | ---- ----------------- --------------------------------------------------------
datanode_2  |    2 04/17/20 19:12:31 HTTP/1877e902e348@EXAMPLE.COM
datanode_2  |    2 04/17/20 19:12:31 HTTP/1877e902e348@EXAMPLE.COM
datanode_2  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_2  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2  | 2020-04-17 19:12:37,472 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2  | /************************************************************
datanode_2  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2  | STARTUP_MSG:   host = 1877e902e348/172.24.0.7
datanode_2  | STARTUP_MSG:   args = []
datanode_2  | STARTUP_MSG:   version = 3.2.0
datanode_2  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_2  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2  | STARTUP_MSG:   java = 11.0.6
datanode_2  | ************************************************************/
datanode_2  | 2020-04-17 19:12:37,574 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2  | 2020-04-17 19:12:39,557 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2  | 2020-04-17 19:12:40,125 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2  | 2020-04-17 19:12:41,351 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2  | 2020-04-17 19:12:41,351 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2  | 2020-04-17 19:12:42,491 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1877e902e348 ip:172.24.0.7
datanode_2  | 2020-04-17 19:12:44,284 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/_HOST@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode_2  | WARNING: An illegal reflective access operation has occurred
datanode_2  | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
datanode_2  | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
datanode_2  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_2  | WARNING: All illegal access operations will be denied in a future release
datanode_2  | 2020-04-17 19:12:45,362 [main] INFO security.UserGroupInformation: Login successful for user dn/1877e902e348@EXAMPLE.COM using keytab file /etc/security/keytabs/dn.keytab
datanode_2  | 2020-04-17 19:12:45,362 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode_2  | 2020-04-17 19:12:45,362 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode_2  | 2020-04-17 19:12:45,363 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode_2  | 2020-04-17 19:12:45,363 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode_2  | 2020-04-17 19:12:45,364 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode_2  | 2020-04-17 19:12:48,620 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode_2  | 2020-04-17 19:12:48,670 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.24.0.7,host:1877e902e348
datanode_2  | 2020-04-17 19:12:48,670 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode_2  | 2020-04-17 19:12:48,681 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@1877e902e348
datanode_2  | 2020-04-17 19:12:50,632 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:51,633 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:52,634 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:53,638 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:54,639 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:55,641 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:12:56,642 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2  | 2020-04-17 19:13:00,169 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode_2  | 2020-04-17 19:13:00,519 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2  | 2020-04-17 19:13:00,552 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_2  | 2020-04-17 19:13:00,553 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2  | 2020-04-17 19:13:00,583 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2  | 2020-04-17 19:13:00,729 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2  | 2020-04-17 19:13:02,748 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2  | 2020-04-17 19:13:02,924 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_2  | 2020-04-17 19:13:03,114 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_2  | 2020-04-17 19:13:03,115 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2  | 2020-04-17 19:13:03,117 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:13:03,117 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2  | 2020-04-17 19:13:03,120 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-17 19:13:04,256 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:13:04,507 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2  | 2020-04-17 19:13:04,601 [main] INFO util.log: Logging initialized @32938ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2  | 2020-04-17 19:13:04,943 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2  | 2020-04-17 19:13:04,957 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2  | 2020-04-17 19:13:05,011 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2  | 2020-04-17 19:13:05,022 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2  | 2020-04-17 19:13:05,022 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2  | 2020-04-17 19:13:05,022 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2  | 2020-04-17 19:13:05,153 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2  | 2020-04-17 19:13:05,158 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_2  | 2020-04-17 19:13:05,228 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2  | 2020-04-17 19:13:05,228 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2  | 2020-04-17 19:13:05,234 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2  | 2020-04-17 19:13:05,281 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@372461a9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2  | 2020-04-17 19:13:05,286 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@35e98af{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2  | 2020-04-17 19:13:05,691 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2322e56f{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-1762017479258105151.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2  | 2020-04-17 19:13:05,736 [main] INFO server.AbstractConnector: Started ServerConnector@be164d8{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2  | 2020-04-17 19:13:05,736 [main] INFO server.Server: Started @34073ms
datanode_2  | 2020-04-17 19:13:05,758 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2  | 2020-04-17 19:13:05,758 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2  | 2020-04-17 19:13:05,772 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2  | 2020-04-17 19:13:05,890 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@31cc3812] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2  | 2020-04-17 19:13:06,012 [Datanode State Machine Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_2  | 2020-04-17 19:13:08,273 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2  | 2020-04-17 19:13:08,278 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2  | 2020-04-17 19:13:08,283 [Datanode State Machine Thread - 1] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5f2fec2e-3f74-4923-9dbd-94817f14424e at port 9858
datanode_2  | 2020-04-17 19:13:08,358 [Datanode State Machine Thread - 1] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start RPC server
datanode_2  | 2020-04-17 19:13:08,625 [Datanode State Machine Thread - 1] INFO server.GrpcService: 5f2fec2e-3f74-4923-9dbd-94817f14424e: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2  | 2020-04-17 19:13:12,888 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-B75E7E880CD8:[5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-B75E7E880CD8:java.util.concurrent.CompletableFuture@4f4ba6e3[Not completed]
datanode_2  | 2020-04-17 19:13:12,939 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-B75E7E880CD8:[5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:13:12,941 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:13:12,949 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:13:12,950 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:13:12,952 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:13:12,952 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:13:12,963 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: ConfigurationManager, init=-1: [5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:13:12,967 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:13:12,983 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:13:12,984 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/77b1ef9a-a408-42d5-adf2-b75e7e880cd8 does not exist. Creating ...
datanode_2  | 2020-04-17 19:13:12,993 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/77b1ef9a-a408-42d5-adf2-b75e7e880cd8/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:13:13,004 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/77b1ef9a-a408-42d5-adf2-b75e7e880cd8 has been successfully formatted.
datanode_2  | 2020-04-17 19:13:13,051 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-B75E7E880CD8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:13:13,051 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:13:13,077 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:12:59,955 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_1  | 2020-04-17 19:12:59,960 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1  | 2020-04-17 19:13:00,000 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1  | 2020-04-17 19:13:00,139 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1  | 2020-04-17 19:13:02,530 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1  | 2020-04-17 19:13:02,693 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_1  | 2020-04-17 19:13:03,110 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_1  | 2020-04-17 19:13:03,116 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1  | 2020-04-17 19:13:03,119 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:13:03,121 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1  | 2020-04-17 19:13:03,134 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:13:04,262 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:13:04,629 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1  | 2020-04-17 19:13:04,730 [main] INFO util.log: Logging initialized @34414ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1  | 2020-04-17 19:13:05,306 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1  | 2020-04-17 19:13:05,316 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1  | 2020-04-17 19:13:05,365 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1  | 2020-04-17 19:13:05,373 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1  | 2020-04-17 19:13:05,389 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1  | 2020-04-17 19:13:05,390 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1  | 2020-04-17 19:13:05,575 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1  | 2020-04-17 19:13:05,576 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_1  | 2020-04-17 19:13:05,790 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1  | 2020-04-17 19:13:05,792 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1  | 2020-04-17 19:13:05,794 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1  | 2020-04-17 19:13:05,860 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@243bf087{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1  | 2020-04-17 19:13:05,869 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11c78080{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1  | 2020-04-17 19:13:06,228 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c70aae1{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-5448374091527038015.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1  | 2020-04-17 19:13:06,253 [main] INFO server.AbstractConnector: Started ServerConnector@2d313c8c{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1  | 2020-04-17 19:13:06,254 [main] INFO server.Server: Started @35939ms
datanode_1  | 2020-04-17 19:13:06,260 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1  | 2020-04-17 19:13:06,260 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1  | 2020-04-17 19:13:06,272 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1  | 2020-04-17 19:13:06,343 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@25a17821] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1  | 2020-04-17 19:13:06,470 [Datanode State Machine Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_1  | 2020-04-17 19:13:08,618 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1  | 2020-04-17 19:13:08,620 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1  | 2020-04-17 19:13:08,625 [Datanode State Machine Thread - 1] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 6acf0e94-e616-400e-845a-8f99bcdd25f3 at port 9858
datanode_1  | 2020-04-17 19:13:08,791 [Datanode State Machine Thread - 1] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start RPC server
datanode_1  | 2020-04-17 19:13:08,969 [Datanode State Machine Thread - 1] INFO server.GrpcService: 6acf0e94-e616-400e-845a-8f99bcdd25f3: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1  | 2020-04-17 19:13:13,395 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-AA7F6FA89839:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858] returns group-AA7F6FA89839:java.util.concurrent.CompletableFuture@17220dcb[Not completed]
datanode_1  | 2020-04-17 19:13:13,589 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-AA7F6FA89839:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:13:13,602 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:13:13,605 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:13:13,606 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:13:13,608 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:13:13,613 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:13:13,622 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:13:13,641 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:13:13,671 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:13:13,678 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2b1d3056-312e-49c7-a29e-aa7f6fa89839 does not exist. Creating ...
datanode_1  | 2020-04-17 19:13:13,694 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2b1d3056-312e-49c7-a29e-aa7f6fa89839/in_use.lock acquired by nodename 6@d43c26201e9a
kdc_1       | Issuer is listening on : 8081krb5kdc: starting...
kdc_1       | kadmind: starting...
kdc_1       | otp: Loaded
kdc_1       | Apr 17 19:12:22 kdc krb5kdc[8](info): setting up network...
kdc_1       | krb5kdc: setsockopt(9,IPV6_V6ONLY,1) worked
kdc_1       | krb5kdc: setsockopt(11,IPV6_V6ONLY,1) worked
kdc_1       | Apr 17 19:12:22 kdc krb5kdc[8](info): set up 4 sockets
kdc_1       | Apr 17 19:12:22 kdc krb5kdc[8](info): commencing operation
kdc_1       | Apr 17 19:08:49 7efde7739aa3 kadmin.local[1](info): No dictionary file specified, continuing without one.
kdc_1       | Apr 17 19:08:50 adafe27f7f90 kadmin.local[1](info): No dictionary file specified, continuing without one.
kdc_1       | Apr 17 19:12:26 kdc kadmind[13](info): No dictionary file specified, continuing without one.
kdc_1       | Apr 17 19:12:26 kdc kadmind[13](info): setting up network...
kdc_1       | kadmind: setsockopt(9,IPV6_V6ONLY,1) worked
kdc_1       | kadmind: setsockopt(11,IPV6_V6ONLY,1) worked
kdc_1       | kadmind: setsockopt(13,IPV6_V6ONLY,1) worked
kdc_1       | Apr 17 19:12:26 kdc kadmind[13](info): set up 6 sockets
kdc_1       | Apr 17 19:12:26 kdc kadmind[13](info): Seeding random number generator
kdc_1       | Apr 17 19:12:26 kdc kadmind[13](info): starting
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for test/test@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "test/test@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal test/test@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/test.test.keytab.
kdc_1       | Entry for principal test/test@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/test.test.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for test/test@EXAMPLE.COM; defaulting to no policy
kdc_1       | add_principal: Principal or policy already exists while creating "test/test@EXAMPLE.COM".
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal test/test@EXAMPLE.COM with kvno 3, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/test.test.keytab.
kdc_1       | Entry for principal test/test@EXAMPLE.COM with kvno 3, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/test.test.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for dn/d43c26201e9a@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "dn/d43c26201e9a@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal dn/d43c26201e9a@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.d43c26201e9a.keytab.
kdc_1       | Entry for principal dn/d43c26201e9a@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.d43c26201e9a.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for scm/scm@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "scm/scm@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal scm/scm@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/scm.scm.keytab.
kdc_1       | Entry for principal scm/scm@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/scm.scm.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/d43c26201e9a@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/d43c26201e9a@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/d43c26201e9a@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.d43c26201e9a.keytab.
kdc_1       | Entry for principal HTTP/d43c26201e9a@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.d43c26201e9a.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/scm@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/scm@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/scm@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.scm.keytab.
kdc_1       | Entry for principal HTTP/scm@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.scm.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for testuser/scm@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "testuser/scm@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal testuser/scm@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser.scm.keytab.
kdc_1       | Entry for principal testuser/scm@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser.scm.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for testuser2/scm@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "testuser2/scm@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal testuser2/scm@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser2.scm.keytab.
kdc_1       | Entry for principal testuser2/scm@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser2.scm.keytab.
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150749, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150749, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos daApr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
datanode_3  | Sleeping for 5 seconds
datanode_3  | Setting up kerberos!!
datanode_3  | KDC ISSUER_SERVER => kdc:8081
datanode_3  | Sleeping for 5 seconds
datanode_3  | Got 200, KDC service ready!!
datanode_3  | Download dn/1dab03dd0801@EXAMPLE.COM keytab file to /etc/security/keytabs/dn.keytab
datanode_3  | --2020-04-17 19:12:32--  http://kdc:8081/keytab/1dab03dd0801/dn
datanode_3  | Resolving kdc (kdc)... 172.24.0.4
datanode_3  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_3  | HTTP request sent, awaiting response... 200 OK
datanode_3  | Length: 158 [application/octet-stream]
datanode_3  | Saving to: '/etc/security/keytabs/dn.keytab'
datanode_3  | 
datanode_3  |      0K                                                       100% 21.5M=0s
datanode_3  | 
datanode_3  | 2020-04-17 19:12:32 (21.5 MB/s) - '/etc/security/keytabs/dn.keytab' saved [158/158]
datanode_3  | 
datanode_3  | Keytab name: FILE:/etc/security/keytabs/dn.keytab
datanode_3  | KVNO Timestamp         Principal
datanode_3  | ---- ----------------- --------------------------------------------------------
datanode_3  |    2 04/17/20 19:12:32 dn/1dab03dd0801@EXAMPLE.COM
datanode_3  |    2 04/17/20 19:12:32 dn/1dab03dd0801@EXAMPLE.COM
datanode_3  | Download HTTP/1dab03dd0801@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
datanode_3  | --2020-04-17 19:12:32--  http://kdc:8081/keytab/1dab03dd0801/HTTP
datanode_3  | Resolving kdc (kdc)... 172.24.0.4
datanode_3  | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
datanode_3  | HTTP request sent, awaiting response... 200 OK
datanode_3  | Length: 162 [application/octet-stream]
datanode_3  | Saving to: '/etc/security/keytabs/HTTP.keytab'
datanode_3  | 
datanode_3  |      0K                                                       100% 33.6M=0s
datanode_3  | 
datanode_3  | 2020-04-17 19:12:32 (33.6 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [162/162]
datanode_3  | 
datanode_3  | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
datanode_3  | KVNO Timestamp         Principal
datanode_3  | ---- ----------------- --------------------------------------------------------
datanode_3  |    2 04/17/20 19:12:32 HTTP/1dab03dd0801@EXAMPLE.COM
datanode_3  |    2 04/17/20 19:12:32 HTTP/1dab03dd0801@EXAMPLE.COM
datanode_3  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_3  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3  | 2020-04-17 19:12:38,893 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3  | /************************************************************
datanode_3  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3  | STARTUP_MSG:   host = 1dab03dd0801/172.24.0.10
datanode_3  | STARTUP_MSG:   args = []
datanode_3  | STARTUP_MSG:   version = 3.2.0
datanode_3  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_3  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3  | STARTUP_MSG:   java = 11.0.6
datanode_3  | ************************************************************/
datanode_3  | 2020-04-17 19:12:38,995 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3  | 2020-04-17 19:12:40,820 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3  | 2020-04-17 19:12:41,365 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3  | 2020-04-17 19:12:42,419 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3  | 2020-04-17 19:12:42,419 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3  | 2020-04-17 19:12:43,441 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:1dab03dd0801 ip:172.24.0.10
datanode_3  | 2020-04-17 19:12:45,928 [main] INFO ozone.HddsDatanodeService: Ozone security is enabled. Attempting login for Hdds Datanode user. Principal: dn/_HOST@EXAMPLE.COM,keytab: /etc/security/keytabs/dn.keytab
datanode_3  | WARNING: An illegal reflective access operation has occurred
datanode_3  | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
datanode_3  | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
datanode_3  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
datanode_3  | WARNING: All illegal access operations will be denied in a future release
datanode_3  | 2020-04-17 19:12:47,187 [main] INFO security.UserGroupInformation: Login successful for user dn/1dab03dd0801@EXAMPLE.COM using keytab file /etc/security/keytabs/dn.keytab
datanode_3  | 2020-04-17 19:12:47,187 [main] INFO ozone.HddsDatanodeService: Hdds Datanode login successful.
datanode_3  | 2020-04-17 19:12:47,187 [main] INFO ozone.HddsDatanodeService: Initializing secure Datanode.
datanode_3  | 2020-04-17 19:12:47,197 [main] ERROR client.DNCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
datanode_3  | 2020-04-17 19:12:47,197 [main] INFO client.DNCertificateClient: Certificate client init case: 0
datanode_3  | 2020-04-17 19:12:47,202 [main] INFO client.DNCertificateClient: Creating keypair for client as keypair and certificate not found.
datanode_3  | 2020-04-17 19:12:50,158 [main] INFO ozone.HddsDatanodeService: Init response: GETCERT
datanode_3  | 2020-04-17 19:12:50,186 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.24.0.10,host:1dab03dd0801
datanode_3  | 2020-04-17 19:12:50,198 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
datanode_3  | 2020-04-17 19:12:50,205 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:root@1dab03dd0801
datanode_3  | 2020-04-17 19:12:51,996 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:52,997 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:54,015 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:55,016 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:56,017 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:57,018 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9961. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3  | 2020-04-17 19:12:59,984 [main] INFO ozone.HddsDatanodeService: Successfully stored SCM signed certificate, case:GETCERT.
datanode_3  | 2020-04-17 19:13:00,285 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3  | 2020-04-17 19:13:00,309 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_3  | 2020-04-17 19:13:00,315 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3  | 2020-04-17 19:13:00,354 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3  | 2020-04-17 19:13:00,479 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3  | 2020-04-17 19:13:02,698 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3  | 2020-04-17 19:13:02,869 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_3  | 2020-04-17 19:13:03,076 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_1  | 2020-04-17 19:13:13,707 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2b1d3056-312e-49c7-a29e-aa7f6fa89839 has been successfully formatted.
datanode_1  | 2020-04-17 19:13:13,712 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-AA7F6FA89839: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:13:13,744 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:13:13,759 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:13:13,782 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:13:13,784 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:13:13,794 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:13,821 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:13:13,942 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:13:13,982 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2b1d3056-312e-49c7-a29e-aa7f6fa89839
datanode_1  | 2020-04-17 19:13:13,992 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:13:13,993 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:13:14,000 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:14,000 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:13:14,001 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:14,001 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:13:14,012 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:13:14,016 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:13:14,016 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:13:14,098 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:13:14,134 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:13:14,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:13:14,161 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:13:14,162 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:13:14,162 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:13:14,194 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839
datanode_1  | 2020-04-17 19:13:14,196 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839
datanode_1  | 2020-04-17 19:13:14,197 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858], old=null
datanode_1  | 2020-04-17 19:13:14,198 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:13:14,204 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:13:14,247 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AA7F6FA89839,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:13:14,249 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839
datanode_1  | 2020-04-17 19:13:14,291 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "2b1d3056-312e-49c7-a29e-aa7f6fa89839"
datanode_1  | .
datanode_1  | 2020-04-17 19:13:14,297 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-128988F51F35:java.util.concurrent.CompletableFuture@20bb63b0[Not completed]
datanode_1  | 2020-04-17 19:13:14,299 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:13:14,299 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:13:14,299 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:13:14,299 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:13:14,300 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 does not exist. Creating ...
datanode_1  | 2020-04-17 19:13:14,311 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/in_use.lock acquired by nodename 6@d43c26201e9a
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_create_principal, test/test@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, test/test@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc tabase
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150749, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:29 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150749, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:30 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150750, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | kadmind[13](Notice): Request: kadm5_get_principal, test/test@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_create_principal, test/test@EXAMPLE.COM, Principal or policy already exists, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, test/test@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
datanode_2  | 2020-04-17 19:13:13,097 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:13:13,099 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:13:13,100 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,126 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:13:13,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:13:13,187 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/77b1ef9a-a408-42d5-adf2-b75e7e880cd8
datanode_2  | 2020-04-17 19:13:13,188 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:13:13,188 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:13:13,189 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,189 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:13:13,189 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,189 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:13:13,205 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:13:13,205 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:13:13,206 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:13:13,266 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:13:13,275 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:13:13,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:13:13,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:13:13,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:13:13,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:13:13,314 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8
datanode_2  | 2020-04-17 19:13:13,318 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8
datanode_2  | 2020-04-17 19:13:13,320 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: start as a follower, conf=-1: [5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:13:13,321 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:13:13,321 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:13:13,376 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B75E7E880CD8,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:13:13,381 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8
datanode_2  | 2020-04-17 19:13:13,494 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "77b1ef9a-a408-42d5-adf2-b75e7e880cd8"
datanode_2  | .
datanode_2  | 2020-04-17 19:13:13,499 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-128988F51F35:java.util.concurrent.CompletableFuture@78ad8437[Not completed]
datanode_2  | 2020-04-17 19:13:13,501 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:13:13,504 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:13:13,507 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:13:13,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:13:13,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:13:13,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:13:13,509 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:13:13,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:13:13,510 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:13:13,511 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 does not exist. Creating ...
datanode_2  | 2020-04-17 19:13:13,515 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:13:13,520 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 has been successfully formatted.
datanode_2  | 2020-04-17 19:13:13,531 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-128988F51F35: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:13:13,532 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:13:13,532 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:13:13,532 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](Notice): Request: kadm5_get_principal, test/test@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:29 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, dn/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, dn/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, dn/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, scm/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, scm/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, scm/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/d43c26201e9a@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, testuser/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, testuser/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, testuser/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_create_principal, testuser2/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, testuser2/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
datanode_2  | 2020-04-17 19:13:13,533 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:13:13,533 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,537 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:13:13,559 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35
datanode_2  | 2020-04-17 19:13:13,560 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:13:13,560 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:13:13,560 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,560 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:13:13,561 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:13:13,561 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:13:13,561 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:13:13,562 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:13:13,564 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:13:13,565 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:13:13,586 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:13:13,592 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:13:13,593 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:13:13,593 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:13:13,595 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:13:13,595 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:13:13,595 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:13:13,596 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:13:13,597 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:13:13,598 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:13:13,608 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:13:13,608 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:13:15,029 [grpc-default-executor-0] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-DAF0FA4CF748->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35, cid=0, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:13:15,804 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | .
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](Notice): Request: kadm5_get_principal, testuser2/scm@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:30 kdc kadmind[13](info): closing down fd 18
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for recon/recon@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "recon/recon@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal recon/recon@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/recon.recon.keytab.
kdc_1       | Entry for principal recon/recon@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/recon.recon.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/recon@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/recon@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/recon@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.recon.keytab.
kdc_1       | Entry for principal HTTP/recon@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.recon.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for dn/1877e902e348@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "dn/1877e902e348@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal dn/1877e902e348@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.1877e902e348.keytab.
kdc_1       | Entry for principal dn/1877e902e348@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.1877e902e348.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/1877e902e348@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/1877e902e348@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/1877e902e348@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.1877e902e348.keytab.
kdc_1       | Entry for principal HTTP/1877e902e348@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.1877e902e348.keytab.
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_create_principal, recon/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, recon/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_principal, recon/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/recon@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_create_principal, dn/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, dn/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_principal, dn/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/1877e902e348@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:31 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:31 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150751, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for dn/1dab03dd0801@EXAMPLE.COM; defaulting to no policy
datanode_1  | 2020-04-17 19:13:14,319 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 has been successfully formatted.
datanode_1  | 2020-04-17 19:13:14,320 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-128988F51F35: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:13:14,323 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:13:14,324 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:13:14,325 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:13:14,325 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:13:14,368 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:13:14,368 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:13:14,368 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:13:14,369 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:13:14,369 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:13:14,369 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:13:14,370 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:13:14,370 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:13:14,371 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:13:14,372 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:13:14,372 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:13:15,712 [grpc-default-executor-0] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-29965FFF7B9B->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35, cid=1, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
kms_1       | Sleeping for 5 seconds
kms_1       | Setting up kerberos!!
kms_1       | KDC ISSUER_SERVER => kdc:8081
kms_1       | /opt/starter.sh: line 66: SLEEP_SECONDS: command not found
kms_1       | Sleeping for  seconds
kms_1       | Got 200, KDC service ready!!
kms_1       | # Licensed to the Apache Software Foundation (ASF) under one or more
kms_1       | # contributor license agreements.  See the NOTICE file distributed with
kms_1       | # this work for additional information regarding copyright ownership.
kms_1       | # The ASF licenses this file to You under the Apache License, Version 2.0
kms_1       | # (the "License"); you may not use this file except in compliance with
kms_1       | # the License.  You may obtain a copy of the License at
kms_1       | #
kms_1       | #     http://www.apache.org/licenses/LICENSE-2.0
kms_1       | #
kms_1       | # Unless required by applicable law or agreed to in writing, software
kms_1       | # distributed under the License is distributed on an "AS IS" BASIS,
kms_1       | # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
kms_1       | # See the License for the specific language governing permissions and
kms_1       | # limitations under the License.
kms_1       | 
kms_1       | [logging]
kms_1       |  default = FILE:/var/log/krb5libs.log
kms_1       |  kdc = FILE:/var/log/krb5kdc.log
kms_1       |  admin_server = FILE:/var/log/kadmind.log
kms_1       | 
kms_1       | [libdefaults]
kms_1       |  dns_canonicalize_hostname = false
kms_1       |  dns_lookup_realm = false
kms_1       |  ticket_lifetime = 24h
kms_1       |  renew_lifetime = 7d
kms_1       |  forwardable = true
kms_1       |  rdns = false
kms_1       |  default_realm = EXAMPLE.COM
kms_1       | 
kms_1       | [realms]
kms_1       |  EXAMPLE.COM = {
kms_1       |   kdc = kdc
kms_1       |   admin_server = kdc
kms_1       |  }
kms_1       | 
kms_1       | [domain_realm]
kms_1       |  .example.com = EXAMPLE.COM
kms_1       | WARNING: /opt/hadoop/temp does not exist. Creating.
kms_1       | WARNING: /opt/hadoop/logs does not exist. Creating.
kms_1       | Exception in thread "main" java.io.IOException: Problem starting http server
kms_1       | 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1171)
kms_1       | 	at org.apache.hadoop.crypto.key.kms.server.KMSWebServer.start(KMSWebServer.java:130)
kms_1       | 	at org.apache.hadoop.crypto.key.kms.server.KMSWebServer.main(KMSWebServer.java:176)
kms_1       | Caused by: javax.servlet.ServletException: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/HTTP.keytab
kms_1       | 	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)
kms_1       | 	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194)
kms_1       | 	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180)
kms_1       | 	at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
kms_1       | 	at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873)
kms_1       | 	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349)
kms_1       | 	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778)
kms_1       | 	at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262)
kms_1       | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)
kms_1       | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
kms_1       | 	at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:161)
kms_1       | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)
kms_1       | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
kms_1       | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
kms_1       | 	at org.eclipse.jetty.server.Server.start(Server.java:427)
kms_1       | 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105)
kms_1       | 	at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
kms_1       | 	at org.eclipse.jetty.server.Server.doStart(Server.java:394)
kms_1       | 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
kms_1       | 	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1140)
kms_1       | 	... 2 more
kms_1       | Caused by: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/HTTP.keytab
kms_1       | 	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:145)
kms_1       | 	... 26 more
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:13:15,717 [grpc-default-executor-1] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-5D9DECFDD794->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35, cid=1, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:13:15,844 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | .
datanode_1  | 2020-04-17 19:13:18,305 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_1  | 2020-04-17 19:13:18,308 [grpc-default-executor-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:13:18,309 [Thread-27] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1  | 2020-04-17 19:13:18,309 [grpc-default-executor-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:13:18,565 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-128988F51F35 with new leaderId: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_1  | 2020-04-17 19:13:18,568 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: change Leader from null to 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 at term 1 for appendEntries, leader elected after 4241ms
datanode_1  | 2020-04-17 19:13:18,603 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_1  | 2020-04-17 19:13:18,662 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:13:18,935 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/current/log_inprogress_0
datanode_1  | 2020-04-17 19:13:19,368 [Thread-25] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-FollowerState: change to CANDIDATE, lastRpcTime:5169ms, electionTimeout:5125ms
datanode_1  | 2020-04-17 19:13:19,369 [Thread-25] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:13:19,369 [Thread-25] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-04-17 19:13:19,371 [Thread-25] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderElection
datanode_1  | 2020-04-17 19:13:19,382 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858], old=null
datanode_1  | 2020-04-17 19:13:19,383 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderElection
datanode_1  | 2020-04-17 19:13:19,383 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2020-04-17 19:13:19,383 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-AA7F6FA89839 with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:13:03,077 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3  | 2020-04-17 19:13:03,077 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:13:03,080 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3  | 2020-04-17 19:13:03,081 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2020-04-17 19:13:04,118 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:13:04,504 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3  | 2020-04-17 19:13:04,583 [main] INFO util.log: Logging initialized @31307ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3  | 2020-04-17 19:13:04,901 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3  | 2020-04-17 19:13:04,912 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3  | 2020-04-17 19:13:04,930 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3  | 2020-04-17 19:13:04,944 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3  | 2020-04-17 19:13:04,944 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3  | 2020-04-17 19:13:04,944 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3  | 2020-04-17 19:13:05,038 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3  | 2020-04-17 19:13:05,056 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_3  | 2020-04-17 19:13:05,128 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3  | 2020-04-17 19:13:05,128 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3  | 2020-04-17 19:13:05,130 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3  | 2020-04-17 19:13:05,251 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79d7035{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3  | 2020-04-17 19:13:05,259 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@126f8f24{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3  | 2020-04-17 19:13:05,637 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2a8dd942{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-494570960552582906.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3  | 2020-04-17 19:13:05,707 [main] INFO server.AbstractConnector: Started ServerConnector@4b87760e{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3  | 2020-04-17 19:13:05,713 [main] INFO server.Server: Started @32433ms
datanode_3  | 2020-04-17 19:13:05,716 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3  | 2020-04-17 19:13:05,722 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3  | 2020-04-17 19:13:05,728 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3  | 2020-04-17 19:13:05,802 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@d62498a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3  | 2020-04-17 19:13:05,891 [Datanode State Machine Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.6:9891
datanode_3  | 2020-04-17 19:13:08,273 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3  | 2020-04-17 19:13:08,276 [Datanode State Machine Thread - 1] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3  | 2020-04-17 19:13:08,276 [Datanode State Machine Thread - 1] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 at port 9858
datanode_3  | 2020-04-17 19:13:08,382 [Datanode State Machine Thread - 1] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start RPC server
datanode_3  | 2020-04-17 19:13:08,777 [Datanode State Machine Thread - 1] INFO server.GrpcService: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3  | 2020-04-17 19:13:12,798 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-0257130A8D8A:[3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858] returns group-0257130A8D8A:java.util.concurrent.CompletableFuture@14acdd48[Not completed]
datanode_3  | 2020-04-17 19:13:12,817 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-0257130A8D8A:[3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:13:12,819 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:13:12,819 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:13:12,819 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:13:12,820 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:13:12,820 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:13:12,826 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: ConfigurationManager, init=-1: [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:13:12,826 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:13:12,830 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:13:12,832 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/56f8c321-7697-48d5-9d4e-0257130a8d8a does not exist. Creating ...
datanode_3  | 2020-04-17 19:13:12,836 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/56f8c321-7697-48d5-9d4e-0257130a8d8a/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:13:12,843 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/56f8c321-7697-48d5-9d4e-0257130a8d8a has been successfully formatted.
datanode_3  | 2020-04-17 19:13:12,859 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-0257130A8D8A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:13:12,859 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:13:12,863 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:13:12,867 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:13:12,867 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
kdc_1       | Principal "dn/1dab03dd0801@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal dn/1dab03dd0801@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.1dab03dd0801.keytab.
kdc_1       | Entry for principal dn/1dab03dd0801@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/dn.1dab03dd0801.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for om/om@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "om/om@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal om/om@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/om.om.keytab.
kdc_1       | Entry for principal om/om@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/om.om.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/1dab03dd0801@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/1dab03dd0801@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/1dab03dd0801@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.1dab03dd0801.keytab.
kdc_1       | Entry for principal HTTP/1dab03dd0801@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.1dab03dd0801.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/om@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/om@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/om@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.om.keytab.
kdc_1       | Entry for principal HTTP/om@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.om.keytab.
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_create_principal, dn/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, dn/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_principal, dn/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_create_principal, om/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, om/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_principal, om/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/1dab03dd0801@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/om@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:32 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:32 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150752, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for s3g/s3g@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "s3g/s3g@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal s3g/s3g@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/s3g.s3g.keytab.
kdc_1       | Entry for principal s3g/s3g@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/s3g.s3g.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for HTTP/s3g@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "HTTP/s3g@EXAMPLE.COM" created.
datanode_3  | 2020-04-17 19:13:12,872 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:12,897 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:13:12,920 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:13:12,930 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/56f8c321-7697-48d5-9d4e-0257130a8d8a
datanode_3  | 2020-04-17 19:13:12,933 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:13:12,934 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:13:12,935 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:12,938 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:13:12,939 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:12,939 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:13:12,940 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:13:12,941 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:13:12,943 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:13:12,990 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:13:13,018 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:13:13,022 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:13:13,026 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:13:13,026 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:13:13,031 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:13:13,086 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A
datanode_3  | 2020-04-17 19:13:13,090 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A
datanode_3  | 2020-04-17 19:13:13,096 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: start as a follower, conf=-1: [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858], old=null
datanode_3  | 2020-04-17 19:13:13,097 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:13:13,098 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:13:13,103 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0257130A8D8A,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:13:13,104 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A
datanode_3  | 2020-04-17 19:13:13,122 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "56f8c321-7697-48d5-9d4e-0257130a8d8a"
datanode_3  | .
datanode_3  | 2020-04-17 19:13:13,126 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-128988F51F35:java.util.concurrent.CompletableFuture@3463b367[Not completed]
datanode_3  | 2020-04-17 19:13:13,132 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:13:13,133 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:13:13,134 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:13:13,134 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:13:13,135 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:13:13,135 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:13:13,135 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:13:13,135 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:13:13,135 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:13:13,136 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 does not exist. Creating ...
datanode_3  | 2020-04-17 19:13:13,143 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:13:13,145 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35 has been successfully formatted.
datanode_3  | 2020-04-17 19:13:13,145 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-128988F51F35: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:13:13,146 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:13:13,148 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:13:13,149 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:13:13,151 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:13:15,823 [grpc-default-executor-0] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-F3AF8BC6189B->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35, cid=1, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:13:18,300 [grpc-default-executor-0] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_2  | 2020-04-17 19:13:18,306 [grpc-default-executor-0] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:13:18,306 [grpc-default-executor-0] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:13:18,307 [Thread-27] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:13:18,401 [Thread-25] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-FollowerState: change to CANDIDATE, lastRpcTime:5079ms, electionTimeout:5035ms
datanode_2  | 2020-04-17 19:13:18,403 [Thread-25] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:13:18,403 [Thread-25] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-04-17 19:13:18,405 [Thread-25] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderElection
datanode_2  | 2020-04-17 19:13:18,424 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1: begin an election at term 1 for -1: [5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:13:18,425 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown LeaderElection
datanode_2  | 2020-04-17 19:13:18,425 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2020-04-17 19:13:18,426 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B75E7E880CD8 with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:13:18,426 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for becomeLeader, leader elected after 5375ms
datanode_2  | 2020-04-17 19:13:18,439 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2020-04-17 19:13:18,439 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2020-04-17 19:13:18,441 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8
datanode_2  | 2020-04-17 19:13:18,449 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:13:18,455 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2  | 2020-04-17 19:13:18,460 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2020-04-17 19:13:18,461 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2020-04-17 19:13:18,467 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2020-04-17 19:13:18,475 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderState
datanode_2  | 2020-04-17 19:13:18,517 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:13:18,545 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-LeaderElection1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8: set configuration 0: [5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:13:18,560 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-128988F51F35 with new leaderId: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_2  | 2020-04-17 19:13:18,560 [grpc-default-executor-0] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: change Leader from null to 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 at term 1 for appendEntries, leader elected after 5028ms
datanode_2  | 2020-04-17 19:13:18,662 [grpc-default-executor-0] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:13:18,663 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:13:18,760 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-B75E7E880CD8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/77b1ef9a-a408-42d5-adf2-b75e7e880cd8/current/log_inprogress_0
datanode_2  | 2020-04-17 19:13:18,779 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/current/log_inprogress_0
datanode_2  | 2020-04-17 19:13:33,244 [ChunkWriter-32-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:5848534542888.
datanode_2  | 2020-04-17 19:13:33,340 [ChunkWriter-32-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=644153344 B) is less than the container size (=1073741824 B).
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:13:33,448 [ChunkWriter-32-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: ContainerID 1 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 1 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal HTTP/s3g@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.s3g.keytab.
kdc_1       | Entry for principal HTTP/s3g@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/HTTP.s3g.keytab.
kdc_1       | Generiting keytab
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | WARNING: no policy specified for testuser/s3g@EXAMPLE.COM; defaulting to no policy
kdc_1       | Principal "testuser/s3g@EXAMPLE.COM" created.
kdc_1       | Authenticating as principal admin/admin with keytab /tmp/admin.keytab.
kdc_1       | Entry for principal testuser/s3g@EXAMPLE.COM with kvno 2, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser.s3g.keytab.
kdc_1       | Entry for principal testuser/s3g@EXAMPLE.COM with kvno 2, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/data/testuser.s3g.keytab.
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_create_principal, s3g/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, s3g/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_principal, s3g/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_create_principal, HTTP/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, HTTP/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_principal, HTTP/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_policy, default, Policy does not exist, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_create_principal, testuser/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1, vers=4, flavor=6
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_randkey_principal, testuser/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](Notice): Request: kadm5_get_principal, testuser/s3g@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/admin@EXAMPLE.COM, addr=127.0.0.1
kdc_1       | Apr 17 19:12:33 kdc kadmind[13](info): closing down fd 18
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: SERVER_NOT_FOUND: admin/admin@EXAMPLE.COM for kadmin/localhost@EXAMPLE.COM, Server not found in Kerberos database
kdc_1       | Apr 17 19:12:33 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 127.0.0.1: ISSUE: authtime 1587150753, etypes {rep=18 tkt=18 ses=18}, admin/admin@EXAMPLE.COM for kadmin/admin@EXAMPLE.COM
kdc_1       | Apr 17 19:12:36 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150756, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:41 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.3: ISSUE: authtime 1587150761, etypes {rep=18 tkt=18 ses=18}, dn/d43c26201e9a@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
datanode_3  | 2020-04-17 19:13:13,154 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:13,154 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:13:13,154 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35
datanode_3  | 2020-04-17 19:13:13,154 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:13:13,154 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:13:13,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:13,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:13:13,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:13:13,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:13:13,155 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:13:13,156 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:13:13,159 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:13:13,160 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:13:13,160 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:13:13,160 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:13:13,161 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:13:13,161 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:13:13,161 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:13:13,161 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:13:13,162 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:13:13,162 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:13:13,162 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:13:13,162 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:13:13,163 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:13:13,163 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:13:15,108 [grpc-default-executor-0] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-32FEC35F1454->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35, cid=0, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:13:15,476 [grpc-default-executor-0] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-63E23B34838C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35, cid=0, seq=0, RW, null, Add:group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
om_1        | Sleeping for 5 seconds
om_1        | Setting up kerberos!!
om_1        | KDC ISSUER_SERVER => kdc:8081
om_1        | Sleeping for 5 seconds
om_1        | Got 200, KDC service ready!!
om_1        | Download om/om@EXAMPLE.COM keytab file to /etc/security/keytabs/om.keytab
om_1        | --2020-04-17 19:12:32--  http://kdc:8081/keytab/om/om
om_1        | Resolving kdc (kdc)... 172.24.0.4
om_1        | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
om_1        | HTTP request sent, awaiting response... 200 OK
om_1        | Length: 138 [application/octet-stream]
om_1        | Saving to: '/etc/security/keytabs/om.keytab'
om_1        | 
om_1        |      0K                                                       100% 31.3M=0s
om_1        | 
om_1        | 2020-04-17 19:12:32 (31.3 MB/s) - '/etc/security/keytabs/om.keytab' saved [138/138]
om_1        | 
om_1        | Keytab name: FILE:/etc/security/keytabs/om.keytab
om_1        | KVNO Timestamp         Principal
om_1        | ---- ----------------- --------------------------------------------------------
om_1        |    2 04/17/20 19:12:32 om/om@EXAMPLE.COM
om_1        |    2 04/17/20 19:12:32 om/om@EXAMPLE.COM
om_1        | Download HTTP/om@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
om_1        | --2020-04-17 19:12:32--  http://kdc:8081/keytab/om/HTTP
om_1        | Resolving kdc (kdc)... 172.24.0.4
om_1        | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
om_1        | HTTP request sent, awaiting response... 200 OK
om_1        | Length: 142 [application/octet-stream]
om_1        | Saving to: '/etc/security/keytabs/HTTP.keytab'
om_1        | 
om_1        |      0K                                                       100% 20.5M=0s
om_1        | 
om_1        | 2020-04-17 19:12:32 (20.5 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [142/142]
om_1        | 
om_1        | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
om_1        | KVNO Timestamp         Principal
om_1        | ---- ----------------- --------------------------------------------------------
om_1        |    2 04/17/20 19:12:32 HTTP/om@EXAMPLE.COM
om_1        |    2 04/17/20 19:12:32 HTTP/om@EXAMPLE.COM
om_1        | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
om_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1        | 2020-04-17 19:12:40,599 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1        | /************************************************************
om_1        | STARTUP_MSG: Starting OzoneManager
om_1        | STARTUP_MSG:   host = om/172.24.0.8
om_1        | STARTUP_MSG:   args = [--init]
om_1        | STARTUP_MSG:   version = 3.2.0
datanode_1  | 2020-04-17 19:13:19,384 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for becomeLeader, leader elected after 5668ms
datanode_1  | 2020-04-17 19:13:19,386 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-04-17 19:13:19,386 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2020-04-17 19:13:19,393 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839
datanode_1  | 2020-04-17 19:13:19,404 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:13:19,404 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1  | 2020-04-17 19:13:19,413 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2020-04-17 19:13:19,414 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-04-17 19:13:19,423 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-04-17 19:13:19,443 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderState
datanode_1  | 2020-04-17 19:13:19,455 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:13:19,460 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2b1d3056-312e-49c7-a29e-aa7f6fa89839/current/log_inprogress_0
datanode_1  | 2020-04-17 19:13:19,470 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839-LeaderElection1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-AA7F6FA89839: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858], old=null at 0
datanode_1  | 2020-04-17 19:13:33,033 [ChunkWriter-22-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:5848534542888.
datanode_1  | 2020-04-17 19:13:33,218 [ChunkWriter-22-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=644177920 B) is less than the container size (=1073741824 B).
datanode_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:13:33,300 [ChunkWriter-22-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: ContainerID 1 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 1 creation failed
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:13:33,355 [ChunkWriter-22-0] ERROR ratis.ContainerStateMachine: group-128988F51F35: writeChunk writeStateMachineData failed: blockIdcontainerID: 1
datanode_1  | localID: 104015515565883392
datanode_1  | blockCommitSequenceId: 0
datanode_1  |  logIndex 1 chunkName 104015515565883392_chunk_1 Error message: ContainerID 1 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1  | 2020-04-17 19:13:33,403 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : ContainerID 1 creation failed
datanode_1  | 2020-04-17 19:13:33,630 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om_1        | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om_1        | STARTUP_MSG:   java = 11.0.6
om_1        | ************************************************************/
om_1        | 2020-04-17 19:12:40,640 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1        | 2020-04-17 19:12:45,559 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1        | 2020-04-17 19:12:45,811 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.24.0.8:9862
om_1        | 2020-04-17 19:12:45,815 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1        | WARNING: An illegal reflective access operation has occurred
om_1        | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
om_1        | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
om_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om_1        | WARNING: All illegal access operations will be denied in a future release
om_1        | 2020-04-17 19:12:46,921 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file /etc/security/keytabs/om.keytab
om_1        | 2020-04-17 19:12:46,927 [main] INFO om.OzoneManager: Ozone Manager login successful.
om_1        | 2020-04-17 19:12:46,930 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2020-04-17 19:12:48,737 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:49,738 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:50,739 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:51,740 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:52,741 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:53,742 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:54,743 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:55,744 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:56,745 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:57,746 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1        | 2020-04-17 19:12:59,456 [main] INFO om.OzoneManager: Initializing secure OzoneManager.
om_1        | 2020-04-17 19:13:01,105 [main] ERROR client.OMCertificateClient: Default certificate serial id is not set. Can't locate the default certificate for this client.
om_1        | 2020-04-17 19:13:01,105 [main] INFO client.OMCertificateClient: Certificate client init case: 0
om_1        | 2020-04-17 19:13:01,106 [main] INFO client.OMCertificateClient: Creating keypair for client as keypair and certificate not found.
om_1        | 2020-04-17 19:13:02,609 [main] INFO om.OzoneManager: Init response: GETCERT
om_1        | 2020-04-17 19:13:02,733 [main] INFO ozone.OzoneSecurityUtil: Adding ip:172.24.0.8,host:om
om_1        | 2020-04-17 19:13:02,741 [main] INFO ozone.OzoneSecurityUtil: ip:127.0.0.1 not returned.
om_1        | 2020-04-17 19:13:02,745 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1        | 2020-04-17 19:13:02,751 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.24.0.8:9862
om_1        | 2020-04-17 19:13:02,755 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:13:33,495 [ChunkWriter-32-0] ERROR ratis.ContainerStateMachine: group-128988F51F35: writeChunk writeStateMachineData failed: blockIdcontainerID: 1
datanode_2  | localID: 104015515565883392
datanode_2  | blockCommitSequenceId: 0
datanode_2  |  logIndex 1 chunkName 104015515565883392_chunk_1 Error message: ContainerID 1 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2  | 2020-04-17 19:13:33,553 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : ContainerID 1 creation failed
datanode_2  | 2020-04-17 19:13:33,708 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
datanode_1  | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
datanode_1  | 2020-04-17 19:13:45,321 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #1 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:15:04,323 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Completed APPEND_ENTRIES, lastRequest: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92->6acf0e94-e616-400e-845a-8f99bcdd25f3#8-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=2
datanode_1  | 2020-04-17 19:15:04,401 [grpc-default-executor-1] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5447026C702A:java.util.concurrent.CompletableFuture@5d9ec0a3[Not completed]
datanode_1  | 2020-04-17 19:15:04,402 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:15:04,403 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:15:04,404 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a does not exist. Creating ...
datanode_1  | 2020-04-17 19:15:04,405 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/in_use.lock acquired by nodename 6@d43c26201e9a
datanode_1  | 2020-04-17 19:15:04,407 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a has been successfully formatted.
datanode_1  | 2020-04-17 19:15:04,407 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5447026C702A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:15:04,407 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:15:04,407 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:15:04,408 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:15:04,409 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:15:04,412 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:15:04,412 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:15:04,413 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:15:04,414 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:15:04,414 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:15:04,414 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:15:04,421 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:15:04,421 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:15:04,615 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove  FOLLOWER 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35:t1, leader=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_1  | 2020-04-17 19:15:04,616 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: shutdown
datanode_1  | 2020-04-17 19:15:04,616 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:15:04,616 [Command processor thread] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:15:04,616 [Command processor thread] INFO impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater: set stopIndex = 0
datanode_1  | 2020-04-17 19:15:04,617 [Thread-30] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1  | 2020-04-17 19:15:04,618 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:15:04,619 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:15:04,619 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:15:04,620 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:15:04,620 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:15:04,621 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35: closes. applyIndex: 0
datanode_1  | 2020-04-17 19:15:04,623 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 2020-04-17 19:15:04,624 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35-SegmentedRaftLogWorker close()
datanode_1  | 2020-04-17 19:15:04,626 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:15:04,626 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:15:04,626 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-128988F51F35
datanode_1  | 2020-04-17 19:15:04,628 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  |  command on datanode #6acf0e94-e616-400e-845a-8f99bcdd25f3.
datanode_1  | 2020-04-17 19:15:04,629 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,629 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
kdc_1       | Apr 17 19:12:43 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.6: ISSUE: authtime 1587150763, etypes {rep=18 tkt=18 ses=18}, recon/recon@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:44 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.7: ISSUE: authtime 1587150764, etypes {rep=18 tkt=18 ses=18}, dn/1877e902e348@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:46 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.8: ISSUE: authtime 1587150766, etypes {rep=18 tkt=18 ses=18}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:46 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.10: ISSUE: authtime 1587150766, etypes {rep=18 tkt=18 ses=18}, dn/1dab03dd0801@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:51 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.5: ISSUE: authtime 1587150771, etypes {rep=18 tkt=18 ses=18}, scm/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:12:58 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.10: ISSUE: authtime 1587150766, etypes {rep=18 tkt=18 ses=18}, dn/1dab03dd0801@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:12:58 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.6: ISSUE: authtime 1587150763, etypes {rep=18 tkt=18 ses=18}, recon/recon@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:12:58 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.8: ISSUE: authtime 1587150766, etypes {rep=18 tkt=18 ses=18}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:12:58 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.7: ISSUE: authtime 1587150764, etypes {rep=18 tkt=18 ses=18}, dn/1877e902e348@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:12:58 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.3: ISSUE: authtime 1587150761, etypes {rep=18 tkt=18 ses=18}, dn/d43c26201e9a@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:12:59 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150756, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:13:04 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150784, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:07 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.10: ISSUE: authtime 1587150766, etypes {rep=18 tkt=18 ses=18}, dn/1dab03dd0801@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1       | Apr 17 19:13:08 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.7: ISSUE: authtime 1587150764, etypes {rep=18 tkt=18 ses=18}, dn/1877e902e348@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1       | Apr 17 19:13:08 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.3: ISSUE: authtime 1587150761, etypes {rep=18 tkt=18 ses=18}, dn/d43c26201e9a@EXAMPLE.COM for recon/recon@EXAMPLE.COM
kdc_1       | Apr 17 19:13:09 kdc krb5kdc[8](info): AS_REQ (2 etypes {18 17}) 172.24.0.8: ISSUE: authtime 1587150789, etypes {rep=18 tkt=18 ses=18}, om/om@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:10 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.8: ISSUE: authtime 1587150789, etypes {rep=18 tkt=18 ses=18}, om/om@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:13:10 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150784, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:13:14 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150794, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:19 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150794, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for scm/scm@EXAMPLE.COM
kdc_1       | Apr 17 19:13:21 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150801, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:27 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150807, etypes {rep=18 tkt=18 ses=18}, HTTP/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:27 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150807, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:13:30 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150807, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:11 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:15:13 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:15 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:17 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:18 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.6: ISSUE: authtime 1587150763, etypes {rep=18 tkt=18 ses=18}, recon/recon@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:20 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:22 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:24 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:27 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:29 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:31 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:15:34 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587150911, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:08 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:17:10 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:04,629 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,629 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:04,629 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,630 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:04,630 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,630 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:04,630 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,630 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:04,631 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:04,631 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:09,566 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_1  | 2020-04-17 19:15:09,566 [grpc-default-executor-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:15:09,566 [grpc-default-executor-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:15:09,566 [Thread-90] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1  | 2020-04-17 19:15:09,664 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5447026C702A with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_1  | 2020-04-17 19:15:09,665 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for appendEntries, leader elected after 5257ms
datanode_1  | 2020-04-17 19:15:09,671 [grpc-default-executor-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_1  | 2020-04-17 19:15:09,671 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:15:09,673 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/current/log_inprogress_0
datanode_1  | 2020-04-17 19:15:35,407 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-128988F51F35:null
datanode_1  | 2020-04-17 19:15:35,408 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-128988F51F35 not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:15:36,082 [ChunkWriter-36-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=496017408 B) is less than the container size (=1073741824 B).
datanode_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:15:36,083 [ChunkWriter-36-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
om_1        | 2020-04-17 19:13:02,763 [main] INFO om.OzoneManager: Creating csr for OM->dns:om,ip:172.24.0.8,scmId:8dd19a15-6d05-4567-942d-75cb3f5936b4,clusterId:CID-cc34edd6-d48d-49e7-b4a0-a2799f2024a1,subject:root@om
om_1        | 2020-04-17 19:13:03,166 [main] INFO om.OzoneManager: OzoneManager ports added:[name: "RPC"
om_1        | value: 9862
om_1        | ]
om_1        | 2020-04-17 19:13:03,458 [main] INFO om.OzoneManager: Successfully stored SCM signed certificate.
om_1        | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-cc34edd6-d48d-49e7-b4a0-a2799f2024a1
om_1        | 2020-04-17 19:13:03,629 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1        | /************************************************************
om_1        | SHUTDOWN_MSG: Shutting down OzoneManager at om/172.24.0.8
om_1        | ************************************************************/
om_1        | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
om_1        | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1        | 2020-04-17 19:13:07,071 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1        | /************************************************************
om_1        | STARTUP_MSG: Starting OzoneManager
om_1        | STARTUP_MSG:   host = om/172.24.0.8
om_1        | STARTUP_MSG:   args = []
om_1        | STARTUP_MSG:   version = 3.2.0
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-128988F51F35:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:13:15,789 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | .
datanode_3  | 2020-04-17 19:13:18,169 [Thread-27] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-FollowerState: change to CANDIDATE, lastRpcTime:5006ms, electionTimeout:5005ms
datanode_3  | 2020-04-17 19:13:18,169 [Thread-25] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-FollowerState: change to CANDIDATE, lastRpcTime:5070ms, electionTimeout:5054ms
datanode_3  | 2020-04-17 19:13:18,171 [Thread-27] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:13:18,171 [Thread-25] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:13:18,172 [Thread-25] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2020-04-17 19:13:18,171 [Thread-27] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2020-04-17 19:13:18,174 [Thread-25] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start LeaderElection
datanode_3  | 2020-04-17 19:13:18,174 [Thread-27] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start LeaderElection
datanode_3  | 2020-04-17 19:13:18,191 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.LeaderElection: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1: begin an election at term 1 for -1: [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858], old=null
datanode_3  | 2020-04-17 19:13:18,192 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown LeaderElection
datanode_3  | 2020-04-17 19:13:18,193 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2020-04-17 19:13:18,193 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0257130A8D8A with new leaderId: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:13:18,194 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.LeaderElection: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:13:18,201 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: change Leader from null to 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 at term 1 for becomeLeader, leader elected after 5334ms
datanode_3  | 2020-04-17 19:13:18,218 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2020-04-17 19:13:18,223 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2020-04-17 19:13:18,225 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A
datanode_3  | 2020-04-17 19:13:18,231 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:13:18,232 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3  | 2020-04-17 19:13:18,249 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3  | 2020-04-17 19:13:18,251 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2020-04-17 19:13:18,252 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2020-04-17 19:13:18,302 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start LeaderState
om_1        | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om_1        | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om_1        | STARTUP_MSG:   java = 11.0.6
om_1        | ************************************************************/
om_1        | 2020-04-17 19:13:07,085 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1        | 2020-04-17 19:13:09,265 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1        | 2020-04-17 19:13:09,346 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.24.0.8:9862
om_1        | 2020-04-17 19:13:09,349 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1        | 2020-04-17 19:13:09,353 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | WARNING: An illegal reflective access operation has occurred
om_1        | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
om_1        | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
om_1        | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
om_1        | WARNING: All illegal access operations will be denied in a future release
om_1        | 2020-04-17 19:13:09,669 [main] INFO security.UserGroupInformation: Login successful for user om/om@EXAMPLE.COM using keytab file /etc/security/keytabs/om.keytab
om_1        | 2020-04-17 19:13:09,670 [main] INFO om.OzoneManager: Ozone Manager login successful.
om_1        | 2020-04-17 19:13:09,670 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2020-04-17 19:13:11,333 [main] INFO client.OMCertificateClient: Loading certificate from location:/data/metadata/om/certs.
om_1        | 2020-04-17 19:13:11,406 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/5848534542888.crt.
om_1        | 2020-04-17 19:13:11,413 [main] INFO client.OMCertificateClient: Added certificate from file:/data/metadata/om/certs/CA-1.crt.
om_1        | 2020-04-17 19:13:11,437 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1        | 2020-04-17 19:13:11,632 [main] INFO Configuration.deprecation: No unit for ozone.manager.delegation.remover.scan.interval(3600000) assuming MILLISECONDS
om_1        | 2020-04-17 19:13:11,638 [main] INFO security.OzoneSecretStore: Loaded 0 tokens
om_1        | 2020-04-17 19:13:11,638 [main] INFO security.OzoneDelegationTokenSecretManager: Loading token state into token manager.
om_1        | 2020-04-17 19:13:11,679 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1        | 2020-04-17 19:13:11,685 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1        | 2020-04-17 19:13:11,892 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1        | 2020-04-17 19:13:11,989 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1        | 2020-04-17 19:13:11,989 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1        | 2020-04-17 19:13:12,023 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.8:9862
om_1        | 2020-04-17 19:13:12,024 [main] INFO om.OzoneManager: Reading keypair and certificate from file system.
om_1        | 2020-04-17 19:13:12,034 [main] INFO om.OzoneManager: Starting OM block token secret manager
om_1        | 2020-04-17 19:13:12,034 [main] INFO security.OzoneBlockTokenSecretManager: Updating the current master key for generating tokens
om_1        | 2020-04-17 19:13:12,035 [main] INFO om.OzoneManager: Starting OM delegation token secret manager
om_1        | 2020-04-17 19:13:12,035 [main] INFO security.OzoneDelegationTokenSecretManager: Updating the current master key for generating tokens
om_1        | 2020-04-17 19:13:12,036 [Thread[Thread-11,5,main]] INFO security.OzoneDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
om_1        | 2020-04-17 19:13:12,046 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1        | 2020-04-17 19:13:12,047 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1        | 2020-04-17 19:13:12,116 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1        | 2020-04-17 19:13:12,135 [main] INFO util.log: Logging initialized @8046ms to org.eclipse.jetty.util.log.Slf4jLog
om_1        | 2020-04-17 19:13:12,228 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1        | 2020-04-17 19:13:12,232 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1        | 2020-04-17 19:13:12,237 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1        | 2020-04-17 19:13:12,239 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1        | 2020-04-17 19:13:12,240 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
om_1        | 2020-04-17 19:13:12,240 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
om_1        | 2020-04-17 19:13:12,293 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1        | 2020-04-17 19:13:12,295 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
om_1        | 2020-04-17 19:13:12,322 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1        | 2020-04-17 19:13:12,323 [main] INFO server.session: No SessionScavenger set, using defaults
om_1        | 2020-04-17 19:13:12,324 [main] INFO server.session: node0 Scavenging every 600000ms
om_1        | 2020-04-17 19:13:12,333 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@60f77af{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1        | 2020-04-17 19:13:12,333 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11d86b9d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1        | 2020-04-17 19:13:12,518 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1320e68a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-0_6_0-SNAPSHOT_jar-_-any-2954065510697586459.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1        | 2020-04-17 19:13:12,529 [main] INFO server.AbstractConnector: Started ServerConnector@40ed1802{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1        | 2020-04-17 19:13:12,529 [main] INFO server.Server: Started @8441ms
om_1        | 2020-04-17 19:13:12,533 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1        | 2020-04-17 19:13:12,533 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1        | 2020-04-17 19:13:12,543 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1        | 2020-04-17 19:13:17,099 [qtp1743702241-139] INFO om.OMDBCheckpointServlet: Received request to obtain OM DB checkpoint snapshot
om_1        | 2020-04-17 19:13:17,118 [qtp1743702241-139] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1587150797100 in 17 milliseconds
om_1        | 2020-04-17 19:13:17,167 [qtp1743702241-139] INFO om.OMDBCheckpointServlet: Time taken to write the checkpoint to response output stream: 38 milliseconds
om_1        | 2020-04-17 19:13:17,168 [qtp1743702241-139] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1587150797100
om_1        | 2020-04-17 19:13:30,167 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:13:30,183 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:13:30,910 [IPC Server handler 1 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-0-86893 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:13:30,953 [IPC Server handler 5 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-1-21965 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:13:30,965 [IPC Server handler 3 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-2-83924 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:13:30,972 [IPC Server handler 17 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-3-30707 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:13:30,978 [IPC Server handler 14 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-4-15690 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:14:18,073 [qtp1743702241-138] INFO om.OMDBCheckpointServlet: Received request to obtain OM DB checkpoint snapshot
om_1        | 2020-04-17 19:14:18,091 [qtp1743702241-138] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1587150858074 in 17 milliseconds
om_1        | 2020-04-17 19:14:18,099 [qtp1743702241-138] INFO om.OMDBCheckpointServlet: Time taken to write the checkpoint to response output stream: 6 milliseconds
om_1        | 2020-04-17 19:14:18,099 [qtp1743702241-138] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1587150858074
om_1        | 2020-04-17 19:15:05,456 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:05,464 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:13,130 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:13,147 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:15,182 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:15,199 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:15,536 [IPC Server handler 60 on 9862] INFO volume.OMVolumeCreateRequest: created volume:86893-rpcwoport for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:15:17,490 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:17,498 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:18,342 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:18,357 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:20,034 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:20,052 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:22,548 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:22,560 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
datanode_2  | 2020-04-17 19:13:44,523 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #1 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:15:04,328 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Completed APPEND_ENTRIES, lastRequest: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92->5f2fec2e-3f74-4923-9dbd-94817f14424e#8-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=2
datanode_2  | 2020-04-17 19:15:04,439 [grpc-default-executor-0] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5447026C702A:java.util.concurrent.CompletableFuture@2c0094bb[Not completed]
datanode_2  | 2020-04-17 19:15:04,441 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:15:04,441 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:15:04,442 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:15:04,443 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:15:04,443 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a does not exist. Creating ...
datanode_2  | 2020-04-17 19:15:04,444 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:15:04,445 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a has been successfully formatted.
datanode_2  | 2020-04-17 19:15:04,448 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5447026C702A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:15:04,450 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:15:04,450 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:15:04,450 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:15:04,450 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:15:04,454 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:15:04,454 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:15:04,454 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:15:04,455 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:15:04,456 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:15:04,456 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:15:04,458 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:15:04,459 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:15:04,459 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:15:04,459 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:15:04,710 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove  FOLLOWER 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35:t1, leader=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_2  | 2020-04-17 19:15:04,712 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: shutdown
datanode_2  | 2020-04-17 19:15:04,712 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:15:04,712 [Command processor thread] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:15:04,713 [Command processor thread] INFO impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater: set stopIndex = 0
datanode_2  | 2020-04-17 19:15:04,713 [Thread-30] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:15:04,715 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:15:04,716 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:15:04,717 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:15:04,717 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:15:04,717 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:15:04,717 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:15:04,719 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2  | 2020-04-17 19:15:04,720 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35-SegmentedRaftLogWorker close()
datanode_2  | 2020-04-17 19:15:04,720 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:15:04,720 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:15:04,721 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-128988F51F35
datanode_2  | 2020-04-17 19:15:04,723 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  |  command on datanode #5f2fec2e-3f74-4923-9dbd-94817f14424e.
datanode_2  | 2020-04-17 19:15:04,724 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:04,724 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:04,725 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
kdc_1       | Apr 17 19:17:12 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:15 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:17 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:19 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:21 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:24 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:26 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151028, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:27 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:17:29 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:31 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:33 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:35 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:38 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:40 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:43 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:45 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151047, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:17:45 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587151065, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:17:47 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151065, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:21 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:19:23 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:26 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:28 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:30 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:33 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:35 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:37 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:39 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:42 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:19:44 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151161, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:18 kdc krb5kdc[8](info): AS_REQ (8 etypes {18 17 20 19 16 23 25 26}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
kdc_1       | Apr 17 19:21:20 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:22 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:25 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:27 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:29 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:32 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:34 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:36 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
kdc_1       | Apr 17 19:21:39 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
s3g_1       | Sleeping for 5 seconds
s3g_1       | Setting up kerberos!!
s3g_1       | KDC ISSUER_SERVER => kdc:8081
s3g_1       | Sleeping for 5 seconds
s3g_1       | Got 200, KDC service ready!!
s3g_1       | Download s3g/s3g@EXAMPLE.COM keytab file to /etc/security/keytabs/s3g.keytab
s3g_1       | --2020-04-17 19:12:33--  http://kdc:8081/keytab/s3g/s3g
s3g_1       | Resolving kdc (kdc)... 172.24.0.4
s3g_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
s3g_1       | HTTP request sent, awaiting response... 200 OK
s3g_1       | Length: 142 [application/octet-stream]
s3g_1       | Saving to: '/etc/security/keytabs/s3g.keytab'
s3g_1       | 
s3g_1       |      0K                                                       100% 30.8M=0s
s3g_1       | 
s3g_1       | 2020-04-17 19:12:33 (30.8 MB/s) - '/etc/security/keytabs/s3g.keytab' saved [142/142]
s3g_1       | 
s3g_1       | Keytab name: FILE:/etc/security/keytabs/s3g.keytab
s3g_1       | KVNO Timestamp         Principal
s3g_1       | ---- ----------------- --------------------------------------------------------
s3g_1       |    2 04/17/20 19:12:33 s3g/s3g@EXAMPLE.COM
s3g_1       |    2 04/17/20 19:12:33 s3g/s3g@EXAMPLE.COM
s3g_1       | Download HTTP/s3g@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
s3g_1       | --2020-04-17 19:12:33--  http://kdc:8081/keytab/s3g/HTTP
s3g_1       | Resolving kdc (kdc)... 172.24.0.4
s3g_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
s3g_1       | HTTP request sent, awaiting response... 200 OK
s3g_1       | Length: 144 [application/octet-stream]
s3g_1       | Saving to: '/etc/security/keytabs/HTTP.keytab'
s3g_1       | 
s3g_1       |      0K                                                       100% 22.5M=0s
s3g_1       | 
s3g_1       | 2020-04-17 19:12:33 (22.5 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [144/144]
s3g_1       | 
s3g_1       | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
s3g_1       | KVNO Timestamp         Principal
s3g_1       | ---- ----------------- --------------------------------------------------------
s3g_1       |    2 04/17/20 19:12:33 HTTP/s3g@EXAMPLE.COM
s3g_1       |    2 04/17/20 19:12:33 HTTP/s3g@EXAMPLE.COM
s3g_1       | Download testuser/s3g@EXAMPLE.COM keytab file to /etc/security/keytabs/testuser.keytab
s3g_1       | --2020-04-17 19:12:33--  http://kdc:8081/keytab/s3g/testuser
s3g_1       | Resolving kdc (kdc)... 172.24.0.4
s3g_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
s3g_1       | HTTP request sent, awaiting response... 200 OK
s3g_1       | Length: 152 [application/octet-stream]
s3g_1       | Saving to: '/etc/security/keytabs/testuser.keytab'
s3g_1       | 
s3g_1       |      0K                                                       100% 32.9M=0s
s3g_1       | 
s3g_1       | 2020-04-17 19:12:33 (32.9 MB/s) - '/etc/security/keytabs/testuser.keytab' saved [152/152]
s3g_1       | 
s3g_1       | Keytab name: FILE:/etc/security/keytabs/testuser.keytab
s3g_1       | KVNO Timestamp         Principal
s3g_1       | ---- ----------------- --------------------------------------------------------
s3g_1       |    2 04/17/20 19:12:33 testuser/s3g@EXAMPLE.COM
s3g_1       |    2 04/17/20 19:12:33 testuser/s3g@EXAMPLE.COM
s3g_1       | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
s3g_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1       | WARNING: An illegal reflective access operation has occurred
s3g_1       | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
s3g_1       | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
s3g_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1       | WARNING: All illegal access operations will be denied in a future release
s3g_1       | 2020-04-17 19:12:41,841 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1       | 2020-04-17 19:12:41,986 [main] INFO util.log: Logging initialized @8027ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1       | 2020-04-17 19:12:42,722 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1       | 2020-04-17 19:12:42,844 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
kdc_1       | Apr 17 19:21:41 kdc krb5kdc[8](info): TGS_REQ (6 etypes {18 17 20 19 16 23}) 172.24.0.5: ISSUE: authtime 1587151278, etypes {rep=18 tkt=18 ses=18}, testuser/scm@EXAMPLE.COM for om/om@EXAMPLE.COM
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:15:36,084 [ChunkWriter-36-0] ERROR ratis.ContainerStateMachine: group-5447026C702A: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_1  | localID: 104015523656892417
datanode_1  | blockCommitSequenceId: 0
datanode_1  |  logIndex 1 chunkName 104015523656892417_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1  | 2020-04-17 19:15:36,084 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : ContainerID 2 creation failed
datanode_1  | 2020-04-17 19:15:36,146 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
datanode_1  | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
datanode_1  | 2020-04-17 19:16:05,407 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:17:07,143 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Completed APPEND_ENTRIES, lastRequest: 5f2fec2e-3f74-4923-9dbd-94817f14424e->6acf0e94-e616-400e-845a-8f99bcdd25f3#13-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=2
datanode_1  | 2020-04-17 19:17:07,149 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove  FOLLOWER 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A:t1, leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_1  | 2020-04-17 19:17:07,149 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: shutdown
datanode_1  | 2020-04-17 19:17:07,149 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:17:07,149 [Command processor thread] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:17:07,149 [Command processor thread] INFO impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater: set stopIndex = 0
datanode_1  | 2020-04-17 19:17:07,150 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:17:07,150 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:17:07,150 [Thread-93] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1  | 2020-04-17 19:17:07,151 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:17:07,151 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:17:07,151 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:17:07,152 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:15:04,725 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:04,725 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:04,725 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:04,726 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:04,726 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:04,726 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:04,726 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:04,727 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:04,727 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:09,536 [Thread-89] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-FollowerState: change to CANDIDATE, lastRpcTime:5077ms, electionTimeout:5077ms
datanode_2  | 2020-04-17 19:15:09,537 [Thread-89] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:15:09,537 [Thread-89] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-04-17 19:15:09,537 [Thread-89] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderElection
datanode_2  | 2020-04-17 19:15:09,544 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:15:09,582 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2: Election PASSED; received 1 response(s) [5f2fec2e-3f74-4923-9dbd-94817f14424e<-6acf0e94-e616-400e-845a-8f99bcdd25f3#0:OK-t1] and 0 exception(s); 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A:t1, leader=null, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:15:09,582 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown LeaderElection
datanode_2  | 2020-04-17 19:15:09,583 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2020-04-17 19:15:09,583 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5447026C702A with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:15:09,583 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for becomeLeader, leader elected after 5133ms
datanode_2  | 2020-04-17 19:15:09,587 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2020-04-17 19:15:09,587 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2020-04-17 19:15:09,587 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:15:09,596 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:15:09,600 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2  | 2020-04-17 19:15:09,601 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2020-04-17 19:15:09,601 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2020-04-17 19:15:09,601 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2020-04-17 19:15:09,609 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
s3g_1       | 2020-04-17 19:12:42,911 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1       | 2020-04-17 19:12:42,913 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1       | 2020-04-17 19:12:42,919 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1       | 2020-04-17 19:12:42,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1       | 2020-04-17 19:12:43,056 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1       | 2020-04-17 19:12:43,091 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1       | 2020-04-17 19:12:43,096 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
s3g_1       | 2020-04-17 19:12:43,236 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1       | 2020-04-17 19:12:43,239 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1       | 2020-04-17 19:12:43,241 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1       | 2020-04-17 19:12:43,310 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@338c99c8{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1       | 2020-04-17 19:12:43,319 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1fa1cab1{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1       | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1       | Apr 17, 2020 7:12:54 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1       | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1       | 
s3g_1       | 2020-04-17 19:12:54,341 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5aa781f2{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-0_6_0-SNAPSHOT_jar-_-any-8704657058860323282.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-0.6.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1       | 2020-04-17 19:12:54,359 [main] INFO server.AbstractConnector: Started ServerConnector@7331196b{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1       | 2020-04-17 19:12:54,359 [main] INFO server.Server: Started @20401ms
s3g_1       | 2020-04-17 19:12:54,379 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode_3  | 2020-04-17 19:13:18,353 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.LeaderElection: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2: Election PASSED; received 1 response(s) [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92<-5f2fec2e-3f74-4923-9dbd-94817f14424e#0:OK-t1] and 0 exception(s); 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35:t1, leader=null, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:13:18,360 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown LeaderElection
datanode_3  | 2020-04-17 19:13:18,361 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3  | 2020-04-17 19:13:18,361 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-128988F51F35 with new leaderId: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:13:18,366 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: change Leader from null to 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 at term 1 for becomeLeader, leader elected after 5215ms
datanode_3  | 2020-04-17 19:13:18,366 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3  | 2020-04-17 19:13:18,367 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3  | 2020-04-17 19:13:18,367 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:13:18,375 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:13:18,376 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3  | 2020-04-17 19:13:18,376 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3  | 2020-04-17 19:13:18,376 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3  | 2020-04-17 19:13:18,376 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3  | 2020-04-17 19:13:18,386 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3  | 2020-04-17 19:13:18,386 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:13:18,387 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3  | 2020-04-17 19:13:18,389 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3  | 2020-04-17 19:13:18,400 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2020-04-17 19:13:18,400 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:13:18,417 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3  | 2020-04-17 19:13:18,426 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:13:18,416 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:13:18,426 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3  | 2020-04-17 19:13:18,460 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3  | 2020-04-17 19:13:18,460 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3  | 2020-04-17 19:13:18,462 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-LeaderElection1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A: set configuration 0: [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858], old=null at 0
datanode_3  | 2020-04-17 19:13:18,467 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:13:18,487 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start LeaderState
datanode_3  | 2020-04-17 19:13:18,488 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:13:18,498 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-LeaderElection2] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_3  | 2020-04-17 19:13:18,672 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a3d68f16-2060-40fb-8f52-128988f51f35/current/log_inprogress_0
datanode_3  | 2020-04-17 19:13:18,687 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-0257130A8D8A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/56f8c321-7697-48d5-9d4e-0257130a8d8a/current/log_inprogress_0
datanode_3  | 2020-04-17 19:13:32,800 [ChunkWriter-5-0] INFO client.DNCertificateClient: Getting certificate with certSerialId:5848534542888.
recon_1     | Sleeping for 5 seconds
recon_1     | Setting up kerberos!!
recon_1     | KDC ISSUER_SERVER => kdc:8081
recon_1     | Sleeping for 5 seconds
recon_1     | Got 200, KDC service ready!!
recon_1     | Download recon/recon@EXAMPLE.COM keytab file to /etc/security/keytabs/recon.keytab
recon_1     | --2020-04-17 19:12:31--  http://kdc:8081/keytab/recon/recon
recon_1     | Resolving kdc (kdc)... 172.24.0.4
recon_1     | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
recon_1     | HTTP request sent, awaiting response... 200 OK
recon_1     | Length: 150 [application/octet-stream]
recon_1     | Saving to: '/etc/security/keytabs/recon.keytab'
recon_1     | 
recon_1     |      0K                                                       100% 25.1M=0s
recon_1     | 
recon_1     | 2020-04-17 19:12:31 (25.1 MB/s) - '/etc/security/keytabs/recon.keytab' saved [150/150]
recon_1     | 
recon_1     | Keytab name: FILE:/etc/security/keytabs/recon.keytab
recon_1     | KVNO Timestamp         Principal
recon_1     | ---- ----------------- --------------------------------------------------------
recon_1     |    2 04/17/20 19:12:31 recon/recon@EXAMPLE.COM
recon_1     |    2 04/17/20 19:12:31 recon/recon@EXAMPLE.COM
recon_1     | Download HTTP/recon@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
recon_1     | --2020-04-17 19:12:31--  http://kdc:8081/keytab/recon/HTTP
recon_1     | Resolving kdc (kdc)... 172.24.0.4
recon_1     | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
recon_1     | HTTP request sent, awaiting response... 200 OK
recon_1     | Length: 148 [application/octet-stream]
recon_1     | Saving to: '/etc/security/keytabs/HTTP.keytab'
recon_1     | 
recon_1     |      0K                                                       100% 23.9M=0s
recon_1     | 
recon_1     | 2020-04-17 19:12:31 (23.9 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [148/148]
recon_1     | 
recon_1     | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
recon_1     | KVNO Timestamp         Principal
recon_1     | ---- ----------------- --------------------------------------------------------
recon_1     |    2 04/17/20 19:12:31 HTTP/recon@EXAMPLE.COM
recon_1     |    2 04/17/20 19:12:31 HTTP/recon@EXAMPLE.COM
recon_1     | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
recon_1     | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1     | WARNING: An illegal reflective access operation has occurred
recon_1     | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1     | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1     | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1     | WARNING: All illegal access operations will be denied in a future release
recon_1     | 2020-04-17 19:12:39,226 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1     | 2020-04-17 19:12:40,960 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1     | 2020-04-17 19:12:42,385 [main] INFO recon.ReconServer: Ozone security is enabled. Attempting login for Recon service. Principal: recon/recon@EXAMPLE.COM, keytab: /etc/security/keytabs/recon.keytab
recon_1     | 2020-04-17 19:12:43,840 [main] INFO security.UserGroupInformation: Login successful for user recon/recon@EXAMPLE.COM using keytab file /etc/security/keytabs/recon.keytab
recon_1     | 2020-04-17 19:12:43,855 [main] INFO recon.ReconServer: Recon login successful.
recon_1     | 2020-04-17 19:12:46,610 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1     | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1     | 2020-04-17 19:12:49,699 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1     | 2020-04-17 19:12:49,734 [main] INFO util.log: Logging initialized @18088ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1     | 2020-04-17 19:12:50,237 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1     | 2020-04-17 19:12:50,260 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1     | 2020-04-17 19:12:50,284 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1     | 2020-04-17 19:12:50,286 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1     | 2020-04-17 19:12:50,288 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1     | 2020-04-17 19:12:50,288 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
recon_1     | 2020-04-17 19:12:50,650 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1     | 2020-04-17 19:12:51,634 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1     | 2020-04-17 19:12:52,480 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2020-04-17 19:12:52,483 [main] INFO Configuration.deprecation: No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1     | 2020-04-17 19:12:53,081 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2020-04-17 19:12:53,116 [main] INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@b791a81
recon_1     | 2020-04-17 19:12:53,128 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1     | 2020-04-17 19:12:53,304 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1     | 2020-04-17 19:12:53,321 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2020-04-17 19:12:53,354 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1     | 2020-04-17 19:12:53,450 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1     | 2020-04-17 19:12:53,518 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1     | 2020-04-17 19:12:53,747 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2020-04-17 19:12:53,789 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1     | 2020-04-17 19:12:53,797 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_1  | 2020-04-17 19:17:07,152 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 2020-04-17 19:17:07,154 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A-SegmentedRaftLogWorker close()
datanode_1  | 2020-04-17 19:17:07,155 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:17:07,156 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:17:07,156 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5447026C702A
datanode_1  | 2020-04-17 19:17:07,160 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  |  command on datanode #6acf0e94-e616-400e-845a-8f99bcdd25f3.
datanode_1  | 2020-04-17 19:17:07,160 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-15CB4D7AF47F:java.util.concurrent.CompletableFuture@1fc0a561[Not completed]
datanode_1  | 2020-04-17 19:17:07,163 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:17:07,164 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f does not exist. Creating ...
datanode_1  | 2020-04-17 19:17:07,171 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/in_use.lock acquired by nodename 6@d43c26201e9a
datanode_1  | 2020-04-17 19:17:07,173 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f has been successfully formatted.
datanode_1  | 2020-04-17 19:17:07,173 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-15CB4D7AF47F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:17:07,173 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:17:07,182 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:17:07,183 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:17:07,183 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:17:07,183 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:17:07,183 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:17:07,183 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:17:07,184 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f
datanode_1  | 2020-04-17 19:17:07,185 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:17:07,185 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:17:07,185 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:17:07,191 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:17:07,192 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:17:07,192 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:17:07,192 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:17:07,192 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:17:07,192 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:17:07,193 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:17:07,198 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:17:07,203 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:13:32,954 [ChunkWriter-5-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=644198400 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:13:33,008 [ChunkWriter-5-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0 , Message: ContainerID 1 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 1 creation failed
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:13:33,025 [ChunkWriter-5-0] ERROR ratis.ContainerStateMachine: group-128988F51F35: writeChunk writeStateMachineData failed: blockIdcontainerID: 1
datanode_3  | localID: 104015515565883392
datanode_3  | blockCommitSequenceId: 0
datanode_3  |  logIndex 1 chunkName 104015515565883392_chunk_1 Error message: ContainerID 1 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 2020-04-17 19:13:33,094 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : ContainerID 1 creation failed
datanode_3  | 2020-04-17 19:13:33,306 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
datanode_3  | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
datanode_3  | 2020-04-17 19:13:44,147 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #1 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:14:32,728 [java.util.concurrent.ThreadPoolExecutor$Worker@14620655[State = -1, empty queue]] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=7,entriesCount=1,lastEntry=(t:1, i:1)
datanode_3  | 2020-04-17 19:14:32,729 [java.util.concurrent.ThreadPoolExecutor$Worker@14620655[State = -1, empty queue]] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=7,entriesCount=1,lastEntry=(t:1, i:1)
datanode_3  | 2020-04-17 19:14:32,764 [java.util.concurrent.ThreadPoolExecutor$Worker@14620655[State = -1, empty queue]] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=8,entriesCount=1,lastEntry=(t:1, i:2)
datanode_3  | 2020-04-17 19:14:32,777 [java.util.concurrent.ThreadPoolExecutor$Worker@14620655[State = -1, empty queue]] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=8,entriesCount=1,lastEntry=(t:1, i:2)
datanode_3  | 2020-04-17 19:15:04,315 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove    LEADER 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35:t1, leader=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_3  | 2020-04-17 19:15:04,317 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: shutdown
datanode_3  | 2020-04-17 19:15:04,317 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-128988F51F35,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:15:04,318 [Command processor thread] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown LeaderState
datanode_3  | 2020-04-17 19:15:04,318 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$449/0x00000008405f5440@5d7bd176] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3  | 2020-04-17 19:15:04,318 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$449/0x00000008405f5440@208b5072] WARN server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_3  | 2020-04-17 19:15:04,325 [grpc-default-executor-2] INFO server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->6acf0e94-e616-400e-845a-8f99bcdd25f3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3  | 2020-04-17 19:15:04,318 [Command processor thread] INFO impl.PendingRequests: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-PendingRequests: sendNotLeaderResponses
datanode_3  | 2020-04-17 19:15:04,332 [grpc-default-executor-1] INFO server.GrpcLogAppender: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->5f2fec2e-3f74-4923-9dbd-94817f14424e-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3  | 2020-04-17 19:15:04,334 [grpc-default-executor-2] INFO impl.FollowerInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->6acf0e94-e616-400e-845a-8f99bcdd25f3: nextIndex: updateUnconditionally 3 -> 1
datanode_3  | 2020-04-17 19:15:04,347 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:15:04,348 [Command processor thread] INFO impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater: set stopIndex = 0
datanode_3  | 2020-04-17 19:15:04,348 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | Apr 17, 2020 7:15:04 PM org.apache.ratis.thirdparty.io.grpc.netty.NettyServerHandler onStreamError
datanode_3  | WARNING: Stream Error
datanode_3  | org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 3
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:147)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:591)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:239)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:422)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:174)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:505)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:444)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:283)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1421)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697)
datanode_1  | 2020-04-17 19:17:07,205 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:17:07,205 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:17:07,205 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:17:07,206 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:17:07,206 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:17:07,329 [grpc-default-executor-1] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-EC3053DE135E->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F, cid=3, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:17:07,340 [grpc-default-executor-1] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-108DC3CD3E18->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F, cid=5, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_2  | 2020-04-17 19:15:09,609 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:15:09,610 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-04-17 19:15:09,616 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-04-17 19:15:09,618 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-17 19:15:09,618 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-17 19:15:09,625 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:15:09,626 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderState
datanode_2  | 2020-04-17 19:15:09,627 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
datanode_3  | 	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 
datanode_3  | 2020-04-17 19:15:04,348 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 2020-04-17 19:17:07,351 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | .
datanode_1  | 2020-04-17 19:17:07,352 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,352 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:07,352 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,352 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:07,353 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,353 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:07,353 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,354 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:07,354 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,354 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 2020-04-17 19:15:09,628 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/current/log_inprogress_0
datanode_2  | 2020-04-17 19:15:09,636 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-LeaderElection2] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:15:09,639 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-   LEADER: Withhold vote from candidate 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 with term 1. State: leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, term=1, lastRpcElapsed=null
datanode_2  | 2020-04-17 19:15:35,446 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-128988F51F35:null
datanode_2  | 2020-04-17 19:15:35,447 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-128988F51F35 not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:15:36,051 [ChunkWriter-46-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=496021504 B) is less than the container size (=1073741824 B).
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:15:36,055 [ChunkWriter-46-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:15:36,059 [ChunkWriter-46-0] ERROR ratis.ContainerStateMachine: group-5447026C702A: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_2  | localID: 104015523656892417
datanode_2  | blockCommitSequenceId: 0
datanode_2  |  logIndex 1 chunkName 104015523656892417_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2  | 2020-04-17 19:15:36,059 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : ContainerID 2 creation failed
datanode_2  | 2020-04-17 19:15:36,138 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
datanode_2  | 2020-04-17 19:15:40,583 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:16:36,080 [java.util.concurrent.ThreadPoolExecutor$Worker@1db735b1[State = -1, empty queue]] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=12,entriesCount=1,lastEntry=(t:1, i:1)
datanode_2  | 2020-04-17 19:16:36,082 [java.util.concurrent.ThreadPoolExecutor$Worker@1db735b1[State = -1, empty queue]] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=12,entriesCount=1,lastEntry=(t:1, i:1)
datanode_2  | 2020-04-17 19:16:36,144 [java.util.concurrent.ThreadPoolExecutor$Worker@1db735b1[State = -1, empty queue]] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=13,entriesCount=1,lastEntry=(t:1, i:2)
datanode_2  | 2020-04-17 19:16:36,154 [java.util.concurrent.ThreadPoolExecutor$Worker@1db735b1[State = -1, empty queue]] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=13,entriesCount=1,lastEntry=(t:1, i:2)
datanode_2  | 2020-04-17 19:17:07,140 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove    LEADER 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A:t1, leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_2  | 2020-04-17 19:17:07,140 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: shutdown
datanode_2  | 2020-04-17 19:17:07,140 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:17:07,140 [Command processor thread] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown LeaderState
datanode_2  | 2020-04-17 19:17:07,141 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$547/0x0000000840670440@d78fa7d] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->6acf0e94-e616-400e-845a-8f99bcdd25f3-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2  | 2020-04-17 19:17:07,144 [Command processor thread] INFO impl.PendingRequests: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-PendingRequests: sendNotLeaderResponses
datanode_2  | 2020-04-17 19:17:07,145 [grpc-default-executor-2] INFO server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->6acf0e94-e616-400e-845a-8f99bcdd25f3-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_2  | 2020-04-17 19:17:07,145 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$547/0x0000000840670440@2cb0b210] WARN server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2  | 2020-04-17 19:17:07,155 [grpc-default-executor-1] INFO server.GrpcLogAppender: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_2  | 2020-04-17 19:17:07,156 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:17:07,157 [Command processor thread] INFO impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater: set stopIndex = 0
datanode_2  | 2020-04-17 19:17:07,157 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:17:07,158 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:17:07,158 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:17:07,158 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:17:07,159 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:17:07,181 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:17:07,184 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2  | 2020-04-17 19:17:07,195 [grpc-default-executor-2] INFO impl.FollowerInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->6acf0e94-e616-400e-845a-8f99bcdd25f3: nextIndex: updateUnconditionally 3 -> 1
datanode_2  | Apr 17, 2020 7:17:07 PM org.apache.ratis.thirdparty.io.grpc.netty.NettyServerHandler onStreamError
datanode_2  | WARNING: Stream Error
datanode_2  | org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 3
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:147)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:15:04,358 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:15:04,358 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-128988F51F35 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:15:04,359 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:15:04,359 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35: closes. applyIndex: 0
datanode_3  | 2020-04-17 19:15:04,358 [grpc-default-executor-1] INFO impl.FollowerInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35->5f2fec2e-3f74-4923-9dbd-94817f14424e: nextIndex: updateUnconditionally 3 -> 1
datanode_3  | 2020-04-17 19:15:04,361 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3  | 2020-04-17 19:15:04,363 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35-SegmentedRaftLogWorker close()
datanode_3  | 2020-04-17 19:15:04,363 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:15:04,364 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:15:04,364 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-128988F51F35
datanode_3  | 2020-04-17 19:15:04,366 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  |  command on datanode #3850e7c9-74c1-4e1e-9c77-287bd4a1bb92.
datanode_3  | 2020-04-17 19:15:04,370 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:15:04,370 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:15:04,371 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:15:04,372 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:15:04,372 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a does not exist. Creating ...
datanode_3  | 2020-04-17 19:15:04,373 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-5447026C702A:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5447026C702A:java.util.concurrent.CompletableFuture@651df351[Not completed]
datanode_3  | 2020-04-17 19:15:04,374 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:15:04,375 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a has been successfully formatted.
datanode_3  | 2020-04-17 19:15:04,375 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5447026C702A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:15:04,376 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:15:04,377 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1     | 2020-04-17 19:12:53,905 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1     | 2020-04-17 19:12:53,939 [main] INFO scm.ReconScmTask: Registered MissingContainerTask task 
recon_1     | 2020-04-17 19:12:53,941 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1     | 2020-04-17 19:12:53,943 [main] INFO recon.ReconServer: Starting Recon server
recon_1     | 2020-04-17 19:12:54,156 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1     | 2020-04-17 19:12:54,293 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1     | 2020-04-17 19:12:54,293 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1     | 2020-04-17 19:12:54,703 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1     | 2020-04-17 19:12:54,705 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
recon_1     | 2020-04-17 19:12:54,764 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1     | 2020-04-17 19:12:54,764 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1     | 2020-04-17 19:12:54,765 [main] INFO server.session: node0 Scavenging every 600000ms
recon_1     | 2020-04-17 19:12:54,812 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7fd8c559{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1     | 2020-04-17 19:12:54,815 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@772861aa{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1     | 2020-04-17 19:12:56,815 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7bda01da{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-0_6_0-SNAPSHOT_jar-_-any-16271379668284047270.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-0.6.0-SNAPSHOT.jar!/webapps/recon}
recon_1     | 2020-04-17 19:12:56,833 [main] INFO server.AbstractConnector: Started ServerConnector@15b82644{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1     | 2020-04-17 19:12:56,834 [main] INFO server.Server: Started @25188ms
recon_1     | 2020-04-17 19:12:56,838 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1     | 2020-04-17 19:12:56,838 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1     | 2020-04-17 19:12:56,844 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1     | 2020-04-17 19:12:56,844 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1     | 2020-04-17 19:12:56,857 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1     | 2020-04-17 19:12:56,885 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1     | 2020-04-17 19:12:56,888 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1     | 2020-04-17 19:12:56,888 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1     | 2020-04-17 19:12:56,890 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1     | 2020-04-17 19:12:56,892 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1     | 2020-04-17 19:12:57,985 [main] INFO ipc.Client: Retrying connect to server: scm/172.24.0.5:9860. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
recon_1     | 2020-04-17 19:13:00,091 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1     | 2020-04-17 19:13:00,096 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1     | 2020-04-17 19:13:00,096 [main] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1     | 2020-04-17 19:13:00,164 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1     | 2020-04-17 19:13:00,129 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1     | 2020-04-17 19:13:00,364 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1     | 2020-04-17 19:13:00,364 [main] INFO scm.ReconScmTask: Starting MissingContainerTask Thread.
recon_1     | 2020-04-17 19:13:00,379 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1     | 2020-04-17 19:13:00,383 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 18 milliseconds.
recon_1     | 2020-04-17 19:13:01,438 [MissingContainerTask] INFO fsck.MissingContainerTask: Missing Container task Thread took 1071 milliseconds for processing 0 containers.
recon_1     | 2020-04-17 19:13:08,005 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:13:08,012 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:08,096 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:13:08,120 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:08,448 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:13:08,480 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:09,828 [IPC Server handler 4 on 9891] INFO net.NetworkTopology: Added a new node: /default-rack/3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
recon_1     | 2020-04-17 19:13:09,829 [IPC Server handler 4 on 9891] INFO node.SCMNodeManager: Registered Data node : 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:13:09,834 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92 to Node DB.
recon_1     | 2020-04-17 19:13:09,896 [IPC Server handler 3 on 9891] INFO net.NetworkTopology: Added a new node: /default-rack/5f2fec2e-3f74-4923-9dbd-94817f14424e
recon_1     | 2020-04-17 19:13:09,896 [IPC Server handler 3 on 9891] INFO node.SCMNodeManager: Registered Data node : 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:13:09,897 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5f2fec2e-3f74-4923-9dbd-94817f14424e to Node DB.
recon_1     | 2020-04-17 19:13:10,368 [IPC Server handler 6 on 9891] INFO net.NetworkTopology: Added a new node: /default-rack/6acf0e94-e616-400e-845a-8f99bcdd25f3
recon_1     | 2020-04-17 19:13:10,368 [IPC Server handler 6 on 9891] INFO node.SCMNodeManager: Registered Data node : 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:13:10,368 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 6acf0e94-e616-400e-845a-8f99bcdd25f3 to Node DB.
recon_1     | 2020-04-17 19:13:12,996 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=56f8c321-7697-48d5-9d4e-0257130a8d8a. Trying to get from SCM.
recon_1     | 2020-04-17 19:13:13,067 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 56f8c321-7697-48d5-9d4e-0257130a8d8a, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.092Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:13:13,094 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 56f8c321-7697-48d5-9d4e-0257130a8d8a, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.092Z]
recon_1     | 2020-04-17 19:13:13,147 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:13:13,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.393Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:13:13,153 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:13,153 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:13:13,233 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=77b1ef9a-a408-42d5-adf2-b75e7e880cd8. Trying to get from SCM.
recon_1     | 2020-04-17 19:13:13,241 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 77b1ef9a-a408-42d5-adf2-b75e7e880cd8, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.019Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:13:13,243 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 77b1ef9a-a408-42d5-adf2-b75e7e880cd8, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.019Z]
recon_1     | 2020-04-17 19:13:13,243 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=77b1ef9a-a408-42d5-adf2-b75e7e880cd8 reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:13:13,248 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 77b1ef9a-a408-42d5-adf2-b75e7e880cd8, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:13:10.019Z] moved to OPEN state
recon_1     | 2020-04-17 19:13:13,555 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:13:14,168 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=2b1d3056-312e-49c7-a29e-aa7f6fa89839. Trying to get from SCM.
recon_1     | 2020-04-17 19:13:14,171 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 2b1d3056-312e-49c7-a29e-aa7f6fa89839, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.383Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:13:14,172 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2b1d3056-312e-49c7-a29e-aa7f6fa89839, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.383Z]
recon_1     | 2020-04-17 19:13:14,172 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=2b1d3056-312e-49c7-a29e-aa7f6fa89839 reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:13:14,172 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2b1d3056-312e-49c7-a29e-aa7f6fa89839, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:13:10.383Z] moved to OPEN state
recon_1     | 2020-04-17 19:13:14,371 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:15:04,378 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:15:04,379 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:15:04,386 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:15:04,386 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:15:04,387 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:15:04,387 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:15:04,387 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:15:04,387 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:15:04,387 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:15:04,388 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:15:04,388 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:15:04,388 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:15:04,388 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:15:04,389 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:15:04,389 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:15:04,470 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | .
datanode_3  | 2020-04-17 19:15:04,470 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,470 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:04,471 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,471 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:04,471 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,471 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
scm_1       | Sleeping for 5 seconds
scm_1       | Setting up kerberos!!
scm_1       | KDC ISSUER_SERVER => kdc:8081
scm_1       | Sleeping for 5 seconds
scm_1       | Got 200, KDC service ready!!
scm_1       | Download scm/scm@EXAMPLE.COM keytab file to /etc/security/keytabs/scm.keytab
scm_1       | --2020-04-17 19:12:30--  http://kdc:8081/keytab/scm/scm
scm_1       | Resolving kdc (kdc)... 172.24.0.4
scm_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
scm_1       | HTTP request sent, awaiting response... 200 OK
scm_1       | Length: 142 [application/octet-stream]
scm_1       | Saving to: '/etc/security/keytabs/scm.keytab'
scm_1       | 
scm_1       |      0K                                                       100% 26.0M=0s
scm_1       | 
scm_1       | 2020-04-17 19:12:30 (26.0 MB/s) - '/etc/security/keytabs/scm.keytab' saved [142/142]
scm_1       | 
scm_1       | Keytab name: FILE:/etc/security/keytabs/scm.keytab
scm_1       | KVNO Timestamp         Principal
scm_1       | ---- ----------------- --------------------------------------------------------
scm_1       |    2 04/17/20 19:12:30 scm/scm@EXAMPLE.COM
scm_1       |    2 04/17/20 19:12:30 scm/scm@EXAMPLE.COM
scm_1       | Download HTTP/scm@EXAMPLE.COM keytab file to /etc/security/keytabs/HTTP.keytab
scm_1       | --2020-04-17 19:12:30--  http://kdc:8081/keytab/scm/HTTP
scm_1       | Resolving kdc (kdc)... 172.24.0.4
scm_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
scm_1       | HTTP request sent, awaiting response... 200 OK
scm_1       | Length: 144 [application/octet-stream]
scm_1       | Saving to: '/etc/security/keytabs/HTTP.keytab'
scm_1       | 
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:04,472 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,472 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:04,472 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,472 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:04,473 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-128988F51F35:null
datanode_3  | 2020-04-17 19:15:04,473 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "a3d68f16-2060-40fb-8f52-128988f51f35"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-128988F51F35 not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:15:09,584 [Thread-53] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-FollowerState: change to CANDIDATE, lastRpcTime:5196ms, electionTimeout:5195ms
datanode_3  | 2020-04-17 19:15:09,584 [Thread-53] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:15:09,585 [Thread-53] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3  | 2020-04-17 19:15:09,585 [Thread-53] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start LeaderElection
datanode_3  | 2020-04-17 19:15:09,593 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3] INFO impl.LeaderElection: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:15:09,642 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3] INFO impl.LeaderElection: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3: Election REJECTED; received 2 response(s) [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92<-6acf0e94-e616-400e-845a-8f99bcdd25f3#0:FAIL-t1, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92<-5f2fec2e-3f74-4923-9dbd-94817f14424e#0:FAIL-t1] and 0 exception(s); 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A:t1, leader=null, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:15:09,642 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: changes role from CANDIDATE to FOLLOWER at term 1 for DISCOVERED_A_NEW_TERM
datanode_3  | 2020-04-17 19:15:09,642 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown LeaderElection
scm_1       |      0K                                                       100% 31.9M=0s
scm_1       | 
scm_1       | 2020-04-17 19:12:30 (31.9 MB/s) - '/etc/security/keytabs/HTTP.keytab' saved [144/144]
scm_1       | 
scm_1       | Keytab name: FILE:/etc/security/keytabs/HTTP.keytab
scm_1       | KVNO Timestamp         Principal
scm_1       | ---- ----------------- --------------------------------------------------------
scm_1       |    2 04/17/20 19:12:30 HTTP/scm@EXAMPLE.COM
scm_1       |    2 04/17/20 19:12:30 HTTP/scm@EXAMPLE.COM
scm_1       | Download testuser/scm@EXAMPLE.COM keytab file to /etc/security/keytabs/testuser.keytab
scm_1       | --2020-04-17 19:12:30--  http://kdc:8081/keytab/scm/testuser
scm_1       | Resolving kdc (kdc)... 172.24.0.4
scm_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
scm_1       | HTTP request sent, awaiting response... 200 OK
scm_1       | Length: 152 [application/octet-stream]
scm_1       | Saving to: '/etc/security/keytabs/testuser.keytab'
scm_1       | 
scm_1       |      0K                                                       100% 29.0M=0s
scm_1       | 
scm_1       | 2020-04-17 19:12:30 (29.0 MB/s) - '/etc/security/keytabs/testuser.keytab' saved [152/152]
scm_1       | 
scm_1       | Keytab name: FILE:/etc/security/keytabs/testuser.keytab
scm_1       | KVNO Timestamp         Principal
scm_1       | ---- ----------------- --------------------------------------------------------
scm_1       |    2 04/17/20 19:12:30 testuser/scm@EXAMPLE.COM
scm_1       |    2 04/17/20 19:12:30 testuser/scm@EXAMPLE.COM
scm_1       | Download testuser2/scm@EXAMPLE.COM keytab file to /etc/security/keytabs/testuser2.keytab
scm_1       | --2020-04-17 19:12:30--  http://kdc:8081/keytab/scm/testuser2
scm_1       | Resolving kdc (kdc)... 172.24.0.4
scm_1       | Connecting to kdc (kdc)|172.24.0.4|:8081... connected.
scm_1       | HTTP request sent, awaiting response... 200 OK
scm_1       | Length: 154 [application/octet-stream]
scm_1       | Saving to: '/etc/security/keytabs/testuser2.keytab'
scm_1       | 
scm_1       |      0K                                                       100% 28.8M=0s
scm_1       | 
scm_1       | 2020-04-17 19:12:30 (28.8 MB/s) - '/etc/security/keytabs/testuser2.keytab' saved [154/154]
scm_1       | 
scm_1       | Keytab name: FILE:/etc/security/keytabs/testuser2.keytab
scm_1       | KVNO Timestamp         Principal
scm_1       | ---- ----------------- --------------------------------------------------------
scm_1       |    2 04/17/20 19:12:30 testuser2/scm@EXAMPLE.COM
scm_1       |    2 04/17/20 19:12:30 testuser2/scm@EXAMPLE.COM
scm_1       | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2020-04-17 19:12:36,487 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = scm/172.24.0.5
scm_1       | STARTUP_MSG:   args = [--init]
scm_1       | STARTUP_MSG:   version = 3.2.0
recon_1     | 2020-04-17 19:13:16,891 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:13:16,892 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1     | 2020-04-17 19:13:17,262 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1587150796892
recon_1     | 2020-04-17 19:13:17,381 [pool-9-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1587150796892.
recon_1     | 2020-04-17 19:13:17,401 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1     | 2020-04-17 19:13:17,402 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1     | 2020-04-17 19:13:17,448 [pool-10-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1587150797402
recon_1     | 2020-04-17 19:13:17,449 [pool-10-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1587150763858.
recon_1     | 2020-04-17 19:13:17,530 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1     | 2020-04-17 19:13:17,538 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.135 seconds to process 0 keys.
recon_1     | 2020-04-17 19:13:18,056 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:13:18,208 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:13:18,374 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:13:18,374 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z] moved to OPEN state
recon_1     | 2020-04-17 19:13:33,133 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:13:33,161 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:33,186 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 1 creation failed
recon_1     | 2020-04-17 19:13:33,187 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:33,188 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z] moved to CLOSED state
recon_1     | 2020-04-17 19:13:33,317 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
recon_1     | 2020-04-17 19:13:33,318 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:33,398 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | STARTUP_MSG:   java = 11.0.6
scm_1       | ************************************************************/
scm_1       | 2020-04-17 19:12:36,541 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-04-17 19:12:37,048 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-17 19:12:37,339 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-cc34edd6-d48d-49e7-b4a0-a2799f2024a1
scm_1       | 2020-04-17 19:12:37,428 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1       | /************************************************************
scm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at scm/172.24.0.5
scm_1       | ************************************************************/
scm_1       | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
scm_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1       | 2020-04-17 19:12:50,291 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1       | /************************************************************
scm_1       | STARTUP_MSG: Starting StorageContainerManager
scm_1       | STARTUP_MSG:   host = scm/172.24.0.5
scm_1       | STARTUP_MSG:   args = []
scm_1       | STARTUP_MSG:   version = 3.2.0
om_1        | 2020-04-17 19:15:24,826 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:24,842 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:27,113 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:27,126 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:29,523 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:29,535 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:31,825 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:07,354 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-5447026C702A:null
datanode_1  | 2020-04-17 19:17:07,355 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-5447026C702A not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:17:12,289 [Thread-154] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-FollowerState: change to CANDIDATE, lastRpcTime:5083ms, electionTimeout:5080ms
datanode_1  | 2020-04-17 19:17:12,289 [Thread-154] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:17:12,289 [Thread-154] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-04-17 19:17:12,290 [Thread-154] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderElection
datanode_1  | 2020-04-17 19:17:12,296 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:17:12,332 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2: Election PASSED; received 1 response(s) [6acf0e94-e616-400e-845a-8f99bcdd25f3<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:OK-t1] and 0 exception(s); 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F:t1, leader=null, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:17:12,333 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderElection
datanode_1  | 2020-04-17 19:17:12,333 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2020-04-17 19:17:12,333 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-15CB4D7AF47F with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:17:12,334 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for becomeLeader, leader elected after 5160ms
datanode_1  | 2020-04-17 19:17:12,334 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-04-17 19:17:12,339 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2020-04-17 19:17:12,339 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:17:12,339 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:17:12,340 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1  | 2020-04-17 19:17:12,340 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2020-04-17 19:17:12,340 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-04-17 19:17:12,340 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-04-17 19:17:12,350 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:17:12,358 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:17:12,373 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:17:12,377 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:17:12,379 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:17:12,381 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:17:12,387 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:17:12,387 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:17:12,388 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:17:12,388 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:17:12,388 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:17:12,388 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:17:12,390 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderState
datanode_1  | 2020-04-17 19:17:12,391 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:17:12,392 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/current/log_inprogress_0
datanode_1  | 2020-04-17 19:17:12,399 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-LeaderElection2] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_1  | 2020-04-17 19:17:49,172 [ChunkWriter-50-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495591424 B) is less than the container size (=1073741824 B).
datanode_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:17:49,172 [ChunkWriter-50-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:17:49,173 [ChunkWriter-50-0] ERROR ratis.ContainerStateMachine: group-15CB4D7AF47F: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_1  | localID: 104015532387205122
datanode_1  | blockCommitSequenceId: 0
om_1        | 2020-04-17 19:15:31,841 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:15:34,249 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:15:34,270 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:16:18,767 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:16:18,780 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:08,249 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:08,277 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:10,507 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:10,530 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:10,852 [IPC Server handler 80 on 9862] INFO volume.OMVolumeCreateRequest: created volume:86893-rpcwoport2 for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:17:12,747 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:12,763 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:15,101 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:15,131 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:17,291 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:17,300 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:18,913 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:18,925 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:19,719 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:19,734 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:21,835 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:21,851 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:24,326 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:24,338 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:26,652 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:26,667 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:29,100 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:29,113 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:31,348 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:31,394 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:33,821 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:33,833 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:34,154 [IPC Server handler 15 on 9862] ERROR acl.OMBucketAddAclRequest: Add acl [user:superuser1:rwxy[ACCESS]] to path /86893-rpcwoport2/bb1 failed, because acl already exist
om_1        | 2020-04-17 19:17:35,962 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:35,974 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:38,421 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:38,435 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:40,565 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:40,578 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:43,060 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:591)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:239)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:422)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:174)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:505)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:444)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:283)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1421)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:697)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
datanode_2  | 	at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 
datanode_2  | 2020-04-17 19:17:07,197 [grpc-default-executor-1] INFO impl.FollowerInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: nextIndex: updateUnconditionally 3 -> 1
datanode_2  | 2020-04-17 19:17:07,210 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A-SegmentedRaftLogWorker close()
datanode_2  | 2020-04-17 19:17:07,210 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:17:07,210 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:17:07,210 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5447026C702A
datanode_2  | 2020-04-17 19:17:07,211 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  |  command on datanode #5f2fec2e-3f74-4923-9dbd-94817f14424e.
datanode_2  | 2020-04-17 19:17:07,215 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-15CB4D7AF47F:java.util.concurrent.CompletableFuture@79cf55c0[Not completed]
datanode_2  | 2020-04-17 19:17:07,225 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:17:07,225 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:17:07,225 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:17:07,225 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:17:07,225 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:17:07,226 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:17:07,226 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:17:07,226 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:17:07,227 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:17:07,227 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f does not exist. Creating ...
datanode_2  | 2020-04-17 19:17:07,229 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:17:07,232 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f has been successfully formatted.
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-15CB4D7AF47F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:17:07,236 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:17:07,237 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:17:07,237 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f
datanode_2  | 2020-04-17 19:17:07,237 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:17:07,238 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:17:07,239 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:17:07,245 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:17:07,245 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:17:07,268 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-8FF54D60CC05->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F, cid=4, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  |  logIndex 1 chunkName 104015532387205122_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1  | 2020-04-17 19:17:49,173 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : ContainerID 3 creation failed
datanode_1  | 2020-04-17 19:17:49,210 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
recon_1     | 2020-04-17 19:13:33,431 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:33,432 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 1 creation failed
recon_1     | 2020-04-17 19:13:33,433 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:33,561 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:13:33,601 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:13:33,604 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 1 creation failed
recon_1     | 2020-04-17 19:13:33,608 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:33,620 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
recon_1     | 2020-04-17 19:13:33,621 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:13:33,718 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
recon_1     | 2020-04-17 19:13:33,718 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | 2020-04-17 19:14:03,337 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:03,350 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:03,653 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:03,665 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:03,724 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.47.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.3.4.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1       | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1       | STARTUP_MSG:   java = 11.0.6
scm_1       | ************************************************************/
scm_1       | 2020-04-17 19:12:50,325 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1       | 2020-04-17 19:12:50,896 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | WARNING: An illegal reflective access operation has occurred
scm_1       | WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar) to method sun.security.krb5.Config.getInstance()
scm_1       | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
scm_1       | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm_1       | WARNING: All illegal access operations will be denied in a future release
scm_1       | 2020-04-17 19:12:52,338 [main] INFO security.UserGroupInformation: Login successful for user scm/scm@EXAMPLE.COM using keytab file /etc/security/keytabs/scm.keytab
scm_1       | 2020-04-17 19:12:52,348 [main] INFO server.StorageContainerManager: SCM login successful.
scm_1       | 2020-04-17 19:12:52,354 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-17 19:12:56,446 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 200, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-17 19:12:56,488 [Socket Reader #1 for port 9961] INFO ipc.Server: Starting Socket Reader #1 for port 9961
scm_1       | 2020-04-17 19:12:56,634 [main] INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@c3fa05a
scm_1       | 2020-04-17 19:12:56,635 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1       | 2020-04-17 19:12:56,800 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1       | 2020-04-17 19:12:56,977 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
om_1        | 2020-04-17 19:17:43,075 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:45,143 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:45,159 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:17:47,554 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:17:47,570 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:18:19,058 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:18:19,068 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:19,172 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:19,179 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:21,318 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:21,334 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:23,478 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:23,489 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:26,051 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:26,067 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:26,396 [IPC Server handler 53 on 9862] INFO volume.OMVolumeCreateRequest: created volume:86893-rpcwport for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:19:28,358 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:28,375 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:30,722 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:30,747 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:33,096 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:33,104 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:35,529 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:35,547 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:37,852 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:37,867 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:40,019 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:40,034 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:42,447 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:42,462 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:19:44,764 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:19:44,775 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:20:19,194 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:20:19,200 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:18,547 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:18,555 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:19,266 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:19,269 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:20,787 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:20,797 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
datanode_3  | 2020-04-17 19:15:09,642 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-LeaderElection3] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:15:09,658 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5447026C702A with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_3  | 2020-04-17 19:15:09,659 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for appendEntries, leader elected after 5282ms
datanode_3  | 2020-04-17 19:15:09,710 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_3  | 2020-04-17 19:15:09,711 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:15:09,712 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ec5b9ad5-e5fe-47df-a5e5-5447026c702a/current/log_inprogress_0
datanode_3  | 2020-04-17 19:15:36,090 [ChunkWriter-19-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=496009216 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:15:36,091 [ChunkWriter-19-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:15:36,095 [ChunkWriter-19-0] ERROR ratis.ContainerStateMachine: group-5447026C702A: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_3  | localID: 104015523656892417
datanode_3  | blockCommitSequenceId: 0
datanode_3  |  logIndex 1 chunkName 104015523656892417_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 2020-04-17 19:15:36,096 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : ContainerID 2 creation failed
datanode_3  | 2020-04-17 19:15:36,147 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
datanode_3  | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
datanode_3  | 2020-04-17 19:16:05,376 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:17:07,147 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Completed APPEND_ENTRIES, lastRequest: 5f2fec2e-3f74-4923-9dbd-94817f14424e->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#13-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=2
datanode_3  | 2020-04-17 19:17:07,147 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove  FOLLOWER 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A:t1, leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, voted=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_3  | 2020-04-17 19:17:07,151 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: shutdown
datanode_3  | 2020-04-17 19:17:07,151 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-5447026C702A,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:17:07,152 [Command processor thread] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:17:07,152 [Command processor thread] INFO impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater: set stopIndex = 0
datanode_3  | 2020-04-17 19:17:07,153 [Thread-58] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:17:07,153 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:17:07,164 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:17:07,164 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:17:07,165 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-5447026C702A as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:17:07,165 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:17:07,165 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A: closes. applyIndex: 0
datanode_3  | 2020-04-17 19:17:07,165 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3  | 2020-04-17 19:17:07,166 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A-SegmentedRaftLogWorker close()
datanode_3  | 2020-04-17 19:17:07,167 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:17:07,169 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:17:07,169 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5447026C702A
datanode_3  | 2020-04-17 19:17:07,171 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  |  command on datanode #3850e7c9-74c1-4e1e-9c77-287bd4a1bb92.
datanode_3  | 2020-04-17 19:17:07,174 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-15CB4D7AF47F:java.util.concurrent.CompletableFuture@43fdc495[Not completed]
datanode_3  | 2020-04-17 19:17:07,175 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:17:07,176 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f does not exist. Creating ...
scm_1       | 2020-04-17 19:12:56,991 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-17 19:12:57,078 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1       | 2020-04-17 19:12:57,084 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1       | 2020-04-17 19:12:57,135 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1       | 2020-04-17 19:12:57,145 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1       | 2020-04-17 19:12:57,195 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 0 nodes. Healthy nodes 0
scm_1       | 2020-04-17 19:12:57,551 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-17 19:12:57,553 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1       | 2020-04-17 19:12:57,600 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-17 19:12:57,601 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1       | 2020-04-17 19:12:57,662 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1       | 2020-04-17 19:12:57,663 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1       | 2020-04-17 19:12:57,704 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1       | 2020-04-17 19:12:57,725 [main] INFO util.log: Logging initialized @18914ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1       | 2020-04-17 19:12:57,821 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1       | 2020-04-17 19:12:57,869 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1       | 2020-04-17 19:12:57,874 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1       | 2020-04-17 19:12:57,876 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1       | 2020-04-17 19:12:57,876 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
scm_1       | 2020-04-17 19:12:57,876 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1       | 2020-04-17 19:12:57,917 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1       | 2020-04-17 19:12:57,977 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1       | 2020-04-17 19:12:58,038 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1       | 2020-04-17 19:12:58,038 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1       | 2020-04-17 19:12:58,216 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1       | 2020-04-17 19:12:58,219 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-04-17 19:12:58,269 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1       | 2020-04-17 19:12:58,304 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1       | 2020-04-17 19:12:58,305 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1       | 2020-04-17 19:12:58,309 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-04-17 19:12:58,309 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1       | 2020-04-17 19:12:58,345 [main] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1       | 2020-04-17 19:12:58,345 [main] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1       | 2020-04-17 19:12:58,347 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-04-17 19:12:58,347 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1       | 2020-04-17 19:12:58,406 [main] INFO server.SCMClientProtocolServer: Starting RPC server for SCMSecurityProtocolServer. is listening at /0.0.0.0:9961
scm_1       | 2020-04-17 19:12:58,412 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1       | 2020-04-17 19:12:58,412 [IPC Server listener on 9961] INFO ipc.Server: IPC Server listener on 9961: starting
scm_1       | 2020-04-17 19:12:58,417 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1       | 2020-04-17 19:12:58,418 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
scm_1       | 2020-04-17 19:12:58,884 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1       | 2020-04-17 19:12:58,891 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1       | 2020-04-17 19:12:58,893 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1       | 2020-04-17 19:12:58,969 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1db7157f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1       | 2020-04-17 19:12:58,969 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2a53f215{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1       | 2020-04-17 19:12:59,040 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,040 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,040 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,099 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,121 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:12:59,127 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:12:59,127 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:12:59,128 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:12:59,143 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,176 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:12:59,189 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:12:59,236 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:12:59,351 [IPC Server handler 1 on 9961] INFO server.SCMClientProtocolServer: Processing CSR for dn d43c26201e9a, UUID: 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:12:59,356 [IPC Server handler 0 on 9961] INFO server.SCMClientProtocolServer: Processing CSR for dn 1dab03dd0801, UUID: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:12:59,391 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@f5a7226{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-0_6_0-SNAPSHOT_jar-_-any-4934276453018098330.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/scm}
scm_1       | 2020-04-17 19:12:59,407 [main] INFO server.AbstractConnector: Started ServerConnector@5a49af50{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1       | 2020-04-17 19:12:59,408 [main] INFO server.Server: Started @20597ms
scm_1       | 2020-04-17 19:12:59,429 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1       | 2020-04-17 19:12:59,429 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1       | 2020-04-17 19:12:59,435 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1       | 2020-04-17 19:12:59,467 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@69f2cb04] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1       | 2020-04-17 19:12:59,613 [IPC Server handler 1 on 9961] INFO server.SCMClientProtocolServer: Processing CSR for dn 1877e902e348, UUID: 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:13:00,662 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:00,740 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:13:03,313 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:03,331 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:03,338 [IPC Server handler 0 on 9961] INFO server.SCMClientProtocolServer: Processing CSR for om om, UUID: ffcdf3b6-402b-4e6b-aa5a-103293be1083
scm_1       | 2020-04-17 19:13:07,879 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:07,924 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:08,101 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:08,118 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:08,489 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:03,731 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:18,069 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:14:18,070 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1     | 2020-04-17 19:14:18,133 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1587150858070
recon_1     | 2020-04-17 19:14:18,133 [pool-9-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Cleaning up old OM snapshot db at /data/metadata/om.snapshot.db_1587150796892.
recon_1     | 2020-04-17 19:14:18,153 [pool-9-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1587150858070.
recon_1     | 2020-04-17 19:14:18,169 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1     | 2020-04-17 19:14:18,169 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1     | 2020-04-17 19:14:18,188 [pool-10-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1587150858169
recon_1     | 2020-04-17 19:14:18,188 [pool-10-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1587150797402.
recon_1     | 2020-04-17 19:14:18,195 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1     | 2020-04-17 19:14:18,195 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.026 seconds to process 0 keys.
recon_1     | 2020-04-17 19:14:18,288 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:14:33,336 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:33,357 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:33,628 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:33,633 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:33,732 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:14:33,743 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:14:39,193 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z] removed from db
recon_1     | 2020-04-17 19:14:39,320 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:14:39,441 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
om_1        | 2020-04-17 19:21:22,988 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:22,998 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:23,354 [IPC Server handler 24 on 9862] INFO volume.OMVolumeCreateRequest: created volume:86893-rpcwoscheme for user:testuser/scm@EXAMPLE.COM
om_1        | 2020-04-17 19:21:25,169 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:25,183 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:27,598 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:27,607 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:30,014 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:30,025 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:32,323 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:32,335 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:34,622 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:34,634 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:36,791 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:36,802 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:39,089 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:39,101 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:21:41,445 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:21:41,461 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:22:19,284 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:22:19,290 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:23:15,182 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:23:15,187 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for testuser/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
om_1        | 2020-04-17 19:23:19,365 [Socket Reader #1 for port 9862] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
om_1        | 2020-04-17 19:23:19,372 [Socket Reader #1 for port 9862] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol
datanode_1  | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
datanode_1  | 2020-04-17 19:18:13,335 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:18:49,174 [java.util.concurrent.ThreadPoolExecutor$Worker@203bbdf0[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=16,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1  | 2020-04-17 19:18:49,200 [java.util.concurrent.ThreadPoolExecutor$Worker@203bbdf0[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=16,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1  | 2020-04-17 19:18:49,215 [java.util.concurrent.ThreadPoolExecutor$Worker@203bbdf0[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=17,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:18:49,231 [java.util.concurrent.ThreadPoolExecutor$Worker@203bbdf0[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=17,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:19:20,211 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove    LEADER 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_1  | 2020-04-17 19:19:20,211 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: shutdown
datanode_1  | 2020-04-17 19:19:20,211 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:19:20,211 [Command processor thread] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderState
datanode_1  | 2020-04-17 19:19:20,213 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@1f317798] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:19:20,213 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@5f886efb] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:19:20,217 [Command processor thread] INFO impl.PendingRequests: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-PendingRequests: sendNotLeaderResponses
datanode_1  | 2020-04-17 19:19:20,217 [grpc-default-executor-2] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:19:20,219 [grpc-default-executor-2] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:19:20,221 [grpc-default-executor-2] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->5f2fec2e-3f74-4923-9dbd-94817f14424e-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:19:20,231 [grpc-default-executor-2] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F->5f2fec2e-3f74-4923-9dbd-94817f14424e: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:19:20,231 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:19:20,232 [Command processor thread] INFO impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater: set stopIndex = 0
datanode_1  | 2020-04-17 19:19:20,232 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:19:20,232 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:19:20,233 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:19:20,233 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:19:20,245 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:17:07,268 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:17:07,270 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:17:07,270 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:17:07,271 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:17:07,271 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_2  | 2020-04-17 19:17:07,271 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:17:07,178 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:17:07,180 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f has been successfully formatted.
datanode_3  | 2020-04-17 19:17:07,180 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-15CB4D7AF47F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:17:07,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:17:07,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:17:07,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:17:07,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:17:07,181 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:17:07,184 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:17:07,191 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:17:07,191 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f
datanode_3  | 2020-04-17 19:17:07,194 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:17:07,194 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:17:07,196 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:17:07,197 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:17:07,197 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:17:07,204 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:17:07,204 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:17:07,207 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:17:07,207 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:17:07,208 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:17:07,208 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:17:07,209 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:17:07,209 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:17:07,209 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:17:07,209 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:17:07,210 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:17:07,210 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:17:07,211 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:17:07,211 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:17:07,211 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:17:07,211 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:17:07,212 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:17:07,242 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-0E2CE00EBE2A->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F, cid=2, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 2020-04-17 19:19:20,246 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F: closes. applyIndex: 0
datanode_1  | 2020-04-17 19:19:20,247 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 2020-04-17 19:19:20,249 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F-SegmentedRaftLogWorker close()
datanode_1  | 2020-04-17 19:19:20,249 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:19:20,250 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:19:20,250 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-15CB4D7AF47F
datanode_1  | 2020-04-17 19:19:20,251 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  |  command on datanode #6acf0e94-e616-400e-845a-8f99bcdd25f3.
datanode_1  | 2020-04-17 19:19:20,267 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-BD2EB99E7C9C:java.util.concurrent.CompletableFuture@71be8985[Not completed]
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:19:20,269 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:19:20,270 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:19:20,270 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c does not exist. Creating ...
datanode_1  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/in_use.lock acquired by nodename 6@d43c26201e9a
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c has been successfully formatted.
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-BD2EB99E7C9C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:19:20,292 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c
datanode_1  | 2020-04-17 19:19:20,293 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:19:20,295 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:19:20,296 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:19:20,296 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:19:20,301 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:19:20,302 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:19:20,306 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:19:20,306 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:19:20,306 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:19:20,307 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:19:20,307 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
scm_1       | 2020-04-17 19:13:08,511 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:09,829 [IPC Server handler 16 on 9861] INFO net.NetworkTopology: Added a new node: /default-rack/3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:13:09,829 [IPC Server handler 16 on 9861] INFO node.SCMNodeManager: Registered Data node : 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
scm_1       | 2020-04-17 19:13:09,904 [IPC Server handler 14 on 9861] INFO net.NetworkTopology: Added a new node: /default-rack/5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:13:09,932 [IPC Server handler 14 on 9861] INFO node.SCMNodeManager: Registered Data node : 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
scm_1       | 2020-04-17 19:13:09,932 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1       | 2020-04-17 19:13:09,932 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1       | 2020-04-17 19:13:09,954 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-04-17 19:13:09,970 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-04-17 19:13:10,031 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=77b1ef9a-a408-42d5-adf2-b75e7e880cd8 to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:13:10,085 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 77b1ef9a-a408-42d5-adf2-b75e7e880cd8, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.019156Z]
scm_1       | 2020-04-17 19:13:10,092 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=56f8c321-7697-48d5-9d4e-0257130a8d8a to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:13:10,093 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 56f8c321-7697-48d5-9d4e-0257130a8d8a, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.092890Z]
scm_1       | 2020-04-17 19:13:10,094 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 2 nodes. Healthy nodes 2
scm_1       | 2020-04-17 19:13:10,369 [IPC Server handler 13 on 9861] INFO net.NetworkTopology: Added a new node: /default-rack/6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:13:10,369 [IPC Server handler 13 on 9861] INFO node.SCMNodeManager: Registered Data node : 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
scm_1       | 2020-04-17 19:13:10,370 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1       | 2020-04-17 19:13:10,370 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1       | 2020-04-17 19:13:10,383 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:10,383 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2b1d3056-312e-49c7-a29e-aa7f6fa89839 to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:13:10,385 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1       | 2020-04-17 19:13:10,386 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1       | 2020-04-17 19:13:10,386 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2b1d3056-312e-49c7-a29e-aa7f6fa89839, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.383280Z]
scm_1       | 2020-04-17 19:13:10,387 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:13:10,393 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:13:10,393 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:13:10,393 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:13:10,394 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:10,394 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:13:10,427 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:13:10,890 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:10,909 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:11,115 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:11,123 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:13:12,999 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 56f8c321-7697-48d5-9d4e-0257130a8d8a, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.092890Z] moved to OPEN state
scm_1       | 2020-04-17 19:13:13,014 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-04-17 19:13:13,014 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-04-17 19:13:13,037 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:13,055 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:13:13,244 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 77b1ef9a-a408-42d5-adf2-b75e7e880cd8, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:13:10.019156Z] moved to OPEN state
scm_1       | 2020-04-17 19:13:13,245 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-04-17 19:13:13,245 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-04-17 19:13:14,188 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2b1d3056-312e-49c7-a29e-aa7f6fa89839, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:13:10.383280Z] moved to OPEN state
scm_1       | 2020-04-17 19:13:14,188 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-04-17 19:13:14,188 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1       | 2020-04-17 19:13:18,374 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z] moved to OPEN state
scm_1       | 2020-04-17 19:13:18,374 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1       | 2020-04-17 19:13:18,374 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1       | 2020-04-17 19:13:18,383 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1       | 2020-04-17 19:13:18,383 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1       | 2020-04-17 19:13:18,383 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1       | 2020-04-17 19:13:19,067 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:19,094 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:19,224 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:19,229 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for HTTP/scm@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:13:30,472 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:30,476 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:13:31,214 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:31,215 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:13:32,900 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:32,904 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:33,095 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:33,132 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:33,136 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:33,141 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:33,230 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 1 creation failed
scm_1       | 2020-04-17 19:13:33,235 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:33,235 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z] moved to CLOSED state
scm_1       | 2020-04-17 19:13:33,239 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1
scm_1       | 2020-04-17 19:13:33,301 [Socket Reader #1 for port 9961] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:33,309 [Socket Reader #1 for port 9961] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.protocol.SCMSecurityProtocol
scm_1       | 2020-04-17 19:13:33,326 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
scm_1       | 2020-04-17 19:13:33,334 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:33,412 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:33,539 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:33,545 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 1 creation failed
scm_1       | 2020-04-17 19:13:33,554 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:33,583 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:13:33,609 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:13:33,611 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 1 creation failed
scm_1       | 2020-04-17 19:13:33,613 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:33,627 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
scm_1       | 2020-04-17 19:13:33,627 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:13:33,720 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-7FFC354560F8, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "4be730416805fecb:5680fa612f3535b3:4be730416805fecb:0" containerID: 1 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "a3d68f16-2060-40fb-8f52-128988f51f35" writeChunk { blockID { containerID: 1 localID: 104015515565883392 blockCommitSequenceId: 0 } chunkData { chunkName: "104015515565883392_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\221\331;\036" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDEgbG9jSUQ6IDEwNDAxNTUxNTU2NTg4MzM5MhiAmdf1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAL4hbvTMTVyBaJy3z8Vm1atGVrJAe2vu0zG3Zsa104tXSd9r9CpbbXPSZDn1hyvMldmZPSM3PyUuReTa_6N8hKV2w7lB3KYooYfdK01WCgyNYDoRTR48c5OraSoQR4wh2090n9MZ5M896Qd3aJ16-UVxcLdnnu-MciU6XEdO7cuk4S6bGYfy19Jija3IH8LjeNJXqLHxgWB7qp9cwFG_JxAu1yQiRWQboe254XnhwTnyF8Yof53vwZKe8MaRzmorGFyDAUpYrtUtTI0sh-t_POGJPugWT1obwJ3EMHpZpwoNAnnOd1gwWpjBAOOrqvY1znTw94g9M5seRB2IH9sHOgMQSEREU19CTE9DS19UT0tFTiJjb25JRDogMSBsb2NJRDogMTA0MDE1NTE1NTY1ODgzMzky", container path=nonexistent
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:17:07,295 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-168D9C3EE9DE->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F, cid=2, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:17:07,375 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | .
datanode_3  | 2020-04-17 19:17:07,375 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,376 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:07,376 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,376 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:14:39,609 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:14:39,621 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:14:39,719 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:03,339 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:03,372 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
datanode_2  | 2020-04-17 19:17:07,272 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:17:07,272 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:17:07,272 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:17:07,275 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:17:07,275 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_2  | 2020-04-17 19:17:07,291 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-9689A54AB64B->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F, cid=3, seq=0, RW, null, Add:group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-15CB4D7AF47F:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:17:07,346 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | .
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-5447026C702A:null
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-5447026C702A:null
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 2020-04-17 19:19:20,307 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:19:20,307 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:19:20,308 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:19:20,308 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:19:20,308 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:19:20,308 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:19:20,340 [grpc-default-executor-2] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-8AB8D1842033->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C, cid=4, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:19:20,386 [grpc-default-executor-2] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-F325D258E311->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C, cid=7, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
scm_1       | 2020-04-17 19:13:33,722 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | 2020-04-17 19:14:03,353 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:03,368 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:03,642 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:03,652 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:03,725 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:03,737 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:33,339 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:33,352 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:33,645 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:33,650 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:33,739 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:14:33,751 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:14:39,240 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,241 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,241 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,241 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z] removed from db
scm_1       | 2020-04-17 19:14:39,242 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:14:39,243 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,243 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,243 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,243 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:14:39,244 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:14:39,334 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,335 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,335 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,336 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:14:39,555 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,555 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,555 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,556 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:14:39,614 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,614 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,614 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,615 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:14:39,628 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,628 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_1  | 2020-04-17 19:19:20,432 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | .
datanode_1  | 2020-04-17 19:19:20,433 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-15CB4D7AF47F:null
datanode_1  | 2020-04-17 19:19:20,434 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:19:20,435 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-15CB4D7AF47F:null
datanode_1  | 2020-04-17 19:19:20,435 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:19:20,436 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-15CB4D7AF47F:null
datanode_1  | 2020-04-17 19:19:20,436 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:19:20,437 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-15CB4D7AF47F:null
datanode_1  | 2020-04-17 19:19:20,437 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:19:20,437 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-15CB4D7AF47F:null
datanode_1  | 2020-04-17 19:19:20,438 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-15CB4D7AF47F not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:19:25,449 [Thread-179] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-FollowerState: change to CANDIDATE, lastRpcTime:5141ms, electionTimeout:5140ms
datanode_1  | 2020-04-17 19:19:25,450 [Thread-179] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:19:25,450 [Thread-179] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-04-17 19:19:25,450 [Thread-179] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderElection
datanode_1  | 2020-04-17 19:19:25,458 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:19:25,501 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3: Election PASSED; received 1 response(s) [6acf0e94-e616-400e-845a-8f99bcdd25f3<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:OK-t1] and 0 exception(s); 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C:t1, leader=null, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderElection
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BD2EB99E7C9C with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for becomeLeader, leader elected after 5221ms
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-04-17 19:19:25,502 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2020-04-17 19:19:25,503 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:19:25,503 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:19:25,503 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:19:25,504 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:19:25,505 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:19:25,505 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:19:25,505 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:19:25,511 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:19:25,511 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:19:25,512 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:19:25,512 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:19:25,512 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:19:25,512 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:19:25,513 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderState
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:07,376 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,377 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:07,377 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,378 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:07,388 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,388 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:07,388 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-5447026C702A:null
datanode_3  | 2020-04-17 19:17:07,388 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-5447026C702A not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:17:12,318 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:17:12,318 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:17:12,318 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:17:12,319 [Thread-121] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:17:12,439 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-15CB4D7AF47F with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:17:12,439 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5258ms
datanode_3  | 2020-04-17 19:17:12,485 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_3  | 2020-04-17 19:17:12,485 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:17:12,491 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/current/log_inprogress_0
datanode_3  | 2020-04-17 19:17:49,196 [ChunkWriter-33-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 2020-04-17 19:14:39,628 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,628 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:14:39,722 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:14:39,723 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:14:39,723 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:14:39,723 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: a3d68f16-2060-40fb-8f52-128988f51f35, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92, CreationTimestamp2020-04-17T19:13:10.393019Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:14:57,196 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:14:57,196 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:15:03,332 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:03,364 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:03,386 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:03,396 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:15:03,397 [IPC Server handler 0 on 9860] INFO ipc.Server: IPC Server handler 0 on 9860, call Call#6 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:46269
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:15:03,631 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:03,658 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:03,662 [IPC Server handler 4 on 9860] INFO ipc.Server: IPC Server handler 4 on 9860, call Call#7 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:46269
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:15:03,732 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-5447026C702A:null
datanode_2  | 2020-04-17 19:17:07,347 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:17:07,348 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-5447026C702A:null
datanode_2  | 2020-04-17 19:17:07,348 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:17:07,348 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-5447026C702A:null
datanode_2  | 2020-04-17 19:17:07,348 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-5447026C702A not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:17:12,361 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_2  | 2020-04-17 19:17:12,361 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:17:12,361 [Thread-113] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:17:12,361 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:17:12,433 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-15CB4D7AF47F with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_2  | 2020-04-17 19:17:12,434 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5197ms
datanode_2  | 2020-04-17 19:17:12,486 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:17:12,487 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:17:12,488 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6955dc48-6a27-489c-aa23-15cb4d7af47f/current/log_inprogress_0
datanode_2  | 2020-04-17 19:17:49,205 [ChunkWriter-0-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495583232 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:17:49,197 [ChunkWriter-33-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:17:49,197 [ChunkWriter-33-0] ERROR ratis.ContainerStateMachine: group-15CB4D7AF47F: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_3  | localID: 104015532387205122
datanode_3  | blockCommitSequenceId: 0
datanode_3  |  logIndex 1 chunkName 104015532387205122_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 2020-04-17 19:17:49,197 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : ContainerID 3 creation failed
datanode_3  | 2020-04-17 19:17:49,233 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
datanode_3  | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
datanode_3  | 2020-04-17 19:18:08,180 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:19:20,215 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#17-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=2
recon_1     | 2020-04-17 19:15:03,373 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:03,403 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "a3d68f16-2060-40fb-8f52-128988f51f35"
recon_1     | }
recon_1     | isLeader: true
recon_1     | bytesWritten: 0
recon_1     |  from dn=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:03,635 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:03,659 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:15:03,660 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:03,663 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "a3d68f16-2060-40fb-8f52-128988f51f35"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
datanode_1  | 2020-04-17 19:19:25,535 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:19:25,543 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/current/log_inprogress_0
datanode_1  | 2020-04-17 19:19:25,547 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-LeaderElection3] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_1  | 2020-04-17 19:19:25,558 [grpc-default-executor-2] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-   LEADER: Withhold vote from candidate 5f2fec2e-3f74-4923-9dbd-94817f14424e with term 1. State: leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, term=1, lastRpcElapsed=null
datanode_1  | 2020-04-17 19:19:46,415 [ChunkWriter-48-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495239168 B) is less than the container size (=1073741824 B).
datanode_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:19:46,416 [ChunkWriter-48-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:19:46,417 [grpc-default-executor-4] ERROR ratis.ContainerStateMachine: group-BD2EB99E7C9C: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_1  | localID: 104015540068089859
datanode_1  | blockCommitSequenceId: 0
datanode_1  |  logIndex 1 chunkName 104015540068089859_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1  | 2020-04-17 19:19:46,417 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : ContainerID 4 creation failed
datanode_1  | 2020-04-17 19:19:46,456 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
datanode_1  | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
datanode_1  | 2020-04-17 19:19:51,281 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:19:51,281 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:20:46,426 [java.util.concurrent.ThreadPoolExecutor$Worker@1d604d21[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=10,entriesCount=1,lastEntry=(t:1, i:1)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495579136 B) is less than the container size (=1073741824 B).
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:17:49,212 [ChunkWriter-0-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:17:49,212 [ChunkWriter-0-0] ERROR ratis.ContainerStateMachine: group-15CB4D7AF47F: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_2  | localID: 104015532387205122
datanode_2  | blockCommitSequenceId: 0
datanode_2  |  logIndex 1 chunkName 104015532387205122_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2  | 2020-04-17 19:17:49,212 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : ContainerID 3 creation failed
datanode_2  | 2020-04-17 19:17:49,239 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
datanode_2  | 2020-04-17 19:18:08,236 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:19:20,216 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->5f2fec2e-3f74-4923-9dbd-94817f14424e#17-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=2
datanode_2  | 2020-04-17 19:19:20,245 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove  FOLLOWER 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_2  | 2020-04-17 19:19:20,245 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: shutdown
datanode_2  | 2020-04-17 19:19:20,245 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:19:20,245 [Command processor thread] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:19:20,245 [Command processor thread] INFO impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater: set stopIndex = 0
datanode_2  | 2020-04-17 19:19:20,245 [Thread-116] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:19:20,245 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
scm_1       | 2020-04-17 19:15:03,745 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:03,753 [IPC Server handler 3 on 9860] INFO ipc.Server: IPC Server handler 3 on 9860, call Call#8 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:46269
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:15:04,416 [IPC Server handler 1 on 9860] INFO ipc.Server: IPC Server handler 1 on 9860, call Call#10 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:46269
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:15:04,450 [IPC Server handler 4 on 9860] INFO ipc.Server: IPC Server handler 4 on 9860, call Call#11 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:46269
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:15:05,483 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:05,487 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:15:05,488 [IPC Server handler 5 on 9863] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:15:05,488 [IPC Server handler 5 on 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:173)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:196)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:116)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:232)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:200)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:161)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
datanode_2  | 2020-04-17 19:19:20,246 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:19:20,246 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:19:20,246 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:19:20,247 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_2  | 2020-04-17 19:19:20,247 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:19:20,247 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2  | 2020-04-17 19:19:20,248 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F-SegmentedRaftLogWorker close()
datanode_2  | 2020-04-17 19:19:20,248 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:19:20,248 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_2  | 2020-04-17 19:19:20,248 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-15CB4D7AF47F
datanode_2  | 2020-04-17 19:19:20,249 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  |  command on datanode #5f2fec2e-3f74-4923-9dbd-94817f14424e.
datanode_2  | 2020-04-17 19:19:20,252 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:19:20,254 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:19:20,254 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c does not exist. Creating ...
datanode_2  | 2020-04-17 19:19:20,254 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-BD2EB99E7C9C:java.util.concurrent.CompletableFuture@39b7aaae[Not completed]
datanode_2  | 2020-04-17 19:19:20,256 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:19:20,260 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c has been successfully formatted.
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-BD2EB99E7C9C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:19:20,276 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:19:20,277 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:19:20,278 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:19:20,287 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:19:20,331 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-4B58D4634518->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C, cid=6, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 2020-04-17 19:19:20,233 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove  FOLLOWER 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_3  | 2020-04-17 19:19:20,234 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: shutdown
datanode_3  | 2020-04-17 19:19:20,234 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-15CB4D7AF47F,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:19:20,234 [Command processor thread] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:19:20,235 [Thread-125] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:19:20,237 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:19:20,237 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:19:20,237 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:19:20,237 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-15CB4D7AF47F as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:19:20,237 [Command processor thread] INFO impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater: set stopIndex = 0
datanode_3  | 2020-04-17 19:19:20,238 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:19:20,238 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F: closes. applyIndex: 0
datanode_3  | 2020-04-17 19:19:20,239 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3  | 2020-04-17 19:19:20,239 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F-SegmentedRaftLogWorker close()
datanode_3  | 2020-04-17 19:19:20,240 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:19:20,240 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:19:20,240 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-15CB4D7AF47F
datanode_3  | 2020-04-17 19:19:20,241 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  |  command on datanode #3850e7c9-74c1-4e1e-9c77-287bd4a1bb92.
datanode_3  | 2020-04-17 19:19:20,241 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-BD2EB99E7C9C:java.util.concurrent.CompletableFuture@33a19835[Not completed]
datanode_3  | 2020-04-17 19:19:20,244 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:19:20,250 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:19:20,251 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:19:20,251 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:19:20,251 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:19:20,251 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:19:20,252 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:19:20,252 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:19:20,252 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:19:20,253 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c does not exist. Creating ...
datanode_3  | 2020-04-17 19:19:20,255 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:19:20,258 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c has been successfully formatted.
scm_1       | 2020-04-17 19:15:05,489 [IPC Server handler 5 on 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2020-04-17 19:15:09,591 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z] moved to OPEN state
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:03,738 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:03,749 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:15:03,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:03,754 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "a3d68f16-2060-40fb-8f52-128988f51f35"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
scm_1       | 2020-04-17 19:15:13,158 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:13,164 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:15:34,391 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:34,397 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:34,447 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:34,469 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:34,697 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:34,704 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:15:36,108 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 2 creation failed
scm_1       | 2020-04-17 19:15:36,115 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:15:36,119 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z] moved to CLOSED state
scm_1       | 2020-04-17 19:15:36,119 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2
scm_1       | 2020-04-17 19:15:36,121 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 2 creation failed
scm_1       | 2020-04-17 19:15:36,128 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:15:36,133 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:15:36,142 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:15:36,151 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 2 creation failed
scm_1       | 2020-04-17 19:15:36,153 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:15:36,153 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
datanode_1  | 2020-04-17 19:20:46,444 [java.util.concurrent.ThreadPoolExecutor$Worker@1d604d21[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=10,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1  | 2020-04-17 19:20:46,479 [java.util.concurrent.ThreadPoolExecutor$Worker@1d604d21[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=11,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:20:46,480 [java.util.concurrent.ThreadPoolExecutor$Worker@1d604d21[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=11,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:21:17,457 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove    LEADER 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_1  | 2020-04-17 19:21:17,457 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: shutdown
datanode_1  | 2020-04-17 19:21:17,457 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:21:17,457 [Command processor thread] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderState
datanode_1  | 2020-04-17 19:21:17,458 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@37bb8ed3] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:21:17,458 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@646fde9] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:21:17,461 [Command processor thread] INFO impl.PendingRequests: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-PendingRequests: sendNotLeaderResponses
datanode_1  | 2020-04-17 19:21:17,462 [grpc-default-executor-2] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->5f2fec2e-3f74-4923-9dbd-94817f14424e-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:21:17,466 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:21:17,474 [Command processor thread] INFO impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater: set stopIndex = 0
datanode_1  | 2020-04-17 19:21:17,475 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:21:17,475 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:21:17,475 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:21:17,475 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:21:17,475 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:21:17,479 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C: closes. applyIndex: 0
datanode_1  | 2020-04-17 19:21:17,479 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 2020-04-17 19:21:17,480 [grpc-default-executor-2] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->5f2fec2e-3f74-4923-9dbd-94817f14424e: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:21:17,481 [grpc-default-executor-7] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:21:17,482 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C-SegmentedRaftLogWorker close()
datanode_1  | 2020-04-17 19:21:17,483 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:21:17,483 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:21:17,483 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C
datanode_1  | 2020-04-17 19:21:17,483 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  |  command on datanode #6acf0e94-e616-400e-845a-8f99bcdd25f3.
datanode_1  | 2020-04-17 19:21:17,484 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-BD2EB99E7C9C:null
datanode_1  | 2020-04-17 19:21:17,484 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:21:17,484 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-11955824EF5B:java.util.concurrent.CompletableFuture@24ee568c[Not completed]
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:21:17,487 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:21:17,488 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:21:17,488 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:21:17,488 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:21:17,488 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b does not exist. Creating ...
datanode_1  | 2020-04-17 19:21:17,491 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/in_use.lock acquired by nodename 6@d43c26201e9a
datanode_1  | 2020-04-17 19:21:17,494 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b has been successfully formatted.
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-11955824EF5B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:21:17,513 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm_1       | 2020-04-17 19:15:36,153 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:15:36,161 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
scm_1       | 2020-04-17 19:15:36,162 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:15:36,192 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
scm_1       | 2020-04-17 19:15:36,193 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | 2020-04-17 19:16:06,218 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:06,225 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:06,252 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:06,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:06,253 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:06,257 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:36,187 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:36,196 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:36,198 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:36,210 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:16:36,218 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:36,219 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:16:42,119 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,126 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,126 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_3  | 2020-04-17 19:19:20,259 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-BD2EB99E7C9C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:19:20,263 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:19:20,265 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:19:20,271 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:19:20,271 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:19:20,271 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:19:20,271 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:19:20,271 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:19:20,279 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c
datanode_3  | 2020-04-17 19:19:20,280 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:19:20,281 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:19:20,282 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:19:20,282 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:19:20,282 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:19:20,282 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:19:20,283 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:19:20,287 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:19:20,288 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:19:20,289 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:19:20,289 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:19:20,290 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:19:20,290 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:19:20,298 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:19:20,298 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:19:20,394 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-3064A20270D3->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C, cid=5, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:21:17,514 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:21:17,538 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:21:17,538 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm_1       | 2020-04-17 19:16:42,126 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z] removed from db
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:16:42,127 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:16:42,133 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,133 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,133 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,133 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:16:42,153 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,153 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,153 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,154 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:04,379 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:04,381 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:14:39.243Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:15:04,382 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:04,382 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:15:04,415 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:04,417 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "a3d68f16-2060-40fb-8f52-128988f51f35"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:04,417 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:15:04,448 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35. Trying to get from SCM.
recon_1     | 2020-04-17 19:15:04,451 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "a3d68f16-2060-40fb-8f52-128988f51f35"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=a3d68f16-2060-40fb-8f52-128988f51f35 not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:19:20,351 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-D9F3A85C933E->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C, cid=4, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 2020-04-17 19:21:17,538 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:21:17,539 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:21:17,539 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:21:17,539 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:21:17,539 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:21:17,540 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:21:17,540 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:21:17,549 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:21:17,549 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:21:17,503 [grpc-default-executor-7] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-BD2EB99E7C9C->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:21:17,578 [grpc-default-executor-7] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-BE0993D21E1F->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B, cid=8, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:21:17,605 [grpc-default-executor-7] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-B63A760E820C->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B, cid=6, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:16:42,155 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,155 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,155 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,155 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:16:42,162 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,163 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,163 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,163 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:16:42,193 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:16:42,193 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:16:42,193 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:16:42,193 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243136Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:16:57,197 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:16:57,197 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:17:06,225 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:06,230 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:06,235 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:06,236 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:06,249 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:06,263 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:06,277 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:06,279 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:17:06,280 [IPC Server handler 19 on 9860] INFO ipc.Server: IPC Server handler 19 on 9860, call Call#14 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:37251
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:21:17,626 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | .
datanode_1  | 2020-04-17 19:21:17,627 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-BD2EB99E7C9C:null
datanode_1  | 2020-04-17 19:21:17,627 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:21:17,642 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-BD2EB99E7C9C:null
datanode_1  | 2020-04-17 19:21:17,642 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:21:17,643 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-BD2EB99E7C9C:null
datanode_1  | 2020-04-17 19:21:17,643 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:21:17,644 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-BD2EB99E7C9C:null
datanode_1  | 2020-04-17 19:21:17,644 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-BD2EB99E7C9C not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:21:22,586 [Thread-203] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-FollowerState: change to CANDIDATE, lastRpcTime:5045ms, electionTimeout:5036ms
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:17:06,284 [IPC Server handler 30 on 9860] INFO ipc.Server: IPC Server handler 30 on 9860, call Call#15 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:37251
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:19:20,416 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-632281700BB6->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C, cid=5, seq=0, RW, null, Add:group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:19:20,424 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | .
datanode_3  | 2020-04-17 19:19:20,424 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,424 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:20,425 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,425 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:20,425 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,426 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:17:08,291 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:08,304 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:17:08,305 [IPC Server handler 24 on 9863] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:17:08,305 [IPC Server handler 24 on 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:173)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:196)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:116)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:232)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:200)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:161)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:17:08,308 [IPC Server handler 24 on 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2020-04-17 19:17:10,555 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:10,560 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:17:12,380 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z] moved to OPEN state
scm_1       | 2020-04-17 19:17:37,235 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:37,289 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:37,311 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:37,319 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:42,377 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:42,387 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:47,914 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:47,919 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:17:49,194 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 3 creation failed
scm_1       | 2020-04-17 19:17:49,194 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:15:04,452 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:15:09,586 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:15:09,586 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z] moved to OPEN state
recon_1     | 2020-04-17 19:15:18,295 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
datanode_1  | 2020-04-17 19:21:22,586 [Thread-203] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:21:22,586 [Thread-203] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-04-17 19:21:22,586 [Thread-203] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderElection
datanode_1  | 2020-04-17 19:21:22,589 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:21:22,626 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4: Election PASSED; received 1 response(s) [6acf0e94-e616-400e-845a-8f99bcdd25f3<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:OK-t1] and 0 exception(s); 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B:t1, leader=null, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:21:22,626 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderElection
datanode_1  | 2020-04-17 19:21:22,626 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1  | 2020-04-17 19:21:22,626 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-11955824EF5B with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:21:22,638 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for becomeLeader, leader elected after 5118ms
datanode_1  | 2020-04-17 19:21:22,641 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1  | 2020-04-17 19:21:22,641 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1  | 2020-04-17 19:21:22,642 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:21:22,649 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:21:22,650 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1  | 2020-04-17 19:21:22,652 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1  | 2020-04-17 19:21:22,653 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1  | 2020-04-17 19:21:22,655 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1  | 2020-04-17 19:21:22,655 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:21:22,655 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:21:22,657 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:21:22,657 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:21:22,657 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:21:22,657 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1  | 2020-04-17 19:21:22,663 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:21:22,664 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderState
datanode_1  | 2020-04-17 19:21:22,664 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:21:22,668 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/current/log_inprogress_0
datanode_1  | 2020-04-17 19:21:22,681 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-LeaderElection4] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-BD2EB99E7C9C:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:19:20,419 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | .
datanode_2  | 2020-04-17 19:19:20,419 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-15CB4D7AF47F:null
datanode_2  | 2020-04-17 19:19:20,419 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:19:20,420 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-15CB4D7AF47F:null
datanode_2  | 2020-04-17 19:19:20,420 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:19:20,420 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-15CB4D7AF47F:null
datanode_2  | 2020-04-17 19:19:20,420 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:19:20,425 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-15CB4D7AF47F:null
datanode_2  | 2020-04-17 19:19:20,426 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | 
datanode_1  | 2020-04-17 19:21:43,018 [ChunkWriter-57-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=494899200 B) is less than the container size (=1073741824 B).
datanode_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:21:43,020 [ChunkWriter-57-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:21:43,020 [ChunkWriter-57-0] ERROR ratis.ContainerStateMachine: group-11955824EF5B: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_1  | localID: 104015547713323012
datanode_1  | blockCommitSequenceId: 0
datanode_1  |  logIndex 1 chunkName 104015547713323012_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1  | 2020-04-17 19:21:43,020 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : ContainerID 5 creation failed
datanode_1  | 2020-04-17 19:21:43,047 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
datanode_1  | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
datanode_1  | 2020-04-17 19:22:14,049 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #5 does not exist in datanode. Container close failed.
datanode_1  | 2020-04-17 19:22:43,019 [java.util.concurrent.ThreadPoolExecutor$Worker@436f3862[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=10,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1  | 2020-04-17 19:22:43,020 [java.util.concurrent.ThreadPoolExecutor$Worker@436f3862[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=10,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1  | 2020-04-17 19:22:43,047 [java.util.concurrent.ThreadPoolExecutor$Worker@436f3862[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=11,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:22:43,047 [java.util.concurrent.ThreadPoolExecutor$Worker@436f3862[State = -1, empty queue]] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=11,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1  | 2020-04-17 19:23:14,048 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove    LEADER 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:20,427 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,427 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:20,429 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,429 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:20,430 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-15CB4D7AF47F:null
datanode_3  | 2020-04-17 19:19:20,430 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-15CB4D7AF47F not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:19:25,485 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:19:25,485 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:19:25,485 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:19:25,488 [Thread-194] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:19:25,548 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BD2EB99E7C9C with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:19:25,548 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5289ms
datanode_3  | 2020-04-17 19:19:25,570 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:19:20,426 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-15CB4D7AF47F:null
datanode_2  | 2020-04-17 19:19:20,426 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-15CB4D7AF47F not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:19:25,472 [Thread-185] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-FollowerState: change to CANDIDATE, lastRpcTime:5190ms, electionTimeout:5184ms
datanode_2  | 2020-04-17 19:19:25,472 [Thread-185] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:19:25,472 [Thread-185] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-04-17 19:19:25,473 [Thread-185] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderElection
datanode_2  | 2020-04-17 19:19:25,499 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-LeaderElection3] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-LeaderElection3: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:19:25,559 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: changes role from CANDIDATE to FOLLOWER at term 1 for appendEntries
datanode_2  | 2020-04-17 19:19:25,559 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown LeaderElection
datanode_2  | 2020-04-17 19:19:25,567 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:19:25,567 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BD2EB99E7C9C with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_2  | 2020-04-17 19:19:25,568 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5291ms
datanode_2  | 2020-04-17 19:19:25,568 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-LeaderElection3] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-LeaderElection3: Election REJECTED; received 2 response(s) [5f2fec2e-3f74-4923-9dbd-94817f14424e<-6acf0e94-e616-400e-845a-8f99bcdd25f3#0:FAIL-t1, 5f2fec2e-3f74-4923-9dbd-94817f14424e<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:FAIL-t1] and 0 exception(s); 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:19:25,582 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:19:25,582 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:19:25,584 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/current/log_inprogress_0
datanode_2  | 2020-04-17 19:19:46,449 [ChunkWriter-58-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
recon_1     | 2020-04-17 19:15:18,297 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:15:18,483 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1
recon_1     | 2020-04-17 19:15:18,558 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:15:18,744 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:15:34,399 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:34,431 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:15:34,445 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:34,465 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:15:36,087 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 2 creation failed
recon_1     | 2020-04-17 19:15:36,087 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:36,087 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z] moved to CLOSED state
recon_1     | 2020-04-17 19:15:36,110 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 2 creation failed
recon_1     | 2020-04-17 19:15:36,110 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:36,124 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:15:36,156 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
recon_1     | 2020-04-17 19:15:36,157 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:36,160 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
scm_1       | 2020-04-17 19:17:49,194 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z] moved to CLOSED state
scm_1       | 2020-04-17 19:17:49,195 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #3
scm_1       | 2020-04-17 19:17:49,239 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
scm_1       | 2020-04-17 19:17:49,252 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:17:49,253 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:49,299 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:49,300 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 3 creation failed
scm_1       | 2020-04-17 19:17:49,301 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:17:49,305 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
scm_1       | 2020-04-17 19:17:49,305 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:17:49,306 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:17:49,328 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:17:49,329 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 3 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495222784 B) is less than the container size (=1073741824 B).
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:19:46,450 [ChunkWriter-58-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:19:46,451 [ChunkWriter-58-0] ERROR ratis.ContainerStateMachine: group-BD2EB99E7C9C: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_2  | localID: 104015540068089859
datanode_2  | blockCommitSequenceId: 0
datanode_2  |  logIndex 1 chunkName 104015540068089859_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2  | 2020-04-17 19:19:46,451 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : ContainerID 4 creation failed
datanode_2  | 2020-04-17 19:19:46,495 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
datanode_2  | 2020-04-17 19:19:51,260 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:19:51,260 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:21:17,459 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->5f2fec2e-3f74-4923-9dbd-94817f14424e#11-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=2
datanode_2  | 2020-04-17 19:21:17,499 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove  FOLLOWER 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_2  | 2020-04-17 19:21:17,499 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: shutdown
datanode_2  | 2020-04-17 19:21:17,499 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:21:17,499 [Command processor thread] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:21:17,499 [Command processor thread] INFO impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater: set stopIndex = 0
datanode_2  | 2020-04-17 19:21:17,500 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:21:17,500 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
recon_1     | 2020-04-17 19:15:36,160 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:36,187 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:15:36,188 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 2 creation failed
recon_1     | 2020-04-17 19:15:36,189 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:15:36,199 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-6D7502D3392A, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "71cebef137ca83c2:ef83cba59df290ef:71cebef137ca83c2:0" containerID: 2 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a" writeChunk { blockID { containerID: 2 localID: 104015523656892417 blockCommitSequenceId: 0 } chunkData { chunkName: "104015523656892417_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDIgbG9jSUQ6IDEwNDAxNTUyMzY1Njg5MjQxNxiz3d71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAKOjNWKZUJvWKxU91iAm2GeMrQkQJ_I8luWu6iek1yxgmzqw5C0Go0rKvBzYE99Jx94XVHiRdCthelrQlIFB9ChJkb2dPtY6AAPe0-xqbhQhrPK7WYZNMvobX7a4xwR1KJIKeywoIi87JjX8H7z-sswUjyeJ4f0_pCc7F-E7o3pX5_SWH5wiI01QxMiV3pvXiE2rTRJhkyIVkHSl9q7aXTO2-RCYP7bpw7o9aTvYUAJpN9F2UTaey5tdIzKqHRkEKv1IqUeQgiNRAkd5koa-FVhv7wCGzSKePcx4_bj_CxjQuHuRQ3nDwWmxSL_CSfVYkNXeIpUKCJ7OkKDFb0Ia3VIQSEREU19CTE9DS19UT0tFTiJjb25JRDogMiBsb2NJRDogMTA0MDE1NTIzNjU2ODkyNDE3", container path=nonexistent
recon_1     | 2020-04-17 19:15:36,199 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | 2020-04-17 19:16:06,179 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:06,184 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:06,188 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:16:06,190 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:16:06,217 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:06,247 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:16:18,751 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:16:18,752 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:16:18,782 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 3
recon_1     | 2020-04-17 19:16:18,786 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:16:18,880 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:16:36,186 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:36,192 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:36,201 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:16:36,202 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:16:36,202 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:16:36,210 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
scm_1       | 2020-04-17 19:17:49,329 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:17:49,333 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
scm_1       | 2020-04-17 19:17:49,333 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | 2020-04-17 19:18:18,390 [Thread-343] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1       | 2020-04-17 19:18:18,400 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 3 milliseconds for processing 3 containers.
scm_1       | 2020-04-17 19:18:19,229 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:19,289 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:19,344 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:19,345 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:19,363 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:19,376 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:49,279 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:49,282 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:49,302 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:49,306 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:49,319 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:18:49,326 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:18:55,204 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,204 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,206 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,206 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z] removed from db
scm_1       | 2020-04-17 19:18:55,207 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:18:55,207 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,208 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,208 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:21:17,500 [Thread-191] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:21:17,501 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:21:17,501 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:21:17,501 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:21:17,501 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:21:17,502 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2  | 2020-04-17 19:21:17,508 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C-SegmentedRaftLogWorker close()
datanode_2  | 2020-04-17 19:21:17,514 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:21:17,514 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:21:17,514 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-BD2EB99E7C9C
datanode_2  | 2020-04-17 19:21:17,517 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  |  command on datanode #5f2fec2e-3f74-4923-9dbd-94817f14424e.
datanode_2  | 2020-04-17 19:21:17,517 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,518 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:17,524 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:21:17,527 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:21:17,528 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-11955824EF5B:java.util.concurrent.CompletableFuture@3b9a9814[Not completed]
datanode_2  | 2020-04-17 19:21:17,527 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:21:17,528 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:21:17,529 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:21:17,529 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b does not exist. Creating ...
datanode_2  | 2020-04-17 19:21:17,531 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:21:17,533 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b has been successfully formatted.
datanode_2  | 2020-04-17 19:21:17,543 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-11955824EF5B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:21:17,543 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:21:17,544 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:21:17,545 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:21:17,546 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | 2020-04-17 19:21:17,547 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:21:17,549 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:21:17,551 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:21:17,552 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:21:17,553 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:21:17,553 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:21:17,553 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:21:17,553 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:21:17,580 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:21:17,580 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:21:17,588 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-BD3B9F2C1D4D->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B, cid=6, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
scm_1       | 2020-04-17 19:18:55,208 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:18:55,208 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:18:55,253 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,253 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,253 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,253 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:18:55,301 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,302 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,302 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,302 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:18:55,306 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,306 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,306 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,306 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:18:55,330 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,330 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,330 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,330 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:18:55,333 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:18:55,333 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:18:55,333 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:18:55,334 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127253Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:18:57,198 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:18:57,199 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:19:19,220 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:19,251 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:19,267 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:19,269 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:19,274 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:19,287 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:19,294 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:19,305 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:19:19,305 [IPC Server handler 12 on 9860] INFO ipc.Server: IPC Server handler 12 on 9860, call Call#20 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:45463
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:19:21,343 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:21,351 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:19:21,351 [IPC Server handler 15 on 9863] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:19:21,352 [IPC Server handler 15 on 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
datanode_1  | 2020-04-17 19:23:14,048 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: shutdown
datanode_1  | 2020-04-17 19:23:14,049 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:23:14,049 [Command processor thread] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderState
datanode_1  | 2020-04-17 19:23:14,049 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@7a9e02c6] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:23:14,050 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$551/0x0000000840672040@4b2e195c] WARN server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->5f2fec2e-3f74-4923-9dbd-94817f14424e-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1  | 2020-04-17 19:23:14,050 [Command processor thread] INFO impl.PendingRequests: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-PendingRequests: sendNotLeaderResponses
datanode_1  | 2020-04-17 19:23:14,054 [grpc-default-executor-6] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:23:14,055 [grpc-default-executor-7] INFO server.GrpcLogAppender: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->5f2fec2e-3f74-4923-9dbd-94817f14424e-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1  | 2020-04-17 19:23:14,061 [grpc-default-executor-6] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:23:14,065 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:23:14,065 [Command processor thread] INFO impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater: set stopIndex = 0
datanode_1  | 2020-04-17 19:23:14,070 [grpc-default-executor-7] INFO impl.FollowerInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B->5f2fec2e-3f74-4923-9dbd-94817f14424e: nextIndex: updateUnconditionally 3 -> 1
datanode_1  | 2020-04-17 19:23:14,073 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:23:14,076 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:23:14,077 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 2020-04-17 19:23:14,078 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | 2020-04-17 19:23:14,080 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:23:14,081 [Command processor thread] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B: closes. applyIndex: 0
datanode_1  | 2020-04-17 19:23:14,082 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1  | 2020-04-17 19:23:14,086 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B-SegmentedRaftLogWorker close()
datanode_1  | 2020-04-17 19:23:14,088 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:23:14,089 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:23:14,090 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-11955824EF5B
datanode_1  | 2020-04-17 19:23:14,092 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  |  command on datanode #6acf0e94-e616-400e-845a-8f99bcdd25f3.
datanode_1  | 2020-04-17 19:23:14,093 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: addNew group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5D9ADFEAC228:java.util.concurrent.CompletableFuture@171e6bb9[Not completed]
datanode_1  | 2020-04-17 19:23:14,098 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3: new RaftServerImpl for group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_1  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1  | 2020-04-17 19:23:14,103 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1  | 2020-04-17 19:23:14,103 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1  | 2020-04-17 19:23:14,103 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_1  | 2020-04-17 19:23:14,104 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1  | 2020-04-17 19:23:14,105 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1  | 2020-04-17 19:23:14,106 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 does not exist. Creating ...
datanode_1  | 2020-04-17 19:23:14,107 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/in_use.lock acquired by nodename 6@d43c26201e9a
datanode_1  | 2020-04-17 19:23:14,111 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 has been successfully formatted.
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5D9ADFEAC228: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1  | 2020-04-17 19:23:14,114 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1  | 2020-04-17 19:23:14,124 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1  | 2020-04-17 19:23:14,135 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1  | 2020-04-17 19:23:14,140 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1  | 2020-04-17 19:23:14,140 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1  | 2020-04-17 19:23:14,140 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1  | 2020-04-17 19:23:14,140 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1  | 2020-04-17 19:23:14,140 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228
datanode_1  | 2020-04-17 19:23:14,141 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228
datanode_1  | 2020-04-17 19:23:14,141 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:23:14,141 [pool-70-thread-1] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1  | 2020-04-17 19:23:14,141 [pool-70-thread-1] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:23:14,143 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5D9ADFEAC228,id=6acf0e94-e616-400e-845a-8f99bcdd25f3
recon_1     | 2020-04-17 19:16:42,088 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z] removed from db
recon_1     | 2020-04-17 19:16:42,111 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:16:42,158 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:16:42,161 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:16:42,189 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
datanode_1  | 2020-04-17 19:23:14,143 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228
datanode_1  | 2020-04-17 19:23:14,188 [grpc-default-executor-6] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-E92920023000->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228, cid=9, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:19:25,570 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:19:25,572 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/99f5f103-0a44-4030-9ded-bd2eb99e7c9c/current/log_inprogress_0
datanode_3  | 2020-04-17 19:19:46,432 [ChunkWriter-31-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=495230976 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:19:46,433 [ChunkWriter-31-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:19:46,433 [ChunkWriter-31-0] ERROR ratis.ContainerStateMachine: group-BD2EB99E7C9C: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_3  | localID: 104015540068089859
datanode_3  | blockCommitSequenceId: 0
datanode_3  |  logIndex 1 chunkName 104015540068089859_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 2020-04-17 19:19:46,433 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : ContainerID 4 creation failed
datanode_3  | 2020-04-17 19:19:46,487 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
datanode_3  | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
datanode_3  | 2020-04-17 19:19:51,260 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:19:51,260 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:21:17,459 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#11-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=2
datanode_3  | 2020-04-17 19:21:17,488 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove  FOLLOWER 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:173)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:196)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:116)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:232)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:200)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:161)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:19:21,353 [IPC Server handler 15 on 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2020-04-17 19:19:23,498 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:23,504 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:19:25,512 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z] moved to OPEN state
scm_1       | 2020-04-17 19:19:45,103 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:45,117 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:19:46,488 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:46,502 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:46,507 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:19:46,523 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:46,526 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:46,527 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 4 creation failed
scm_1       | 2020-04-17 19:19:46,528 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:19:46,529 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z] moved to CLOSED state
scm_1       | 2020-04-17 19:19:46,530 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 4 creation failed
scm_1       | 2020-04-17 19:19:46,532 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:19:46,531 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4
scm_1       | 2020-04-17 19:19:46,534 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4
scm_1       | 2020-04-17 19:19:46,536 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:19:46,537 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 4 creation failed
scm_1       | 2020-04-17 19:19:46,537 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:19:46,542 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
scm_1       | 2020-04-17 19:19:46,543 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:19:46,546 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
scm_1       | 2020-04-17 19:19:46,546 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:19:46,547 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
scm_1       | 2020-04-17 19:19:46,547 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | 2020-04-17 19:20:16,466 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:16,473 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:20:16,528 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:16,556 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:16,570 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:20:16,573 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:20:46,492 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:46,533 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:20:46,562 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:46,565 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:20:46,568 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:20:46,569 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
datanode_3  | 2020-04-17 19:21:17,489 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: shutdown
datanode_3  | 2020-04-17 19:21:17,489 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-BD2EB99E7C9C,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:21:17,489 [Command processor thread] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:21:17,489 [Command processor thread] INFO impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater: set stopIndex = 0
datanode_3  | 2020-04-17 19:21:17,489 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:21:17,490 [Thread-197] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:21:17,490 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:21:17,490 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:21:17,491 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-BD2EB99E7C9C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:21:17,491 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:21:17,491 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C: closes. applyIndex: 0
datanode_3  | 2020-04-17 19:21:17,491 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3  | 2020-04-17 19:21:17,492 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C-SegmentedRaftLogWorker close()
datanode_3  | 2020-04-17 19:21:17,492 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:21:17,492 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:21:17,492 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-BD2EB99E7C9C
datanode_3  | 2020-04-17 19:21:17,493 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  |  command on datanode #3850e7c9-74c1-4e1e-9c77-287bd4a1bb92.
datanode_3  | 2020-04-17 19:21:17,493 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-BD2EB99E7C9C:null
datanode_3  | 2020-04-17 19:21:17,493 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:21:17,495 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-11955824EF5B:java.util.concurrent.CompletableFuture@10534ec7[Not completed]
datanode_3  | 2020-04-17 19:21:17,496 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:21:17,496 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:16:42,199 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: ec5b9ad5-e5fe-47df-a5e5-5447026c702a, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:14:39.243Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:17:06,201 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:06,210 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:06,213 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:06,217 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:06,219 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:06,221 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a. Trying to get from SCM.
recon_1     | 2020-04-17 19:17:06,234 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:06,282 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:23:14,230 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "aae84473-73dc-474b-bbb4-5d9adfeac228"
datanode_1  | .
datanode_1  | 2020-04-17 19:23:14,230 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-11955824EF5B:null
datanode_1  | 2020-04-17 19:23:14,231 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:23:14,231 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-11955824EF5B:null
datanode_1  | 2020-04-17 19:23:14,231 [grpc-default-executor-6] WARN impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed groupAdd* GroupManagementRequest:client-214FEBC9AF48->6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228, cid=11, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_1  | 	... 13 more
datanode_1  | 2020-04-17 19:23:14,231 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
scm_1       | 2020-04-17 19:20:52,530 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,532 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,532 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,532 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z] removed from db
scm_1       | 2020-04-17 19:20:52,533 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,533 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,533 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,533 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:20:52,534 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:20:52,534 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,534 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,534 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,535 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:20:52,535 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:20:52,538 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,538 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,538 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,538 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:21:17,497 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:21:17,498 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b does not exist. Creating ...
datanode_3  | 2020-04-17 19:21:17,502 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b has been successfully formatted.
datanode_3  | 2020-04-17 19:21:17,508 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-11955824EF5B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:21:17,509 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:21:17,524 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:21:17,524 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:21:17,525 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:21:17,525 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b
datanode_3  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:21:17,526 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:21:17,527 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:21:17,530 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:21:17,531 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:21:17,533 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:21:17,537 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:21:17,537 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:21:17,538 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:21:17,540 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:21:17,540 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:21:17,540 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:21:17,543 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:21:17,543 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:21:17,543 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:21:17,548 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:21:17,548 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:21:17,614 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-CE26454EEE96->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B, cid=7, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:17:06,283 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a. Trying to get from SCM.
recon_1     | 2020-04-17 19:17:06,285 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "ec5b9ad5-e5fe-47df-a5e5-5447026c702a"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=ec5b9ad5-e5fe-47df-a5e5-5447026c702a not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:17:07,186 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f. Trying to get from SCM.
recon_1     | 2020-04-17 19:17:07,188 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:16:42.127Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:17:07,190 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:07,190 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:17:07,202 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:17:07,270 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:17:12,348 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:17:12,348 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z] moved to OPEN state
recon_1     | 2020-04-17 19:17:18,886 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:17:18,886 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:17:18,928 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 2
recon_1     | 2020-04-17 19:17:18,931 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:17:19,027 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:17:37,207 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:37,217 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:21:17,651 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-450C6A0A8305->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B, cid=7, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:21:17,670 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | .
datanode_3  | 2020-04-17 19:21:17,671 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-BD2EB99E7C9C:null
datanode_3  | 2020-04-17 19:21:17,671 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:21:17,672 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-BD2EB99E7C9C:null
datanode_3  | 2020-04-17 19:21:17,672 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:23:14,232 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-11955824EF5B:null
datanode_1  | 2020-04-17 19:23:14,232 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:23:14,233 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-11955824EF5B:null
datanode_1  | 2020-04-17 19:23:14,233 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:23:14,233 [Command processor thread] INFO impl.RaftServerProxy: 6acf0e94-e616-400e-845a-8f99bcdd25f3: remove group-11955824EF5B:null
datanode_1  | 2020-04-17 19:23:14,233 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_1  | 
datanode_1  | java.io.IOException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 6acf0e94-e616-400e-845a-8f99bcdd25f3: Group group-11955824EF5B not found.
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1  | 	... 4 more
datanode_1  | 2020-04-17 19:23:19,184 [Thread-225] INFO impl.FollowerState: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-FollowerState: change to CANDIDATE, lastRpcTime:5042ms, electionTimeout:5041ms
datanode_1  | 2020-04-17 19:23:19,184 [Thread-225] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown FollowerState
datanode_1  | 2020-04-17 19:23:19,184 [Thread-225] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1  | 2020-04-17 19:23:19,185 [Thread-225] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start LeaderElection
datanode_1  | 2020-04-17 19:23:19,195 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-LeaderElection5] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-LeaderElection5: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_1  | 2020-04-17 19:23:19,224 [grpc-default-executor-6] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: changes role from CANDIDATE to FOLLOWER at term 1 for appendEntries
datanode_1  | 2020-04-17 19:23:19,224 [grpc-default-executor-6] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: shutdown LeaderElection
datanode_1  | 2020-04-17 19:23:19,224 [grpc-default-executor-6] INFO impl.RoleInfo: 6acf0e94-e616-400e-845a-8f99bcdd25f3: start FollowerState
datanode_1  | 2020-04-17 19:23:19,230 [grpc-default-executor-6] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5D9ADFEAC228 with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_1  | 2020-04-17 19:23:19,230 [grpc-default-executor-6] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for appendEntries, leader elected after 5118ms
datanode_1  | 2020-04-17 19:23:19,236 [grpc-default-executor-6] INFO impl.RaftServerImpl: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_1  | 2020-04-17 19:23:19,236 [grpc-default-executor-6] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1  | 2020-04-17 19:23:19,238 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/current/log_inprogress_0
datanode_1  | 2020-04-17 19:23:19,265 [6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-LeaderElection5] INFO impl.LeaderElection: 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-LeaderElection5: Election REJECTED; received 2 response(s) [6acf0e94-e616-400e-845a-8f99bcdd25f3<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:FAIL-t1, 6acf0e94-e616-400e-845a-8f99bcdd25f3<-5f2fec2e-3f74-4923-9dbd-94817f14424e#0:FAIL-t1] and 0 exception(s); 6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228:t1, leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=6acf0e94-e616-400e-845a-8f99bcdd25f3@group-5D9ADFEAC228-SegmentedRaftLog:OPENED:c-1,f0,i0, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
recon_1     | 2020-04-17 19:17:37,272 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:37,280 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:42,360 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:42,372 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:49,176 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 3 creation failed
recon_1     | 2020-04-17 19:17:49,178 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:49,178 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z] moved to CLOSED state
recon_1     | 2020-04-17 19:17:49,236 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
recon_1     | 2020-04-17 19:17:49,236 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:49,237 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:49,244 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:17:49,269 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:49,270 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 3 creation failed
recon_1     | 2020-04-17 19:17:49,271 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:49,290 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
recon_1     | 2020-04-17 19:17:49,291 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:49,306 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:17:49,308 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 3 creation failed
recon_1     | 2020-04-17 19:17:49,308 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:17:49,328 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A9FA23D090E2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "cb08a136052d5cf7:8a30edc618710aaf:cb08a136052d5cf7:0" containerID: 3 datanodeUuid: "3850e7c9-74c1-4e1e-9c77-287bd4a1bb92" pipelineID: "6955dc48-6a27-489c-aa23-15cb4d7af47f" writeChunk { blockID { containerID: 3 localID: 104015532387205122 blockCommitSequenceId: 0 } chunkData { chunkName: "104015532387205122_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDMgbG9jSUQ6IDEwNDAxNTUzMjM4NzIwNTEyMhiR7ub1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAFSDf0RByy6YuxFPSYX1580wc62qt3tdJRLwHugtnlKKAl5RdWTsPw4tvYlhwmHEuMMVt-W7JutqxPQetR6EMyr7Vi8JjBEUC7dehTrFnB2LEfrBs3OYqLibEVv-2X2DqTzwJsG5RhQPKYH81BJlSLLrnTikdezZpJkp_c-S2hUN33ZO1vmx6HMNiOj8bsERBGRLHlTDb9o9hgdl8SRKBlVn5gKjCWXxvsfR3la6gqWkPB-BgRVgsaIcJBW919wyn31uZBLMpBa0u3caotLUw7h6aLAlfOnQCGP7IB7gJNUeHG9HwgncercvZkJwoAce5nHbyMSfsHD6ZI28_KfXi7YQSEREU19CTE9DS19UT0tFTiJjb25JRDogMyBsb2NJRDogMTA0MDE1NTMyMzg3MjA1MTIy", container path=nonexistent
recon_1     | 2020-04-17 19:17:49,329 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | 2020-04-17 19:18:01,441 [MissingContainerTask] INFO fsck.MissingContainerTask: Missing Container task Thread took 3 milliseconds for processing 0 containers.
recon_1     | 2020-04-17 19:18:19,043 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:18:19,044 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:18:19,075 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 7
recon_1     | 2020-04-17 19:18:19,080 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:18:19,154 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:18:19,320 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:19,332 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:19,379 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:19,380 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:19,385 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:19,387 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:49,225 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:49,255 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:49,267 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:49,274 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:49,279 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:18:49,290 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:18:55,179 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z] removed from db
recon_1     | 2020-04-17 19:18:55,236 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:18:55,272 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:18:55,291 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:20:52,543 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,543 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,544 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,544 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:20:52,547 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,547 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,547 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:20:52,547 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:20:52,548 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:20:52,548 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:20:52,548 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:21:17,673 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-BD2EB99E7C9C:null
datanode_3  | 2020-04-17 19:21:17,673 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:21:17,673 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-BD2EB99E7C9C:null
datanode_3  | 2020-04-17 19:21:17,673 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-BD2EB99E7C9C not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:21:22,615 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:21:22,617 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:21:22,617 [Thread-258] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:21:22,618 [grpc-default-executor-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:21:22,739 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-11955824EF5B with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_3  | 2020-04-17 19:21:22,739 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5230ms
datanode_3  | 2020-04-17 19:21:22,740 [grpc-default-executor-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_3  | 2020-04-17 19:21:22,741 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:21:22,743 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/current/log_inprogress_0
datanode_3  | 2020-04-17 19:21:43,023 [ChunkWriter-40-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
scm_1       | 2020-04-17 19:20:52,548 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207880Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:20:57,199 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:20:57,200 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:21:16,468 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:16,474 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:16,496 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:16,513 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:16,513 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:16,529 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:16,547 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:16,552 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:21:16,553 [IPC Server handler 9 on 9860] INFO ipc.Server: IPC Server handler 9 on 9860, call Call#23 Retry#0 org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol.submitRequest from 172.24.0.6:35409
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:21:18,572 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:18,581 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:21:18,587 [IPC Server handler 18 on 9863] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:21:18,587 [IPC Server handler 18 on 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:173)
recon_1     | 2020-04-17 19:18:55,310 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:18:55,329 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 6955dc48-6a27-489c-aa23-15cb4d7af47f, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:16:42.127Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=494891008 B) is less than the container size (=1073741824 B).
datanode_3  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:21:43,033 [ChunkWriter-40-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:21:43,033 [ChunkWriter-40-0] ERROR ratis.ContainerStateMachine: group-11955824EF5B: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_3  | localID: 104015547713323012
datanode_3  | blockCommitSequenceId: 0
datanode_3  |  logIndex 1 chunkName 104015547713323012_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3  | 2020-04-17 19:21:43,037 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : ContainerID 5 creation failed
datanode_3  | 2020-04-17 19:21:43,049 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
datanode_3  | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
datanode_3  | 2020-04-17 19:21:48,511 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #5 does not exist in datanode. Container close failed.
datanode_3  | 2020-04-17 19:23:14,051 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#11-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=2
datanode_3  | 2020-04-17 19:23:14,052 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove  FOLLOWER 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_3  | 2020-04-17 19:23:14,052 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: shutdown
datanode_3  | 2020-04-17 19:23:14,052 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:23:14,052 [Command processor thread] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:23:14,052 [Command processor thread] INFO impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater: set stopIndex = 0
datanode_3  | 2020-04-17 19:23:14,052 [Thread-261] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:23:14,056 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:23:14,056 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:196)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:116)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:232)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:200)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:161)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:21:18,588 [IPC Server handler 18 on 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2020-04-17 19:21:20,807 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:20,810 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:21:22,654 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z] moved to OPEN state
scm_1       | 2020-04-17 19:21:41,771 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:41,777 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:21:43,089 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:43,097 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:43,118 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:21:43,119 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:43,125 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 5 creation failed
scm_1       | 2020-04-17 19:21:43,126 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:21:43,126 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z] moved to CLOSED state
scm_1       | 2020-04-17 19:21:43,126 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #5
scm_1       | 2020-04-17 19:21:43,130 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
scm_1       | 2020-04-17 19:21:43,133 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:21:43,141 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:43,144 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 5 creation failed
scm_1       | 2020-04-17 19:21:43,147 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:21:43,179 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:21:43,180 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
scm_1       | 2020-04-17 19:21:43,199 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:21:43,199 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 5 creation failed
scm_1       | 2020-04-17 19:21:43,200 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | 2020-04-17 19:21:43,215 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
scm_1       | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
scm_1       | 2020-04-17 19:21:43,216 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
datanode_3  | 2020-04-17 19:23:14,056 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 2020-04-17 19:23:14,056 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_3  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | 2020-04-17 19:23:14,057 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:23:14,057 [Command processor thread] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B: closes. applyIndex: 0
datanode_3  | 2020-04-17 19:23:14,057 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3  | 2020-04-17 19:23:14,058 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B-SegmentedRaftLogWorker close()
datanode_3  | 2020-04-17 19:23:14,059 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:23:14,059 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:23:14,059 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-11955824EF5B
datanode_3  | 2020-04-17 19:23:14,062 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  |  command on datanode #3850e7c9-74c1-4e1e-9c77-287bd4a1bb92.
datanode_3  | 2020-04-17 19:23:14,063 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: addNew group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5D9ADFEAC228:java.util.concurrent.CompletableFuture@3d7d20e0[Not completed]
datanode_3  | 2020-04-17 19:23:14,069 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: new RaftServerImpl for group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3  | 2020-04-17 19:23:14,082 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3  | 2020-04-17 19:23:14,085 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3  | 2020-04-17 19:23:14,086 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3  | 2020-04-17 19:23:14,086 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3  | 2020-04-17 19:23:14,087 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3  | 2020-04-17 19:23:14,088 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3  | 2020-04-17 19:23:14,089 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 does not exist. Creating ...
datanode_3  | 2020-04-17 19:23:14,093 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/in_use.lock acquired by nodename 6@1dab03dd0801
datanode_3  | 2020-04-17 19:23:14,099 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 has been successfully formatted.
datanode_3  | 2020-04-17 19:23:14,101 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5D9ADFEAC228: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3  | 2020-04-17 19:23:14,108 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3  | 2020-04-17 19:23:14,111 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3  | 2020-04-17 19:23:14,111 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3  | 2020-04-17 19:23:14,111 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3  | 2020-04-17 19:23:14,111 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:23:14,112 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228
datanode_3  | 2020-04-17 19:23:14,113 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3  | 2020-04-17 19:23:14,114 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3  | 2020-04-17 19:23:14,114 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3  | 2020-04-17 19:23:14,119 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3  | 2020-04-17 19:23:14,120 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3  | 2020-04-17 19:23:14,120 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3  | 2020-04-17 19:23:14,120 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:21:17,644 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-7F2BC92D0C69->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B, cid=9, seq=0, RW, null, Add:group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-11955824EF5B:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:21:17,666 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | .
datanode_2  | 2020-04-17 19:21:17,666 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,666 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:17,670 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,670 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:17,674 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,674 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:17,675 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,675 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:17,675 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-BD2EB99E7C9C:null
datanode_2  | 2020-04-17 19:21:17,675 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-BD2EB99E7C9C not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:21:22,618 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:6acf0e94-e616-400e-845a-8f99bcdd25f3
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:19:19,162 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:19:19,162 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:19:19,181 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0
recon_1     | 2020-04-17 19:19:19,229 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:19,261 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:19,269 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:19,279 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:19,285 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:19,286 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f. Trying to get from SCM.
recon_1     | 2020-04-17 19:19:19,304 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:19,308 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "6955dc48-6a27-489c-aa23-15cb4d7af47f"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=6955dc48-6a27-489c-aa23-15cb4d7af47f not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:19:20,261 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c. Trying to get from SCM.
recon_1     | 2020-04-17 19:19:20,264 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:18:55.207Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:19:20,265 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:20,265 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:19:20,270 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:19:20,293 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:19:25,515 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:19:25,516 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z] moved to OPEN state
recon_1     | 2020-04-17 19:19:46,455 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:46,467 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:46,490 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:46,491 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 4 creation failed
recon_1     | 2020-04-17 19:19:46,496 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:19:46,496 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:46,496 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z] moved to CLOSED state
recon_1     | 2020-04-17 19:19:46,497 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:46,497 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 4 creation failed
recon_1     | 2020-04-17 19:19:46,498 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:46,507 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
datanode_3  | 2020-04-17 19:23:14,120 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3  | 2020-04-17 19:23:14,120 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3  | 2020-04-17 19:23:14,121 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3  | 2020-04-17 19:23:14,131 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228
datanode_3  | 2020-04-17 19:23:14,132 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-D78C5FF5EBEB->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228, cid=8, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:23:14,132 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228
datanode_3  | 2020-04-17 19:23:14,133 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_3  | 2020-04-17 19:23:14,135 [pool-70-thread-1] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3  | 2020-04-17 19:23:14,135 [pool-70-thread-1] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:23:14,136 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5D9ADFEAC228,id=3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
datanode_3  | 2020-04-17 19:23:14,136 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228
datanode_3  | 2020-04-17 19:23:14,191 [grpc-default-executor-1] WARN impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed groupAdd* GroupManagementRequest:client-B402F7B96667->3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228, cid=8, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_3  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 2020-04-17 19:21:22,618 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:21:22,618 [Thread-251] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:21:22,619 [grpc-default-executor-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:21:22,746 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-11955824EF5B with new leaderId: 6acf0e94-e616-400e-845a-8f99bcdd25f3
datanode_2  | 2020-04-17 19:21:22,746 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: change Leader from null to 6acf0e94-e616-400e-845a-8f99bcdd25f3 at term 1 for appendEntries, leader elected after 5202ms
datanode_2  | 2020-04-17 19:21:22,748 [grpc-default-executor-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:21:22,748 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:21:22,753 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9bc2d955-62c8-4241-9d65-11955824ef5b/current/log_inprogress_0
datanode_2  | 2020-04-17 19:21:43,032 [ChunkWriter-7-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
scm_1       | 2020-04-17 19:22:13,074 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:13,083 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:13,105 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:13,106 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:13,113 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:13,119 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:43,057 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:43,113 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:43,116 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:43,118 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:22:43,123 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:43,123 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:22:49,126 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,127 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,127 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,128 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z] removed from db
scm_1       | 2020-04-17 19:22:49,128 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:22:49,128 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 to datanode:3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,128 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 to datanode:5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,129 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 to datanode:6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,129 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: aae84473-73dc-474b-bbb4-5d9adfeac228, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:22:49.128839Z]
scm_1       | 2020-04-17 19:22:49,129 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:22:49,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,134 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3  | 	... 13 more
datanode_3  | 2020-04-17 19:23:14,250 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "aae84473-73dc-474b-bbb4-5d9adfeac228"
datanode_3  | .
datanode_3  | 2020-04-17 19:23:14,251 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-11955824EF5B:null
datanode_3  | 2020-04-17 19:23:14,252 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:23:14,255 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-11955824EF5B:null
datanode_3  | 2020-04-17 19:23:14,255 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:23:14,256 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-11955824EF5B:null
datanode_3  | 2020-04-17 19:23:14,256 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:23:14,256 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-11955824EF5B:null
datanode_3  | 2020-04-17 19:23:14,256 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:19:46,507 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:46,520 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
recon_1     | 2020-04-17 19:19:46,521 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:46,521 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:19:46,522 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 4 creation failed
recon_1     | 2020-04-17 19:19:46,524 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:19:46,529 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-8F5086545CF2, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "d7659af4184dec20:9576ecfad66a1b14:d7659af4184dec20:0" containerID: 4 datanodeUuid: "5f2fec2e-3f74-4923-9dbd-94817f14424e" pipelineID: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c" writeChunk { blockID { containerID: 4 localID: 104015540068089859 blockCommitSequenceId: 0 } chunkData { chunkName: "104015540068089859_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDQgbG9jSUQ6IDEwNDAxNTU0MDA2ODA4OTg1ORjige71mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BAHC_rQk4nmQxo8XT_nYS5bI1plAQX3mZoGcN5iK8K-O-ssN2UKm5mxqwOvmhKnPiTYtxgEm3_QvjTAbQtl3zea2THKPpbD42rlijhxFJXj4gJkmx2fC5aKAVz968XmDbpFmjwz0YqMh2HLGgBLWe3f8TY3FkuO2vOscfnKt8H5T8FTFd-MXcmIscCgAHhC2T8KRl34kllmBqKJQxQW3YlLihdHCvNffAhLt9Gshruqj_8DmPh4H6oiweWS0PhZG3lM97EOvxE1NCKYx3is-nH0HAZQ5vtIb552aYGk4AD-euubFQN_CWmrDjjXMsqxPCb29U_6V1VMg1TPEULwAwpFIQSEREU19CTE9DS19UT0tFTiJjb25JRDogNCBsb2NJRDogMTA0MDE1NTQwMDY4MDg5ODU5", container path=nonexistent
recon_1     | 2020-04-17 19:19:46,529 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | 2020-04-17 19:20:16,477 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:16,482 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:16,531 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:16,548 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:16,568 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:16,572 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:19,185 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:20:19,185 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:20:19,202 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 4
recon_1     | 2020-04-17 19:20:19,205 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:20:19,252 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:20:46,484 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:46,498 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:46,517 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:20:46,524 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:46,532 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:46,538 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:20:52,497 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z] removed from db
recon_1     | 2020-04-17 19:20:52,498 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:20:52,508 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:23:14,257 [Command processor thread] INFO impl.RaftServerProxy: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: remove group-11955824EF5B:null
datanode_3  | 2020-04-17 19:23:14,257 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_3  | 
datanode_3  | java.io.IOException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: Group group-11955824EF5B not found.
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3  | 	... 4 more
datanode_3  | 2020-04-17 19:23:19,188 [grpc-default-executor-4] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_3  | 2020-04-17 19:23:19,188 [grpc-default-executor-4] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: shutdown FollowerState
datanode_3  | 2020-04-17 19:23:19,189 [Thread-321] INFO impl.FollowerState: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3  | 2020-04-17 19:23:19,190 [grpc-default-executor-4] INFO impl.RoleInfo: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92: start FollowerState
datanode_3  | 2020-04-17 19:23:19,247 [grpc-default-executor-4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5D9ADFEAC228 with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_3  | 2020-04-17 19:23:19,247 [grpc-default-executor-4] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for appendEntries, leader elected after 5146ms
datanode_3  | 2020-04-17 19:23:19,257 [grpc-default-executor-4] INFO impl.RaftServerImpl: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_3  | 2020-04-17 19:23:19,257 [grpc-default-executor-4] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3  | 2020-04-17 19:23:19,259 [3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92@group-5D9ADFEAC228-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/current/log_inprogress_0
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:22:49,147 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,147 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,148 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,148 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:22:49,199 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,199 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,199 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,199 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:22:49,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
recon_1     | 2020-04-17 19:20:52,521 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:20:52,524 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:20:52,530 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 99f5f103-0a44-4030-9ded-bd2eb99e7c9c, Nodes: 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:18:55.207Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:21:16,467 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:16,473 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:16,521 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:16,528 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:16,528 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:16,534 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:16,535 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c. Trying to get from SCM.
recon_1     | 2020-04-17 19:21:16,556 [EventQueue-PipelineReportForReconPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
recon_1     |   id: "99f5f103-0a44-4030-9ded-bd2eb99e7c9c"
recon_1     | }
recon_1     | isLeader: false
recon_1     | bytesWritten: 0
recon_1     |  from dn=5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407} {}
recon_1     | org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException): PipelineID=99f5f103-0a44-4030-9ded-bd2eb99e7c9c not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.getPipeline(PipelineStateManager.java:63)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.getPipeline(SCMPipelineManager.java:267)
recon_1     | 	at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getPipeline(SCMClientProtocolServer.java:409)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getPipeline(StorageContainerLocationProtocolServerSideTranslatorPB.java:375)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)
recon_1     | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:120)
recon_1     | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:31605)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
recon_1     | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
recon_1     | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
recon_1     | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1     | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1     | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
recon_1     | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
recon_1     | 
recon_1     | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1     | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
recon_1     | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
recon_1     | 	at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRpcRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:123)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)
recon_1     | 	at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.getPipeline(StorageContainerLocationProtocolClientSideTranslatorPB.java:347)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
scm_1       | 2020-04-17 19:22:49,201 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:22:49,216 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3
scm_1       | 2020-04-17 19:22:49,217 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e
scm_1       | 2020-04-17 19:22:49,217 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b close command to datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92
scm_1       | 2020-04-17 19:22:49,217 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534476Z]
scm_1       | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1       | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1       | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1       | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1       | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1       | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1       | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1       | 2020-04-17 19:22:57,200 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1       | 2020-04-17 19:22:57,201 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:23:00,442 [Socket Reader #1 for port 9860] INFO ipc.Server: Auth successful for recon/recon@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:23:00,445 [Socket Reader #1 for port 9860] INFO authorize.ServiceAuthorizationManager: Authorization successful for recon/recon@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocol
scm_1       | 2020-04-17 19:23:13,083 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:23:13,090 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:23:13,094 [Socket Reader #1 for port 9861] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:23:13,104 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:23:13,115 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:23:13,129 [Socket Reader #1 for port 9861] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol
scm_1       | 2020-04-17 19:23:15,203 [Socket Reader #1 for port 9863] INFO ipc.Server: Auth successful for om/om@EXAMPLE.COM (auth:KERBEROS)
scm_1       | 2020-04-17 19:23:15,209 [Socket Reader #1 for port 9863] INFO authorize.ServiceAuthorizationManager: Authorization successful for om/om@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol
scm_1       | 2020-04-17 19:23:15,210 [IPC Server handler 25 on 9863] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
scm_1       | 2020-04-17 19:23:15,211 [IPC Server handler 25 on 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.
scm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=494886912 B) is less than the container size (=1073741824 B).
datanode_2  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:21:43,032 [ChunkWriter-7-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:21:43,033 [ChunkWriter-7-0] ERROR ratis.ContainerStateMachine: group-11955824EF5B: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_2  | localID: 104015547713323012
datanode_2  | blockCommitSequenceId: 0
datanode_2  |  logIndex 1 chunkName 104015547713323012_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2  | 2020-04-17 19:21:43,045 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : ContainerID 5 creation failed
datanode_2  | 2020-04-17 19:21:43,050 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
datanode_2  | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
datanode_2  | 2020-04-17 19:21:48,534 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #5 does not exist in datanode. Container close failed.
datanode_2  | 2020-04-17 19:23:14,051 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Completed APPEND_ENTRIES, lastRequest: 6acf0e94-e616-400e-845a-8f99bcdd25f3->5f2fec2e-3f74-4923-9dbd-94817f14424e#11-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=2
datanode_2  | 2020-04-17 19:23:14,053 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove  FOLLOWER 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B:t1, leader=6acf0e94-e616-400e-845a-8f99bcdd25f3, voted=6acf0e94-e616-400e-845a-8f99bcdd25f3, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null RUNNING
datanode_2  | 2020-04-17 19:23:14,054 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: shutdown
datanode_2  | 2020-04-17 19:23:14,055 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-11955824EF5B,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:23:14,055 [Command processor thread] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:23:14,055 [Command processor thread] INFO impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater: set stopIndex = 0
datanode_2  | 2020-04-17 19:23:14,055 [Thread-254] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2  | 2020-04-17 19:23:14,063 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:23:14,063 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
recon_1     | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
recon_1     | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
recon_1     | 	at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)
recon_1     | 	at com.sun.proxy.$Proxy42.getPipeline(Unknown Source)
recon_1     | 	at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipeline(StorageContainerServiceProviderImpl.java:55)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineReportHandler.processPipelineReport(ReconPipelineReportHandler.java:65)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:84)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:47)
recon_1     | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:21:17,516 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b. Trying to get from SCM.
recon_1     | 2020-04-17 19:21:17,519 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:20:52.534Z] to Recon pipeline metadata.
recon_1     | 2020-04-17 19:21:17,519 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:21:17,519 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:21:17,535 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:21:17,556 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:21:19,257 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:21:19,257 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:21:19,272 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0
recon_1     | 2020-04-17 19:21:22,643 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:21:22,644 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z] moved to OPEN state
recon_1     | 2020-04-17 19:21:43,076 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:43,091 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:43,094 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:21:43,099 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:43,100 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : ContainerID 5 creation failed
recon_1     | 2020-04-17 19:21:43,101 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:21:43,102 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z] moved to CLOSED state
recon_1     | 2020-04-17 19:21:43,113 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 6acf0e94-e616-400e-845a-8f99bcdd25f3. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
recon_1     | 2020-04-17 19:21:43,113 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:21:43,130 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:43,132 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : ContainerID 5 creation failed
recon_1     | 2020-04-17 19:21:43,132 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
datanode_2  | 2020-04-17 19:23:14,063 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 2020-04-17 19:23:14,063 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater] ERROR impl.StateMachineUpdater: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater: Failed to take snapshot
datanode_2  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-11955824EF5B as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | 2020-04-17 19:23:14,063 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:23:14,064 [Command processor thread] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B: closes. applyIndex: 0
datanode_2  | 2020-04-17 19:23:14,064 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2  | 2020-04-17 19:23:14,065 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B-SegmentedRaftLogWorker close()
datanode_2  | 2020-04-17 19:23:14,065 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:23:14,065 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:23:14,065 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-11955824EF5B
datanode_2  | 2020-04-17 19:23:14,067 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  |  command on datanode #5f2fec2e-3f74-4923-9dbd-94817f14424e.
datanode_2  | 2020-04-17 19:23:14,071 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e: new RaftServerImpl for group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: ConfigurationManager, init=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2  | 2020-04-17 19:23:14,072 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2  | 2020-04-17 19:23:14,074 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2  | 2020-04-17 19:23:14,074 [pool-70-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 does not exist. Creating ...
datanode_2  | 2020-04-17 19:23:14,076 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: addNew group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] returns group-5D9ADFEAC228:java.util.concurrent.CompletableFuture@24703dde[Not completed]
datanode_2  | 2020-04-17 19:23:14,077 [pool-70-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/in_use.lock acquired by nodename 6@1877e902e348
datanode_2  | 2020-04-17 19:23:14,081 [pool-70-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228 has been successfully formatted.
datanode_2  | 2020-04-17 19:23:14,081 [pool-70-thread-1] INFO ratis.ContainerStateMachine: group-5D9ADFEAC228: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2  | 2020-04-17 19:23:14,081 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2  | 2020-04-17 19:23:14,082 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2  | 2020-04-17 19:23:14,082 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2  | 2020-04-17 19:23:14,082 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:23:14,082 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2  | 2020-04-17 19:23:14,083 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:173)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:196)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:116)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)
scm_1       | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:232)
scm_1       | 	at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:200)
scm_1       | 	at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:190)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:161)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:119)
scm_1       | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:75)
scm_1       | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)
scm_1       | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)
scm_1       | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
scm_1       | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
scm_1       | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1       | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1       | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1       | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1       | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1       | 2020-04-17 19:23:15,211 [IPC Server handler 25 on 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE
scm_1       | 2020-04-17 19:23:18,403 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 5 containers.
scm_1       | 2020-04-17 19:23:19,206 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: aae84473-73dc-474b-bbb4-5d9adfeac228, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:22:49.128839Z] moved to OPEN state
recon_1     | 2020-04-17 19:21:43,143 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 5f2fec2e-3f74-4923-9dbd-94817f14424e. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
recon_1     | 2020-04-17 19:21:43,143 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:21:43,155 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:21:43,156 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : ContainerID 5 creation failed
recon_1     | 2020-04-17 19:21:43,157 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:21:43,162 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b from datanode 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-9D491F0BDF50, cid=1
recon_1     | 	 State Machine: cmdType: WriteChunk traceID: "96f9bab37031214b:b6672a94cfd9e0e7:96f9bab37031214b:0" containerID: 5 datanodeUuid: "6acf0e94-e616-400e-845a-8f99bcdd25f3" pipelineID: "9bc2d955-62c8-4241-9d65-11955824ef5b" writeChunk { blockID { containerID: 5 localID: 104015547713323012 blockCommitSequenceId: 0 } chunkData { chunkName: "104015547713323012_chunk_1" offset: 0 len: 17540 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: " wi\267" } } } encodedToken: "TgoEcm9vdBIiY29uSUQ6IDUgbG9jSUQ6IDEwNDAxNTU0NzcxMzMyMzAxMhiUkfX1mC4iDTU4NDg1MzQ1NDI4ODgoASgCKAMoBDCAgICAAY4BALnxhMzDi4qPrdcJvdjbQEXUNQ9QmsWYibfrulAUjOn21QXxr3YMJpBICEzvJfKwzaubJg4JTvWtnmZ99dXNAQj7RGp9WaCL6uHvRNsKO-5WjsNfB5oArN1CWadaB6tTv6kMAusp9cKzi2YQ178tHpcMGrwBy7UKLio8JWydlFa26Aci29i0TxHwgc8tV2kz4q6pBDVGfd-W3h1inrFlUZPvbGl-Mr_JF_mGY2945VNCnzmdyZLNwOPyYg_t0aWvUzvdfsMoL8iCnWGYDgi4h6oZlOTXdMApPibXlqAnLLUxSdcofGWpNT90hRM6Fd38joKjV2tUfV38YsNU4rlhK3wQSEREU19CTE9DS19UT0tFTiJjb25JRDogNSBsb2NJRDogMTA0MDE1NTQ3NzEzMzIzMDEy", container path=nonexistent
recon_1     | 2020-04-17 19:21:43,162 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | 2020-04-17 19:22:13,080 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:22:13,085 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:22:13,086 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:22:13,093 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:13,102 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:13,108 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:19,275 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:22:19,275 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:22:19,292 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 4
recon_1     | 2020-04-17 19:22:19,296 [pool-10-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1     | 2020-04-17 19:22:19,337 [pool-10-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1     | 2020-04-17 19:22:43,086 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:22:43,096 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
datanode_2  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2  | 2020-04-17 19:23:14,084 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2  | 2020-04-17 19:23:14,090 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2  | 2020-04-17 19:23:14,092 [pool-70-thread-1] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2  | 2020-04-17 19:23:14,098 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2  | 2020-04-17 19:23:14,100 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2  | 2020-04-17 19:23:14,101 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2  | 2020-04-17 19:23:14,101 [pool-70-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2  | 2020-04-17 19:23:14,101 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228
datanode_2  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228
datanode_2  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: start as a follower, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2  | 2020-04-17 19:23:14,102 [pool-70-thread-1] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start FollowerState
datanode_2  | 2020-04-17 19:23:14,106 [pool-70-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5D9ADFEAC228,id=5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:23:14,106 [pool-70-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228
datanode_2  | 2020-04-17 19:23:14,155 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-052F1C5333DC->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228, cid=10, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:23:14,222 [grpc-default-executor-1] WARN impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed groupAdd* GroupManagementRequest:client-BD95A27E9791->5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228, cid=9, seq=0, RW, null, Add:group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858]
datanode_2  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
recon_1     | 2020-04-17 19:22:43,103 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:43,109 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:43,116 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:22:43,122 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:22:49,102 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z] removed from db
recon_1     | 2020-04-17 19:22:49,114 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:22:49,133 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:22:49,144 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:22:49,157 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:22:49,162 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 9bc2d955-62c8-4241-9d65-11955824ef5b, Nodes: 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:6acf0e94-e616-400e-845a-8f99bcdd25f3, CreationTimestamp2020-04-17T19:20:52.534Z]
recon_1     | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=9bc2d955-62c8-4241-9d65-11955824ef5b not found
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
recon_1     | 	at org.apache.hadoop.ozone.recon.scm.ReconPipelineManager.destroyPipeline(ReconPipelineManager.java:74)
recon_1     | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
recon_1     | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
recon_1     | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1     | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1     | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1     | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1     | 	at java.base/java.lang.Thread.run(Thread.java:834)
recon_1     | 2020-04-17 19:23:00,447 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 3 pipelines in house.
recon_1     | 2020-04-17 19:23:00,447 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 from SCM.
recon_1     | 2020-04-17 19:23:00,448 [PipelineSyncTask] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: aae84473-73dc-474b-bbb4-5d9adfeac228, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T19:22:49.128Z]
recon_1     | 2020-04-17 19:23:00,450 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 17 milliseconds.
recon_1     | 2020-04-17 19:23:01,445 [MissingContainerTask] INFO fsck.MissingContainerTask: Missing Container task Thread took 3 milliseconds for processing 0 containers.
recon_1     | 2020-04-17 19:23:13,070 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:23:13,074 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:23:13,100 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/d43c26201e9a@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:23:13,101 [Socket Reader #1 for port 9891] INFO ipc.Server: Auth successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS)
recon_1     | 2020-04-17 19:23:13,129 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1dab03dd0801@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:23:13,149 [Socket Reader #1 for port 9891] INFO authorize.ServiceAuthorizationManager: Authorization successful for dn/1877e902e348@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol
recon_1     | 2020-04-17 19:23:14,091 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:23:14,109 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 reported by 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844621200460}
recon_1     | 2020-04-17 19:23:14,129 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 reported by 6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844609254385}
recon_1     | 2020-04-17 19:23:19,203 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=aae84473-73dc-474b-bbb4-5d9adfeac228 reported by 5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: 5844825427407}
recon_1     | 2020-04-17 19:23:19,203 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: aae84473-73dc-474b-bbb4-5d9adfeac228, Nodes: 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92{ip: 172.24.0.10, host: ozonesecure_datanode_3.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}5f2fec2e-3f74-4923-9dbd-94817f14424e{ip: 172.24.0.7, host: ozonesecure_datanode_2.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}6acf0e94-e616-400e-845a-8f99bcdd25f3{ip: 172.24.0.3, host: ozonesecure_datanode_1.ozonesecure_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:5f2fec2e-3f74-4923-9dbd-94817f14424e, CreationTimestamp2020-04-17T19:22:49.128Z] moved to OPEN state
recon_1     | 2020-04-17 19:23:19,341 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1     | 2020-04-17 19:23:19,341 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1     | 2020-04-17 19:23:19,373 [pool-9-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Failed to add group-5D9ADFEAC228:[6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858] since the group already exists in the map.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2  | 	... 13 more
datanode_2  | 2020-04-17 19:23:14,246 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "aae84473-73dc-474b-bbb4-5d9adfeac228"
datanode_2  | .
datanode_2  | 2020-04-17 19:23:14,246 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-11955824EF5B:null
datanode_2  | 2020-04-17 19:23:14,246 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:23:14,247 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-11955824EF5B:null
datanode_2  | 2020-04-17 19:23:14,248 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:23:14,248 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-11955824EF5B:null
datanode_2  | 2020-04-17 19:23:14,248 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:23:14,249 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-11955824EF5B:null
datanode_2  | 2020-04-17 19:23:14,249 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:23:14,249 [Command processor thread] INFO impl.RaftServerProxy: 5f2fec2e-3f74-4923-9dbd-94817f14424e: remove group-11955824EF5B:null
datanode_2  | 2020-04-17 19:23:14,249 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "9bc2d955-62c8-4241-9d65-11955824ef5b"
datanode_2  | 
datanode_2  | java.io.IOException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 5f2fec2e-3f74-4923-9dbd-94817f14424e: Group group-11955824EF5B not found.
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2  | 	... 4 more
datanode_2  | 2020-04-17 19:23:19,171 [Thread-312] INFO impl.FollowerState: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-FollowerState: change to CANDIDATE, lastRpcTime:5068ms, electionTimeout:5064ms
datanode_2  | 2020-04-17 19:23:19,171 [Thread-312] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown FollowerState
datanode_2  | 2020-04-17 19:23:19,171 [Thread-312] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2  | 2020-04-17 19:23:19,172 [Thread-312] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderElection
datanode_2  | 2020-04-17 19:23:19,184 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4: begin an election at term 1 for -1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:23:19,197 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.LeaderElection: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4: Election PASSED; received 1 response(s) [5f2fec2e-3f74-4923-9dbd-94817f14424e<-3850e7c9-74c1-4e1e-9c77-287bd4a1bb92#0:OK-t1] and 0 exception(s); 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228:t1, leader=null, voted=5f2fec2e-3f74-4923-9dbd-94817f14424e, raftlog=5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null
datanode_2  | 2020-04-17 19:23:19,198 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: shutdown LeaderElection
datanode_2  | 2020-04-17 19:23:19,198 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2  | 2020-04-17 19:23:19,198 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5D9ADFEAC228 with new leaderId: 5f2fec2e-3f74-4923-9dbd-94817f14424e
datanode_2  | 2020-04-17 19:23:19,198 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: change Leader from null to 5f2fec2e-3f74-4923-9dbd-94817f14424e at term 1 for becomeLeader, leader elected after 5116ms
datanode_2  | 2020-04-17 19:23:19,210 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2  | 2020-04-17 19:23:19,210 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2  | 2020-04-17 19:23:19,210 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228
datanode_2  | 2020-04-17 19:23:19,210 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2  | 2020-04-17 19:23:19,210 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-17 19:23:19,211 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:23:19,212 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2  | 2020-04-17 19:23:19,212 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2  | 2020-04-17 19:23:19,212 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2  | 2020-04-17 19:23:19,212 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2  | 2020-04-17 19:23:19,215 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2  | 2020-04-17 19:23:19,215 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2  | 2020-04-17 19:23:19,216 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.RoleInfo: 5f2fec2e-3f74-4923-9dbd-94817f14424e: start LeaderState
datanode_2  | 2020-04-17 19:23:19,220 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2  | 2020-04-17 19:23:19,221 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/aae84473-73dc-474b-bbb4-5d9adfeac228/current/log_inprogress_0
datanode_2  | 2020-04-17 19:23:19,221 [5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-LeaderElection4] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228: set configuration 0: [6acf0e94-e616-400e-845a-8f99bcdd25f3:172.24.0.3:9858, 3850e7c9-74c1-4e1e-9c77-287bd4a1bb92:172.24.0.10:9858, 5f2fec2e-3f74-4923-9dbd-94817f14424e:172.24.0.7:9858], old=null at 0
datanode_2  | 2020-04-17 19:23:19,262 [grpc-default-executor-5] INFO impl.RaftServerImpl: 5f2fec2e-3f74-4923-9dbd-94817f14424e@group-5D9ADFEAC228-   LEADER: Withhold vote from candidate 6acf0e94-e616-400e-845a-8f99bcdd25f3 with term 1. State: leader=5f2fec2e-3f74-4923-9dbd-94817f14424e, term=1, lastRpcElapsed=null
