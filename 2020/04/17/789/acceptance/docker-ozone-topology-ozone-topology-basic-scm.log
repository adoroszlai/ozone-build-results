Attaching to ozone-topology_datanode_2_1, ozone-topology_scm_1, ozone-topology_datanode_3_1, ozone-topology_datanode_6_1, ozone-topology_datanode_5_1, ozone-topology_datanode_4_1, ozone-topology_om_1, ozone-topology_datanode_1_1
datanode_2_1  | Enabled profiling in kernel
datanode_2_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_2_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2_1  | 2020-04-17 18:00:55,552 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2_1  | /************************************************************
datanode_2_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_2_1  | STARTUP_MSG:   host = 06da621581ba/10.5.0.5
datanode_2_1  | STARTUP_MSG:   args = []
datanode_2_1  | STARTUP_MSG:   version = 3.2.0
datanode_2_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_2_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_2_1  | STARTUP_MSG:   java = 11.0.6
datanode_2_1  | ************************************************************/
datanode_2_1  | 2020-04-17 18:00:55,609 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2_1  | 2020-04-17 18:00:57,187 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2_1  | 2020-04-17 18:00:57,861 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2_1  | 2020-04-17 18:00:59,066 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2_1  | 2020-04-17 18:00:59,070 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2_1  | 2020-04-17 18:01:00,082 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:06da621581ba ip:10.5.0.5
datanode_2_1  | 2020-04-17 18:01:00,818 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2_1  | 2020-04-17 18:01:00,864 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_2_1  | 2020-04-17 18:01:00,866 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2_1  | 2020-04-17 18:01:00,894 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2_1  | 2020-04-17 18:01:01,029 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2_1  | 2020-04-17 18:01:06,407 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2_1  | 2020-04-17 18:01:06,715 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_2_1  | 2020-04-17 18:01:06,992 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_2_1  | 2020-04-17 18:01:07,006 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2_1  | 2020-04-17 18:01:07,008 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:01:07,008 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2_1  | 2020-04-17 18:01:07,046 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2_1  | 2020-04-17 18:01:08,269 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2_1  | 2020-04-17 18:01:09,010 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2_1  | 2020-04-17 18:01:09,242 [main] INFO util.log: Logging initialized @19442ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2_1  | 2020-04-17 18:01:09,736 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2_1  | 2020-04-17 18:01:09,791 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2_1  | 2020-04-17 18:01:09,879 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2_1  | 2020-04-17 18:01:09,884 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2_1  | 2020-04-17 18:01:09,899 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2_1  | 2020-04-17 18:01:09,899 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2_1  | 2020-04-17 18:01:10,155 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_2_1  | 2020-04-17 18:01:10,177 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2_1  | 2020-04-17 18:01:10,196 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_2_1  | 2020-04-17 18:01:10,406 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2_1  | 2020-04-17 18:01:10,439 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2_1  | 2020-04-17 18:01:10,442 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2_1  | 2020-04-17 18:01:10,489 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77681ce4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2_1  | 2020-04-17 18:01:10,504 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@367f0121{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2_1  | 2020-04-17 18:01:11,198 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c487a8{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-14452536431625577259.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2_1  | 2020-04-17 18:01:11,265 [main] INFO server.AbstractConnector: Started ServerConnector@48904d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2_1  | 2020-04-17 18:01:11,265 [main] INFO server.Server: Started @21465ms
datanode_2_1  | 2020-04-17 18:01:11,282 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2_1  | 2020-04-17 18:01:11,282 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2_1  | 2020-04-17 18:01:11,293 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2_1  | 2020-04-17 18:01:11,414 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@19c4417a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2_1  | 2020-04-17 18:01:12,421 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2_1  | 2020-04-17 18:01:14,546 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2_1  | 2020-04-17 18:01:15,307 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2_1  | 2020-04-17 18:01:15,309 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2_1  | 2020-04-17 18:01:15,309 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at port 9858
datanode_2_1  | 2020-04-17 18:01:15,433 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start RPC server
datanode_2_1  | 2020-04-17 18:01:15,771 [Datanode State Machine Thread - 0] INFO server.GrpcService: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2_1  | 2020-04-17 18:01:20,479 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: addNew group-BDE057308D8F:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858] returns group-BDE057308D8F:java.util.concurrent.CompletableFuture@7dcd9b02[Not completed]
datanode_2_1  | 2020-04-17 18:01:20,611 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: new RaftServerImpl for group-BDE057308D8F:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858] with ContainerStateMachine:uninitialized
datanode_2_1  | 2020-04-17 18:01:20,626 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2_1  | 2020-04-17 18:01:20,629 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2_1  | 2020-04-17 18:01:20,629 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2_1  | 2020-04-17 18:01:20,639 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2_1  | 2020-04-17 18:01:20,639 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:01:20,651 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858], old=null, confs=<EMPTY_MAP>
datanode_2_1  | 2020-04-17 18:01:20,661 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2_1  | 2020-04-17 18:01:20,675 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2_1  | 2020-04-17 18:01:20,694 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e9514b62-8305-4387-8371-bde057308d8f does not exist. Creating ...
datanode_2_1  | 2020-04-17 18:01:20,705 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e9514b62-8305-4387-8371-bde057308d8f/in_use.lock acquired by nodename 6@06da621581ba
datanode_2_1  | 2020-04-17 18:01:20,719 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e9514b62-8305-4387-8371-bde057308d8f has been successfully formatted.
datanode_2_1  | 2020-04-17 18:01:20,724 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-BDE057308D8F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2_1  | 2020-04-17 18:01:20,745 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2_1  | 2020-04-17 18:01:20,747 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2_1  | 2020-04-17 18:01:20,811 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2_1  | 2020-04-17 18:01:20,812 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:01:20,831 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:20,853 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:01:20,956 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2_1  | 2020-04-17 18:01:21,023 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/e9514b62-8305-4387-8371-bde057308d8f
datanode_2_1  | 2020-04-17 18:01:21,043 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2_1  | 2020-04-17 18:01:21,043 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:01:21,048 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:21,049 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2_1  | 2020-04-17 18:01:21,052 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:21,059 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2_1  | 2020-04-17 18:01:21,064 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2_1  | 2020-04-17 18:01:21,068 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2_1  | 2020-04-17 18:01:21,068 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2_1  | 2020-04-17 18:01:21,138 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:01:21,223 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2_1  | 2020-04-17 18:01:21,313 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:01:21,351 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2_1  | 2020-04-17 18:01:21,375 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2_1  | 2020-04-17 18:01:21,379 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2_1  | 2020-04-17 18:01:21,512 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F
datanode_2_1  | 2020-04-17 18:01:21,523 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F
datanode_2_1  | 2020-04-17 18:01:21,531 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858], old=null
datanode_2_1  | 2020-04-17 18:01:21,532 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2_1  | 2020-04-17 18:01:21,534 [pool-69-thread-1] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start FollowerState
datanode_2_1  | 2020-04-17 18:01:21,558 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BDE057308D8F,id=9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:01:21,560 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F
datanode_2_1  | 2020-04-17 18:01:21,619 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "e9514b62-8305-4387-8371-bde057308d8f"
datanode_2_1  | .
datanode_2_1  | 2020-04-17 18:01:21,621 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: addNew group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-3E8BAA29D9A7:java.util.concurrent.CompletableFuture@5ce86862[Not completed]
datanode_2_1  | 2020-04-17 18:01:21,684 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: new RaftServerImpl for group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2_1  | 2020-04-17 18:01:21,695 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2_1  | 2020-04-17 18:01:21,697 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2_1  | 2020-04-17 18:01:21,701 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2_1  | 2020-04-17 18:01:21,703 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2_1  | 2020-04-17 18:01:21,706 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:01:21,706 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2_1  | 2020-04-17 18:01:21,706 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2_1  | 2020-04-17 18:01:21,707 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2_1  | 2020-04-17 18:01:21,707 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 does not exist. Creating ...
datanode_2_1  | 2020-04-17 18:01:21,714 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/in_use.lock acquired by nodename 6@06da621581ba
datanode_2_1  | 2020-04-17 18:01:21,722 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 has been successfully formatted.
datanode_2_1  | 2020-04-17 18:01:21,723 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-3E8BAA29D9A7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2_1  | 2020-04-17 18:01:21,723 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2_1  | 2020-04-17 18:01:21,723 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2_1  | 2020-04-17 18:01:21,737 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2_1  | 2020-04-17 18:01:21,738 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:01:21,740 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:21,743 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2_1  | 2020-04-17 18:01:21,746 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:01:21,746 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2_1  | 2020-04-17 18:01:21,746 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2_1  | 2020-04-17 18:01:21,746 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2_1  | 2020-04-17 18:01:21,749 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2_1  | 2020-04-17 18:01:21,749 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:01:21,749 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2_1  | 2020-04-17 18:01:21,771 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:01:21,775 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2_1  | 2020-04-17 18:01:21,775 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2_1  | 2020-04-17 18:01:21,775 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2_1  | 2020-04-17 18:01:21,775 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:01:21,776 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:01:21,779 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:01:21,779 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2_1  | 2020-04-17 18:01:21,779 [pool-69-thread-1] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start FollowerState
datanode_2_1  | 2020-04-17 18:01:21,780 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:01:21,780 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:01:24,662 [grpc-default-executor-0] WARN impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed groupAdd* GroupManagementRequest:client-F321B2D66D0C->9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7, cid=0, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_2_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3_1  | Enabled profiling in kernel
datanode_3_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_3_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3_1  | 2020-04-17 18:00:55,475 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3_1  | /************************************************************
datanode_3_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_3_1  | STARTUP_MSG:   host = a8c66e3e0d0a/10.5.0.6
datanode_3_1  | STARTUP_MSG:   args = []
datanode_3_1  | STARTUP_MSG:   version = 3.2.0
datanode_3_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_3_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_3_1  | STARTUP_MSG:   java = 11.0.6
datanode_3_1  | ************************************************************/
datanode_3_1  | 2020-04-17 18:00:55,558 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3_1  | 2020-04-17 18:00:57,504 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3_1  | 2020-04-17 18:00:58,121 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3_1  | 2020-04-17 18:00:59,192 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3_1  | 2020-04-17 18:00:59,193 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3_1  | 2020-04-17 18:01:00,118 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:a8c66e3e0d0a ip:10.5.0.6
datanode_3_1  | 2020-04-17 18:01:00,760 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3_1  | 2020-04-17 18:01:00,773 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_3_1  | 2020-04-17 18:01:00,776 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3_1  | 2020-04-17 18:01:00,853 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3_1  | 2020-04-17 18:01:00,962 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3_1  | 2020-04-17 18:01:05,828 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3_1  | 2020-04-17 18:01:06,062 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_3_1  | 2020-04-17 18:01:06,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_3_1  | 2020-04-17 18:01:06,413 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3_1  | 2020-04-17 18:01:06,416 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:01:06,418 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3_1  | 2020-04-17 18:01:06,445 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3_1  | 2020-04-17 18:01:08,029 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:01:09,465 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3_1  | 2020-04-17 18:01:09,598 [main] INFO util.log: Logging initialized @20008ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3_1  | 2020-04-17 18:01:10,097 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3_1  | 2020-04-17 18:01:10,109 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3_1  | 2020-04-17 18:01:10,150 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3_1  | 2020-04-17 18:01:10,154 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3_1  | 2020-04-17 18:01:10,155 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3_1  | 2020-04-17 18:01:10,155 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3_1  | 2020-04-17 18:01:10,303 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_3_1  | 2020-04-17 18:01:10,325 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3_1  | 2020-04-17 18:01:10,340 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_3_1  | 2020-04-17 18:01:10,512 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3_1  | 2020-04-17 18:01:10,521 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3_1  | 2020-04-17 18:01:10,523 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3_1  | 2020-04-17 18:01:10,691 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77681ce4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3_1  | 2020-04-17 18:01:10,706 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@367f0121{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3_1  | 2020-04-17 18:01:11,228 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c487a8{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-18423273166288326167.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3_1  | 2020-04-17 18:01:11,270 [main] INFO server.AbstractConnector: Started ServerConnector@48904d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3_1  | 2020-04-17 18:01:11,274 [main] INFO server.Server: Started @21683ms
datanode_3_1  | 2020-04-17 18:01:11,279 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3_1  | 2020-04-17 18:01:11,279 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3_1  | 2020-04-17 18:01:11,284 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3_1  | 2020-04-17 18:01:11,423 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@193fd07a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3_1  | 2020-04-17 18:01:12,328 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3_1  | 2020-04-17 18:01:14,486 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3_1  | 2020-04-17 18:01:15,242 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3_1  | 2020-04-17 18:01:15,243 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3_1  | 2020-04-17 18:01:15,243 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 4e682fff-8411-4efe-954f-416a0833f687 at port 9858
datanode_3_1  | 2020-04-17 18:01:15,348 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: start RPC server
datanode_3_1  | 2020-04-17 18:01:15,601 [Datanode State Machine Thread - 0] INFO server.GrpcService: 4e682fff-8411-4efe-954f-416a0833f687: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3_1  | 2020-04-17 18:01:20,487 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: addNew group-E0E2714D3E07:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] returns group-E0E2714D3E07:java.util.concurrent.CompletableFuture@1b84ef40[Not completed]
datanode_3_1  | 2020-04-17 18:01:20,608 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687: new RaftServerImpl for group-E0E2714D3E07:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3_1  | 2020-04-17 18:01:20,620 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3_1  | 2020-04-17 18:01:20,620 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3_1  | 2020-04-17 18:01:20,621 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3_1  | 2020-04-17 18:01:20,624 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3_1  | 2020-04-17 18:01:20,628 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3_1  | 2020-04-17 18:01:20,649 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: ConfigurationManager, init=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3_1  | 2020-04-17 18:01:20,655 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:01:20,672 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3_1  | 2020-04-17 18:01:20,673 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1deb4c57-bfe1-4217-b2d1-e0e2714d3e07 does not exist. Creating ...
datanode_3_1  | 2020-04-17 18:01:20,699 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1deb4c57-bfe1-4217-b2d1-e0e2714d3e07/in_use.lock acquired by nodename 6@a8c66e3e0d0a
datanode_3_1  | 2020-04-17 18:01:20,713 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1deb4c57-bfe1-4217-b2d1-e0e2714d3e07 has been successfully formatted.
datanode_3_1  | 2020-04-17 18:01:20,725 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-E0E2714D3E07: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3_1  | 2020-04-17 18:01:20,759 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3_1  | 2020-04-17 18:01:20,778 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3_1  | 2020-04-17 18:01:20,813 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3_1  | 2020-04-17 18:01:20,816 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:01:20,839 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:20,858 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:01:20,951 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3_1  | 2020-04-17 18:01:20,979 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/1deb4c57-bfe1-4217-b2d1-e0e2714d3e07
datanode_3_1  | 2020-04-17 18:01:20,987 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3_1  | 2020-04-17 18:01:20,990 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:01:20,992 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:20,999 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3_1  | 2020-04-17 18:01:20,999 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:21,001 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3_1  | 2020-04-17 18:01:21,018 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3_1  | 2020-04-17 18:01:21,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3_1  | 2020-04-17 18:01:21,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3_1  | 2020-04-17 18:01:21,144 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:21,177 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3_1  | 2020-04-17 18:01:21,216 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:21,239 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3_1  | 2020-04-17 18:01:21,239 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3_1  | 2020-04-17 18:01:21,247 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3_1  | 2020-04-17 18:01:21,376 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07
datanode_3_1  | 2020-04-17 18:01:21,400 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07
datanode_3_1  | 2020-04-17 18:01:21,410 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: start as a follower, conf=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_3_1  | 2020-04-17 18:01:21,416 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3_1  | 2020-04-17 18:01:21,430 [pool-69-thread-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:01:21,571 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E0E2714D3E07,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:01:21,573 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07
datanode_3_1  | 2020-04-17 18:01:21,701 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "1deb4c57-bfe1-4217-b2d1-e0e2714d3e07"
datanode_3_1  | .
datanode_3_1  | 2020-04-17 18:01:21,709 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: addNew group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] returns group-D38C014C699A:java.util.concurrent.CompletableFuture@de432dc[Not completed]
datanode_3_1  | 2020-04-17 18:01:21,805 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687: new RaftServerImpl for group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] with ContainerStateMachine:uninitialized
datanode_3_1  | 2020-04-17 18:01:21,806 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3_1  | 2020-04-17 18:01:21,806 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3_1  | 2020-04-17 18:01:21,806 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3_1  | 2020-04-17 18:01:21,806 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3_1  | 2020-04-17 18:01:21,806 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3_1  | 2020-04-17 18:01:21,826 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: ConfigurationManager, init=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_3_1  | 2020-04-17 18:01:21,826 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:01:21,827 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3_1  | 2020-04-17 18:01:21,833 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a does not exist. Creating ...
datanode_3_1  | 2020-04-17 18:01:21,853 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/in_use.lock acquired by nodename 6@a8c66e3e0d0a
datanode_3_1  | 2020-04-17 18:01:21,899 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a has been successfully formatted.
datanode_3_1  | 2020-04-17 18:01:21,899 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-D38C014C699A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3_1  | 2020-04-17 18:01:21,900 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3_1  | 2020-04-17 18:01:21,900 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3_1  | 2020-04-17 18:01:21,900 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3_1  | 2020-04-17 18:01:21,900 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:01:21,900 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:21,901 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3_1  | 2020-04-17 18:01:21,901 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a
datanode_3_1  | 2020-04-17 18:01:21,901 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3_1  | 2020-04-17 18:01:21,902 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:01:21,902 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:21,902 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3_1  | 2020-04-17 18:01:21,904 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:21,905 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3_1  | 2020-04-17 18:01:21,905 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3_1  | 2020-04-17 18:01:21,907 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1_1  | Enabled profiling in kernel
datanode_1_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_1_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1_1  | 2020-04-17 18:00:55,519 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1_1  | /************************************************************
datanode_1_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_1_1  | STARTUP_MSG:   host = bfed8f6b6a8a/10.5.0.4
datanode_1_1  | STARTUP_MSG:   args = []
datanode_1_1  | STARTUP_MSG:   version = 3.2.0
datanode_1_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_1_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_1_1  | STARTUP_MSG:   java = 11.0.6
datanode_1_1  | ************************************************************/
datanode_1_1  | 2020-04-17 18:00:55,590 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1_1  | 2020-04-17 18:00:57,546 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1_1  | 2020-04-17 18:00:58,217 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1_1  | 2020-04-17 18:00:59,619 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1_1  | 2020-04-17 18:00:59,623 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1_1  | 2020-04-17 18:01:00,653 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:bfed8f6b6a8a ip:10.5.0.4
datanode_1_1  | 2020-04-17 18:01:01,143 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1_1  | 2020-04-17 18:01:01,179 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_1_1  | 2020-04-17 18:01:01,182 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1_1  | 2020-04-17 18:01:01,225 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1_1  | 2020-04-17 18:01:01,357 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1_1  | 2020-04-17 18:01:06,672 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1_1  | 2020-04-17 18:01:06,944 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_1_1  | 2020-04-17 18:01:07,334 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_1_1  | 2020-04-17 18:01:07,352 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1_1  | 2020-04-17 18:01:07,352 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:01:07,359 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1_1  | 2020-04-17 18:01:07,360 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1_1  | 2020-04-17 18:01:08,710 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1_1  | 2020-04-17 18:01:09,728 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1_1  | 2020-04-17 18:01:09,878 [main] INFO util.log: Logging initialized @20197ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1_1  | 2020-04-17 18:01:10,535 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1_1  | 2020-04-17 18:01:10,538 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1_1  | 2020-04-17 18:01:10,577 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1_1  | 2020-04-17 18:01:10,615 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1_1  | 2020-04-17 18:01:10,621 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1_1  | 2020-04-17 18:01:10,621 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1_1  | 2020-04-17 18:01:10,779 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_1_1  | 2020-04-17 18:01:10,839 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1_1  | 2020-04-17 18:01:10,841 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_1_1  | 2020-04-17 18:01:10,940 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1_1  | 2020-04-17 18:01:10,944 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1_1  | 2020-04-17 18:01:10,946 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1_1  | 2020-04-17 18:01:10,981 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77681ce4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1_1  | 2020-04-17 18:01:10,988 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@367f0121{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1_1  | 2020-04-17 18:01:11,434 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c487a8{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-18030713547320091024.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1_1  | 2020-04-17 18:01:11,516 [main] INFO server.AbstractConnector: Started ServerConnector@48904d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1_1  | 2020-04-17 18:01:11,517 [main] INFO server.Server: Started @21836ms
datanode_1_1  | 2020-04-17 18:01:11,545 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1_1  | 2020-04-17 18:01:11,545 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1_1  | 2020-04-17 18:01:11,843 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1_1  | 2020-04-17 18:01:12,014 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@d8a655a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1_1  | 2020-04-17 18:01:12,975 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1_1  | 2020-04-17 18:01:15,108 [Datanode State Machine Thread - 0] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1_1  | java.net.SocketTimeoutException: Call From bfed8f6b6a8a/10.5.0.4 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.5.0.4:41850 remote=scm/10.5.0.71:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
datanode_3_1  | 2020-04-17 18:01:21,907 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3_1  | 2020-04-17 18:01:21,908 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:21,908 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3_1  | 2020-04-17 18:01:21,908 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:21,912 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3_1  | 2020-04-17 18:01:21,912 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3_1  | 2020-04-17 18:01:21,913 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3_1  | 2020-04-17 18:01:21,914 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A
datanode_3_1  | 2020-04-17 18:01:21,914 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A
datanode_3_1  | 2020-04-17 18:01:21,921 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: start as a follower, conf=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_3_1  | 2020-04-17 18:01:21,923 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3_1  | 2020-04-17 18:01:21,923 [pool-69-thread-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:01:21,963 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D38C014C699A,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:01:21,963 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A
datanode_3_1  | 2020-04-17 18:01:24,162 [grpc-default-executor-0] WARN impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: Failed groupAdd* GroupManagementRequest:client-D62F295C5CA5->4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A, cid=0, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_3_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_1_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_1_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
datanode_1_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:01:24,573 [grpc-default-executor-0] WARN impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: Failed groupAdd* GroupManagementRequest:client-323F0A9014F4->4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A, cid=1, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_3_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:01:24,952 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "3a881537-d96e-4768-950c-d38c014c699a"
datanode_3_1  | .
datanode_3_1  | 2020-04-17 18:01:24,955 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: addNew group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-3E8BAA29D9A7:java.util.concurrent.CompletableFuture@5028cc56[Not completed]
datanode_3_1  | 2020-04-17 18:01:24,962 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687: new RaftServerImpl for group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3_1  | 2020-04-17 18:01:24,963 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3_1  | 2020-04-17 18:01:24,963 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3_1  | 2020-04-17 18:01:24,963 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3_1  | 2020-04-17 18:01:24,963 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3_1  | 2020-04-17 18:01:24,964 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3_1  | 2020-04-17 18:01:24,965 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3_1  | 2020-04-17 18:01:24,965 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:01:24,967 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3_1  | 2020-04-17 18:01:24,967 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 does not exist. Creating ...
datanode_5_1  | Enabled profiling in kernel
datanode_5_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_5_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5_1  | 2020-04-17 18:00:57,647 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5_1  | /************************************************************
datanode_5_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_5_1  | STARTUP_MSG:   host = ea97b2d03cd4/10.5.0.8
datanode_5_1  | STARTUP_MSG:   args = []
datanode_5_1  | STARTUP_MSG:   version = 3.2.0
datanode_5_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_5_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_5_1  | STARTUP_MSG:   java = 11.0.6
datanode_5_1  | ************************************************************/
datanode_5_1  | 2020-04-17 18:00:57,701 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5_1  | 2020-04-17 18:00:59,561 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5_1  | 2020-04-17 18:01:00,092 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5_1  | 2020-04-17 18:01:01,199 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5_1  | 2020-04-17 18:01:01,200 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5_1  | 2020-04-17 18:01:02,294 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ea97b2d03cd4 ip:10.5.0.8
datanode_5_1  | 2020-04-17 18:01:02,857 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5_1  | 2020-04-17 18:01:02,878 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_5_1  | 2020-04-17 18:01:02,902 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5_1  | 2020-04-17 18:01:02,956 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5_1  | 2020-04-17 18:01:03,103 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5_1  | 2020-04-17 18:01:08,193 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5_1  | 2020-04-17 18:01:08,446 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_5_1  | 2020-04-17 18:01:08,903 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_5_1  | 2020-04-17 18:01:08,910 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5_1  | 2020-04-17 18:01:08,916 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:01:08,919 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5_1  | 2020-04-17 18:01:08,938 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5_1  | 2020-04-17 18:01:10,526 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5_1  | 2020-04-17 18:01:11,421 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5_1  | 2020-04-17 18:01:11,731 [main] INFO util.log: Logging initialized @19754ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5_1  | 2020-04-17 18:01:12,384 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_5_1  | 2020-04-17 18:01:12,399 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5_1  | 2020-04-17 18:01:12,444 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5_1  | 2020-04-17 18:01:12,467 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5_1  | 2020-04-17 18:01:12,467 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5_1  | 2020-04-17 18:01:12,476 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5_1  | 2020-04-17 18:01:12,633 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_5_1  | 2020-04-17 18:01:12,655 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5_1  | 2020-04-17 18:01:12,692 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_5_1  | 2020-04-17 18:01:12,816 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5_1  | 2020-04-17 18:01:12,817 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5_1  | 2020-04-17 18:01:12,821 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_5_1  | 2020-04-17 18:01:12,908 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@306f6f1d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_5_1  | 2020-04-17 18:01:12,910 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1df1ced0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5_1  | 2020-04-17 18:01:13,126 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@794366a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-4548000345345275728.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5_1  | 2020-04-17 18:01:13,137 [main] INFO server.AbstractConnector: Started ServerConnector@59b65dce{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_5_1  | 2020-04-17 18:01:13,137 [main] INFO server.Server: Started @21161ms
datanode_5_1  | 2020-04-17 18:01:13,147 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5_1  | 2020-04-17 18:01:13,147 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5_1  | 2020-04-17 18:01:13,155 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5_1  | 2020-04-17 18:01:13,225 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1220ac6f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_5_1  | 2020-04-17 18:01:13,652 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5_1  | 2020-04-17 18:01:15,502 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5_1  | 2020-04-17 18:01:15,504 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_5_1  | 2020-04-17 18:01:15,507 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7fb5a271-9f73-44d7-9131-67b4b4fca07d at port 9858
datanode_5_1  | 2020-04-17 18:01:15,619 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start RPC server
datanode_5_1  | 2020-04-17 18:01:15,893 [Datanode State Machine Thread - 0] INFO server.GrpcService: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_5_1  | 2020-04-17 18:01:20,238 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: addNew group-490C4A63FE1D:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858] returns group-490C4A63FE1D:java.util.concurrent.CompletableFuture@d759567[Not completed]
datanode_5_1  | 2020-04-17 18:01:20,317 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: new RaftServerImpl for group-490C4A63FE1D:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858] with ContainerStateMachine:uninitialized
datanode_5_1  | 2020-04-17 18:01:20,320 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5_1  | 2020-04-17 18:01:20,326 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5_1  | 2020-04-17 18:01:20,326 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_5_1  | 2020-04-17 18:01:20,329 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_5_1  | 2020-04-17 18:01:20,330 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5_1  | 2020-04-17 18:01:20,338 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: ConfigurationManager, init=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858], old=null, confs=<EMPTY_MAP>
datanode_5_1  | 2020-04-17 18:01:20,339 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5_1  | 2020-04-17 18:01:20,347 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5_1  | 2020-04-17 18:01:20,348 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9e30214a-3af8-4f12-a594-490c4a63fe1d does not exist. Creating ...
datanode_5_1  | 2020-04-17 18:01:20,372 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9e30214a-3af8-4f12-a594-490c4a63fe1d/in_use.lock acquired by nodename 6@ea97b2d03cd4
datanode_5_1  | 2020-04-17 18:01:20,379 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9e30214a-3af8-4f12-a594-490c4a63fe1d has been successfully formatted.
datanode_5_1  | 2020-04-17 18:01:20,398 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-490C4A63FE1D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5_1  | 2020-04-17 18:01:20,401 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_5_1  | 2020-04-17 18:01:20,418 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5_1  | 2020-04-17 18:01:20,438 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5_1  | 2020-04-17 18:01:20,439 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:01:20,459 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:20,472 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:01:20,523 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5_1  | 2020-04-17 18:01:20,534 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/9e30214a-3af8-4f12-a594-490c4a63fe1d
datanode_5_1  | 2020-04-17 18:01:20,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_5_1  | 2020-04-17 18:01:20,558 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5_1  | 2020-04-17 18:01:20,569 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:20,570 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5_1  | 2020-04-17 18:01:20,573 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:20,589 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5_1  | 2020-04-17 18:01:20,590 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5_1  | 2020-04-17 18:01:20,594 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5_1  | 2020-04-17 18:01:20,602 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5_1  | 2020-04-17 18:01:20,718 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:01:20,783 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5_1  | 2020-04-17 18:01:20,836 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:01:20,842 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5_1  | 2020-04-17 18:01:20,850 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5_1  | 2020-04-17 18:01:20,851 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5_1  | 2020-04-17 18:01:20,959 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D
datanode_5_1  | 2020-04-17 18:01:20,980 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D
datanode_5_1  | 2020-04-17 18:01:20,989 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: start as a follower, conf=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858], old=null
datanode_5_1  | 2020-04-17 18:01:21,001 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5_1  | 2020-04-17 18:01:21,010 [pool-69-thread-1] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start FollowerState
datanode_5_1  | 2020-04-17 18:01:21,104 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-490C4A63FE1D,id=7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:01:21,106 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D
datanode_5_1  | 2020-04-17 18:01:21,156 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "9e30214a-3af8-4f12-a594-490c4a63fe1d"
datanode_5_1  | .
datanode_5_1  | 2020-04-17 18:01:21,163 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: addNew group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] returns group-D38C014C699A:java.util.concurrent.CompletableFuture@6f628c9d[Not completed]
datanode_5_1  | 2020-04-17 18:01:21,276 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: new RaftServerImpl for group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] with ContainerStateMachine:uninitialized
datanode_5_1  | 2020-04-17 18:01:21,282 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5_1  | 2020-04-17 18:01:21,282 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | Enabled profiling in kernel
datanode_4_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_4_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4_1  | 2020-04-17 18:00:56,547 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4_1  | /************************************************************
datanode_4_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_4_1  | STARTUP_MSG:   host = 25eca77e6515/10.5.0.7
datanode_4_1  | STARTUP_MSG:   args = []
datanode_4_1  | STARTUP_MSG:   version = 3.2.0
datanode_3_1  | 2020-04-17 18:01:24,989 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/in_use.lock acquired by nodename 6@a8c66e3e0d0a
datanode_3_1  | 2020-04-17 18:01:25,009 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 has been successfully formatted.
datanode_3_1  | 2020-04-17 18:01:25,010 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-3E8BAA29D9A7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3_1  | 2020-04-17 18:01:25,020 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3_1  | 2020-04-17 18:01:25,020 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3_1  | 2020-04-17 18:01:25,020 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3_1  | 2020-04-17 18:01:25,020 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:01:25,022 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:25,022 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3_1  | 2020-04-17 18:01:25,022 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7
datanode_3_1  | 2020-04-17 18:01:25,022 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3_1  | 2020-04-17 18:01:25,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:01:25,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:25,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3_1  | 2020-04-17 18:01:25,023 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:01:25,027 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3_1  | 2020-04-17 18:01:25,027 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3_1  | 2020-04-17 18:01:25,027 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3_1  | 2020-04-17 18:01:25,030 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3_1  | 2020-04-17 18:01:25,032 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:25,035 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3_1  | 2020-04-17 18:01:25,050 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:01:25,050 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3_1  | 2020-04-17 18:01:25,050 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3_1  | 2020-04-17 18:01:25,056 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3_1  | 2020-04-17 18:01:25,056 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_3_1  | 2020-04-17 18:01:25,058 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_3_1  | 2020-04-17 18:01:25,061 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_3_1  | 2020-04-17 18:01:25,063 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3_1  | 2020-04-17 18:01:25,063 [pool-69-thread-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:01:25,079 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:01:25,079 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_3_1  | 2020-04-17 18:01:25,169 [grpc-default-executor-0] WARN impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: Failed groupAdd* GroupManagementRequest:client-3DEB76F1B8E4->4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7, cid=1, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_3_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:01:25,200 [grpc-default-executor-0] WARN impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: Failed groupAdd* GroupManagementRequest:client-68E972D27CDB->4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7, cid=1, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_3_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_3_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_3_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_4_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_4_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_4_1  | STARTUP_MSG:   java = 11.0.6
datanode_4_1  | ************************************************************/
datanode_4_1  | 2020-04-17 18:00:56,588 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4_1  | 2020-04-17 18:00:58,172 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4_1  | 2020-04-17 18:00:58,769 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4_1  | 2020-04-17 18:00:59,871 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4_1  | 2020-04-17 18:00:59,871 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4_1  | 2020-04-17 18:01:00,936 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:25eca77e6515 ip:10.5.0.7
datanode_4_1  | 2020-04-17 18:01:01,478 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4_1  | 2020-04-17 18:01:01,501 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_4_1  | 2020-04-17 18:01:01,502 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4_1  | 2020-04-17 18:01:01,530 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4_1  | 2020-04-17 18:01:01,646 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4_1  | 2020-04-17 18:01:06,950 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4_1  | 2020-04-17 18:01:07,234 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_4_1  | 2020-04-17 18:01:07,684 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_4_1  | 2020-04-17 18:01:07,691 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4_1  | 2020-04-17 18:01:07,693 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:01:07,699 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4_1  | 2020-04-17 18:01:07,700 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4_1  | 2020-04-17 18:01:09,209 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4_1  | 2020-04-17 18:01:09,955 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4_1  | 2020-04-17 18:01:10,087 [main] INFO util.log: Logging initialized @19489ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4_1  | 2020-04-17 18:01:10,679 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_4_1  | 2020-04-17 18:01:10,687 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4_1  | 2020-04-17 18:01:10,731 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4_1  | 2020-04-17 18:01:10,734 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4_1  | 2020-04-17 18:01:10,739 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4_1  | 2020-04-17 18:01:10,739 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4_1  | 2020-04-17 18:01:10,852 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_4_1  | 2020-04-17 18:01:10,867 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4_1  | 2020-04-17 18:01:10,874 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_4_1  | 2020-04-17 18:01:10,998 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4_1  | 2020-04-17 18:01:11,005 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4_1  | 2020-04-17 18:01:11,008 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_4_1  | 2020-04-17 18:01:11,102 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77681ce4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4_1  | 2020-04-17 18:01:11,114 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@367f0121{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4_1  | 2020-04-17 18:01:11,762 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c487a8{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-11939654122271235134.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4_1  | 2020-04-17 18:01:11,796 [main] INFO server.AbstractConnector: Started ServerConnector@48904d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_4_1  | 2020-04-17 18:01:11,799 [main] INFO server.Server: Started @21201ms
datanode_4_1  | 2020-04-17 18:01:11,827 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4_1  | 2020-04-17 18:01:11,827 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4_1  | 2020-04-17 18:01:11,830 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4_1  | 2020-04-17 18:01:12,012 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6ba4ef50] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_4_1  | 2020-04-17 18:01:12,917 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4_1  | 2020-04-17 18:01:15,089 [Datanode State Machine Thread - 0] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_4_1  | java.net.SocketTimeoutException: Call From 25eca77e6515/10.5.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.5.0.7:43500 remote=scm/10.5.0.71:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4_1  | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4_1  | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4_1  | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4_1  | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
datanode_4_1  | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
datanode_4_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
datanode_4_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
datanode_4_1  | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_4_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_4_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_4_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_4_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.5.0.7:43500 remote=scm/10.5.0.71:9861]
datanode_4_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_4_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:557)
datanode_4_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1816)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)
datanode_4_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)
datanode_4_1  | 2020-04-17 18:01:16,088 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4_1  | 2020-04-17 18:01:16,091 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_4_1  | 2020-04-17 18:01:16,091 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 313d3461-163d-4229-bfba-0697ea0de001 at port 9858
datanode_4_1  | 2020-04-17 18:01:16,281 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: start RPC server
datanode_4_1  | 2020-04-17 18:01:16,549 [Datanode State Machine Thread - 0] INFO server.GrpcService: 313d3461-163d-4229-bfba-0697ea0de001: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_4_1  | 2020-04-17 18:01:21,011 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: addNew group-51237E4653CE:[313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-51237E4653CE:java.util.concurrent.CompletableFuture@49ec293d[Not completed]
datanode_4_1  | 2020-04-17 18:01:21,088 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001: new RaftServerImpl for group-51237E4653CE:[313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_4_1  | 2020-04-17 18:01:21,090 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4_1  | 2020-04-17 18:01:21,116 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | 2020-04-17 18:01:21,116 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_4_1  | 2020-04-17 18:01:21,119 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_4_1  | 2020-04-17 18:01:21,119 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4_1  | 2020-04-17 18:01:21,150 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: ConfigurationManager, init=-1: [313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_4_1  | 2020-04-17 18:01:21,163 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4_1  | 2020-04-17 18:01:21,172 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4_1  | 2020-04-17 18:01:21,184 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fd94547c-946e-4f87-9733-51237e4653ce does not exist. Creating ...
datanode_4_1  | 2020-04-17 18:01:21,233 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fd94547c-946e-4f87-9733-51237e4653ce/in_use.lock acquired by nodename 6@25eca77e6515
datanode_4_1  | 2020-04-17 18:01:21,247 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fd94547c-946e-4f87-9733-51237e4653ce has been successfully formatted.
datanode_4_1  | 2020-04-17 18:01:21,257 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-51237E4653CE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4_1  | 2020-04-17 18:01:21,314 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_4_1  | 2020-04-17 18:01:21,325 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4_1  | 2020-04-17 18:01:21,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4_1  | 2020-04-17 18:01:21,358 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:01:21,359 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:21,426 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:01:21,513 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4_1  | 2020-04-17 18:01:21,563 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/fd94547c-946e-4f87-9733-51237e4653ce
datanode_4_1  | 2020-04-17 18:01:21,569 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_4_1  | 2020-04-17 18:01:21,590 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:01:21,612 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:21,615 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5_1  | 2020-04-17 18:01:21,283 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_5_1  | 2020-04-17 18:01:21,285 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_5_1  | 2020-04-17 18:01:21,285 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5_1  | 2020-04-17 18:01:21,286 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: ConfigurationManager, init=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_5_1  | 2020-04-17 18:01:21,287 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5_1  | 2020-04-17 18:01:21,288 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5_1  | 2020-04-17 18:01:21,290 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a does not exist. Creating ...
datanode_5_1  | 2020-04-17 18:01:21,314 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/in_use.lock acquired by nodename 6@ea97b2d03cd4
datanode_5_1  | 2020-04-17 18:01:21,323 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a has been successfully formatted.
datanode_5_1  | 2020-04-17 18:01:21,326 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-D38C014C699A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5_1  | 2020-04-17 18:01:21,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_5_1  | 2020-04-17 18:01:21,367 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5_1  | 2020-04-17 18:01:21,369 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5_1  | 2020-04-17 18:01:21,369 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:01:21,370 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:21,371 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5_1  | 2020-04-17 18:01:21,372 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a
datanode_5_1  | 2020-04-17 18:01:21,372 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_5_1  | 2020-04-17 18:01:21,375 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5_1  | 2020-04-17 18:01:21,383 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:21,383 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5_1  | 2020-04-17 18:01:21,383 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:01:21,385 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5_1  | 2020-04-17 18:01:21,385 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5_1  | 2020-04-17 18:01:21,385 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5_1  | 2020-04-17 18:01:21,385 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5_1  | 2020-04-17 18:01:21,386 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:01:21,410 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5_1  | 2020-04-17 18:01:21,418 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:01:21,419 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5_1  | 2020-04-17 18:01:21,420 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5_1  | 2020-04-17 18:01:21,422 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5_1  | 2020-04-17 18:01:21,422 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A
datanode_5_1  | 2020-04-17 18:01:21,426 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A
datanode_5_1  | 2020-04-17 18:01:21,429 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: start as a follower, conf=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_5_1  | 2020-04-17 18:01:21,435 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5_1  | 2020-04-17 18:01:21,438 [pool-69-thread-1] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start FollowerState
datanode_5_1  | 2020-04-17 18:01:21,448 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D38C014C699A,id=7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:01:21,449 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A
datanode_5_1  | 2020-04-17 18:01:24,016 [grpc-default-executor-0] WARN impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed groupAdd* GroupManagementRequest:client-328B74766867->7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A, cid=0, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_5_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_3_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 4e682fff-8411-4efe-954f-416a0833f687: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:01:25,346 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | .
datanode_3_1  | 2020-04-17 18:01:26,641 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_3_1  | 2020-04-17 18:01:26,647 [grpc-default-executor-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:01:26,648 [Thread-25] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:01:26,648 [grpc-default-executor-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:01:26,707 [Thread-23] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-FollowerState: change to CANDIDATE, lastRpcTime:5276ms, electionTimeout:5182ms
datanode_3_1  | 2020-04-17 18:01:26,708 [Thread-23] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:01:26,709 [Thread-23] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3_1  | 2020-04-17 18:01:26,712 [Thread-23] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start LeaderElection
datanode_3_1  | 2020-04-17 18:01:26,723 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.LeaderElection: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1: begin an election at term 1 for -1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_3_1  | 2020-04-17 18:01:26,724 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown LeaderElection
datanode_3_1  | 2020-04-17 18:01:26,724 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3_1  | 2020-04-17 18:01:26,724 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E0E2714D3E07 with new leaderId: 4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:01:26,725 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: change Leader from null to 4e682fff-8411-4efe-954f-416a0833f687 at term 1 for becomeLeader, leader elected after 5965ms
datanode_3_1  | 2020-04-17 18:01:26,738 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3_1  | 2020-04-17 18:01:26,738 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3_1  | 2020-04-17 18:01:26,740 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07
datanode_3_1  | 2020-04-17 18:01:26,746 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:01:26,751 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_5_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_5_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_5_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_5_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_5_1  | 	... 13 more
datanode_5_1  | 2020-04-17 18:01:24,061 [grpc-default-executor-1] WARN impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed groupAdd* GroupManagementRequest:client-E58054FBBCED->7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A, cid=0, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_5_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_5_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_5_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_5_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_5_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_5_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_5_1  | 	... 13 more
datanode_5_1  | 2020-04-17 18:01:24,997 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "3a881537-d96e-4768-950c-d38c014c699a"
datanode_5_1  | .
datanode_5_1  | 2020-04-17 18:01:26,251 [Thread-23] INFO impl.FollowerState: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-FollowerState: change to CANDIDATE, lastRpcTime:5240ms, electionTimeout:5191ms
datanode_5_1  | 2020-04-17 18:01:26,253 [Thread-23] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown FollowerState
datanode_5_1  | 2020-04-17 18:01:26,253 [Thread-23] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5_1  | 2020-04-17 18:01:26,255 [Thread-23] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start LeaderElection
datanode_5_1  | 2020-04-17 18:01:26,259 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.LeaderElection: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1: begin an election at term 1 for -1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858], old=null
datanode_5_1  | 2020-04-17 18:01:26,260 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown LeaderElection
datanode_5_1  | 2020-04-17 18:01:26,260 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5_1  | 2020-04-17 18:01:26,260 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-490C4A63FE1D with new leaderId: 7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:01:26,261 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: change Leader from null to 7fb5a271-9f73-44d7-9131-67b4b4fca07d at term 1 for becomeLeader, leader elected after 5861ms
datanode_5_1  | 2020-04-17 18:01:26,264 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5_1  | 2020-04-17 18:01:26,265 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1_1  | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1_1  | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1_1  | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.5.0.4:41850 remote=scm/10.5.0.71:9861]
datanode_1_1  | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1_1  | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1_1  | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1_1  | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1_1  | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:557)
datanode_1_1  | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1816)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)
datanode_1_1  | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)
datanode_1_1  | 2020-04-17 18:01:16,070 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1_1  | 2020-04-17 18:01:16,071 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1_1  | 2020-04-17 18:01:16,071 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 10592d3b-d195-479b-9b4d-1b30ca37f5cc at port 9858
datanode_1_1  | 2020-04-17 18:01:16,439 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start RPC server
datanode_1_1  | 2020-04-17 18:01:16,588 [Datanode State Machine Thread - 0] INFO server.GrpcService: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1_1  | 2020-04-17 18:01:21,095 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: addNew group-C6E015833419:[10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-C6E015833419:java.util.concurrent.CompletableFuture@2803e0ea[Not completed]
datanode_1_1  | 2020-04-17 18:01:21,471 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: new RaftServerImpl for group-C6E015833419:[10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_1_1  | 2020-04-17 18:01:21,485 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1_1  | 2020-04-17 18:01:21,491 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1_1  | 2020-04-17 18:01:21,492 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1_1  | 2020-04-17 18:01:21,496 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1_1  | 2020-04-17 18:01:21,506 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:01:21,546 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: ConfigurationManager, init=-1: [10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_1_1  | 2020-04-17 18:01:21,578 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1_1  | 2020-04-17 18:01:21,597 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1_1  | 2020-04-17 18:01:21,610 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2382b71f-a16d-4a1d-a152-c6e015833419 does not exist. Creating ...
datanode_1_1  | 2020-04-17 18:01:21,640 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2382b71f-a16d-4a1d-a152-c6e015833419/in_use.lock acquired by nodename 6@bfed8f6b6a8a
datanode_1_1  | 2020-04-17 18:01:21,663 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2382b71f-a16d-4a1d-a152-c6e015833419 has been successfully formatted.
datanode_1_1  | 2020-04-17 18:01:21,689 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-C6E015833419: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1_1  | 2020-04-17 18:01:21,743 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1_1  | 2020-04-17 18:01:21,755 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1_1  | 2020-04-17 18:01:21,817 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1_1  | 2020-04-17 18:01:21,848 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:01:21,872 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:01:21,913 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:01:22,079 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1_1  | 2020-04-17 18:01:22,139 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2382b71f-a16d-4a1d-a152-c6e015833419
datanode_1_1  | 2020-04-17 18:01:22,152 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1_1  | 2020-04-17 18:01:22,163 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:01:22,178 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:01:22,178 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1_1  | 2020-04-17 18:01:22,183 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:01:22,183 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2_1  | 	... 13 more
datanode_2_1  | 2020-04-17 18:01:25,171 [grpc-default-executor-0] WARN impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed groupAdd* GroupManagementRequest:client-B86D21A26FB9->9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7, cid=2, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_2_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_2_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_2_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_2_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_2_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_2_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_2_1  | 	... 13 more
datanode_2_1  | 2020-04-17 18:01:25,247 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | .
datanode_2_1  | 2020-04-17 18:01:26,603 [Thread-23] INFO impl.FollowerState: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-FollowerState: change to CANDIDATE, lastRpcTime:5069ms, electionTimeout:5036ms
datanode_2_1  | 2020-04-17 18:01:26,605 [Thread-23] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown FollowerState
datanode_2_1  | 2020-04-17 18:01:26,605 [Thread-23] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2_1  | 2020-04-17 18:01:26,613 [Thread-23] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderElection
datanode_2_1  | 2020-04-17 18:01:26,641 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1: begin an election at term 1 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858], old=null
datanode_2_1  | 2020-04-17 18:01:26,643 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderElection
datanode_2_1  | 2020-04-17 18:01:26,644 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2_1  | 2020-04-17 18:01:26,648 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BDE057308D8F with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:01:26,651 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 1 for becomeLeader, leader elected after 5906ms
datanode_2_1  | 2020-04-17 18:01:26,686 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2_1  | 2020-04-17 18:01:26,688 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1_1  | 2020-04-17 18:01:22,220 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1_1  | 2020-04-17 18:01:22,223 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1_1  | 2020-04-17 18:01:22,227 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1_1  | 2020-04-17 18:01:22,328 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:01:22,404 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1_1  | 2020-04-17 18:01:22,477 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:01:22,479 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1_1  | 2020-04-17 18:01:22,483 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1_1  | 2020-04-17 18:01:22,487 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1_1  | 2020-04-17 18:01:22,670 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419
datanode_1_1  | 2020-04-17 18:01:22,690 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419
datanode_1_1  | 2020-04-17 18:01:22,692 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: start as a follower, conf=-1: [10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:01:22,693 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1_1  | 2020-04-17 18:01:22,708 [pool-69-thread-1] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start FollowerState
datanode_1_1  | 2020-04-17 18:01:22,727 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C6E015833419,id=10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:01:22,728 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419
datanode_1_1  | 2020-04-17 18:01:22,798 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "2382b71f-a16d-4a1d-a152-c6e015833419"
datanode_1_1  | .
datanode_1_1  | 2020-04-17 18:01:27,847 [Thread-24] INFO impl.FollowerState: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-FollowerState: change to CANDIDATE, lastRpcTime:5142ms, electionTimeout:5118ms
datanode_1_1  | 2020-04-17 18:01:27,849 [Thread-24] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown FollowerState
datanode_1_1  | 2020-04-17 18:01:27,849 [Thread-24] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1_1  | 2020-04-17 18:01:27,852 [Thread-24] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderElection
datanode_1_1  | 2020-04-17 18:01:27,857 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.LeaderElection: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1: begin an election at term 1 for -1: [10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:01:27,858 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown LeaderElection
datanode_1_1  | 2020-04-17 18:01:27,859 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1_1  | 2020-04-17 18:01:27,859 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-C6E015833419 with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:01:27,859 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for becomeLeader, leader elected after 6120ms
datanode_1_1  | 2020-04-17 18:01:27,865 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1_1  | 2020-04-17 18:01:27,865 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1_1  | 2020-04-17 18:01:27,874 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419
datanode_1_1  | 2020-04-17 18:01:27,878 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:01:27,888 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1_1  | 2020-04-17 18:01:27,898 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1_1  | 2020-04-17 18:01:27,898 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1_1  | 2020-04-17 18:01:27,899 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1_1  | 2020-04-17 18:01:27,905 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderState
datanode_1_1  | 2020-04-17 18:01:27,921 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1_1  | 2020-04-17 18:01:27,952 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-LeaderElection1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419: set configuration 0: [10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_1_1  | 2020-04-17 18:01:28,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-C6E015833419-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2382b71f-a16d-4a1d-a152-c6e015833419/current/log_inprogress_0
datanode_1_1  | 2020-04-17 18:02:58,861 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: addNew group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-35283765472C:java.util.concurrent.CompletableFuture@42384101[Not completed]
datanode_3_1  | 2020-04-17 18:01:26,770 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3_1  | 2020-04-17 18:01:26,772 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3_1  | 2020-04-17 18:01:26,776 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3_1  | 2020-04-17 18:01:26,821 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start LeaderState
datanode_3_1  | 2020-04-17 18:01:26,868 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D38C014C699A with new leaderId: 7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_3_1  | 2020-04-17 18:01:26,869 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: change Leader from null to 7fb5a271-9f73-44d7-9131-67b4b4fca07d at term 1 for appendEntries, leader elected after 4968ms
datanode_3_1  | 2020-04-17 18:01:26,899 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A: set configuration 0: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null at 0
datanode_3_1  | 2020-04-17 18:01:26,916 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3_1  | 2020-04-17 18:01:26,916 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3_1  | 2020-04-17 18:01:27,001 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-LeaderElection1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07: set configuration 0: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null at 0
datanode_3_1  | 2020-04-17 18:01:27,027 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_3_1  | 2020-04-17 18:01:27,029 [grpc-default-executor-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:01:27,030 [grpc-default-executor-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:01:27,030 [Thread-29] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:01:27,125 [4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-E0E2714D3E07-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1deb4c57-bfe1-4217-b2d1-e0e2714d3e07/current/log_inprogress_0
datanode_3_1  | 2020-04-17 18:01:27,128 [4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-D38C014C699A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/current/log_inprogress_0
datanode_3_1  | 2020-04-17 18:01:27,156 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E8BAA29D9A7 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_3_1  | 2020-04-17 18:01:27,156 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 1 for appendEntries, leader elected after 2136ms
datanode_3_1  | 2020-04-17 18:01:27,228 [grpc-default-executor-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_3_1  | 2020-04-17 18:01:27,228 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3_1  | 2020-04-17 18:01:27,233 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/current/log_inprogress_0
datanode_3_1  | 2020-04-17 18:01:38,317 [ChunkWriter-24-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=0 B) is less than the container size (=1073741824 B).
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:01:38,318 [ChunkWriter-24-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
datanode_2_1  | 2020-04-17 18:01:26,690 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F
datanode_2_1  | 2020-04-17 18:01:26,715 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:01:26,720 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2_1  | 2020-04-17 18:01:26,745 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2_1  | 2020-04-17 18:01:26,745 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2_1  | 2020-04-17 18:01:26,746 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2_1  | 2020-04-17 18:01:26,785 [Thread-25] INFO impl.FollowerState: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-FollowerState: change to CANDIDATE, lastRpcTime:5006ms, electionTimeout:5005ms
datanode_2_1  | 2020-04-17 18:01:26,791 [Thread-25] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown FollowerState
datanode_2_1  | 2020-04-17 18:01:26,791 [Thread-25] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2_1  | 2020-04-17 18:01:26,791 [Thread-25] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderElection
datanode_2_1  | 2020-04-17 18:01:26,836 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderState
datanode_2_1  | 2020-04-17 18:01:26,827 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2: begin an election at term 1 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:01:26,954 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2_1  | 2020-04-17 18:01:27,036 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-LeaderElection1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858], old=null at 0
datanode_2_1  | 2020-04-17 18:01:27,066 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2: Election PASSED; received 1 response(s) [9297bad1-1bc3-407b-ab8e-258f1dd490f7<-313d3461-163d-4229-bfba-0697ea0de001#0:OK-t1] and 0 exception(s); 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7:t1, leader=null, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:01:27,067 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderElection
datanode_2_1  | 2020-04-17 18:01:27,076 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2_1  | 2020-04-17 18:01:27,080 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E8BAA29D9A7 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:01:27,080 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 1 for becomeLeader, leader elected after 5356ms
datanode_2_1  | 2020-04-17 18:01:27,081 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2_1  | 2020-04-17 18:01:27,081 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2_1  | 2020-04-17 18:01:27,081 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:01:27,082 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:01:27,082 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2_1  | 2020-04-17 18:01:27,082 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2_1  | 2020-04-17 18:01:27,082 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2_1  | 2020-04-17 18:01:27,083 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2_1  | 2020-04-17 18:01:27,092 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2_1  | 2020-04-17 18:01:27,092 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:01:27,093 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2_1  | 2020-04-17 18:01:27,099 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2_1  | 2020-04-17 18:01:27,104 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2_1  | 2020-04-17 18:01:27,105 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:01:27,110 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2_1  | 2020-04-17 18:01:27,114 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: new RaftServerImpl for group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1_1  | 2020-04-17 18:02:58,863 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:02:58,864 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: ConfigurationManager, init=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_1_1  | 2020-04-17 18:02:58,866 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1_1  | 2020-04-17 18:02:58,867 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1_1  | 2020-04-17 18:02:58,867 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c does not exist. Creating ...
datanode_1_1  | 2020-04-17 18:02:58,870 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/in_use.lock acquired by nodename 6@bfed8f6b6a8a
datanode_1_1  | 2020-04-17 18:02:58,872 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c has been successfully formatted.
datanode_1_1  | 2020-04-17 18:02:58,873 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-35283765472C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1_1  | 2020-04-17 18:02:58,873 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1_1  | 2020-04-17 18:02:58,874 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1_1  | 2020-04-17 18:02:58,875 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1_1  | 2020-04-17 18:02:58,875 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:02:58,876 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:02:58,880 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1_1  | 2020-04-17 18:02:58,884 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:02:58,885 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1_1  | 2020-04-17 18:02:58,885 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1_1  | 2020-04-17 18:02:58,885 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1_1  | 2020-04-17 18:02:58,885 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1_1  | 2020-04-17 18:02:58,886 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:02:58,886 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1_1  | 2020-04-17 18:02:58,887 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:02:58,887 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1_1  | 2020-04-17 18:02:58,887 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1_1  | 2020-04-17 18:02:58,887 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1_1  | 2020-04-17 18:02:58,887 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:02:58,888 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:02:58,889 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: start as a follower, conf=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:02:58,889 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1_1  | 2020-04-17 18:02:58,889 [pool-69-thread-1] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start FollowerState
datanode_1_1  | 2020-04-17 18:02:58,899 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:02:58,899 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:02:59,391 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | .
datanode_1_1  | 2020-04-17 18:03:03,907 [Thread-32] INFO impl.FollowerState: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-FollowerState: change to CANDIDATE, lastRpcTime:5017ms, electionTimeout:5005ms
datanode_1_1  | 2020-04-17 18:03:03,908 [Thread-32] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown FollowerState
datanode_1_1  | 2020-04-17 18:03:03,908 [Thread-32] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1_1  | 2020-04-17 18:03:03,908 [Thread-32] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderElection
datanode_4_1  | 2020-04-17 18:01:21,616 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:21,620 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4_1  | 2020-04-17 18:01:21,640 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4_1  | 2020-04-17 18:01:21,641 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4_1  | 2020-04-17 18:01:21,659 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4_1  | 2020-04-17 18:01:21,785 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:01:21,855 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4_1  | 2020-04-17 18:01:21,886 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:01:21,923 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4_1  | 2020-04-17 18:01:21,924 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4_1  | 2020-04-17 18:01:21,943 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4_1  | 2020-04-17 18:01:22,045 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE
datanode_4_1  | 2020-04-17 18:01:22,080 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE
datanode_4_1  | 2020-04-17 18:01:22,081 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: start as a follower, conf=-1: [313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:01:22,090 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4_1  | 2020-04-17 18:01:22,096 [pool-69-thread-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:01:22,164 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-51237E4653CE,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:01:22,165 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE
datanode_4_1  | 2020-04-17 18:01:22,289 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "fd94547c-946e-4f87-9733-51237e4653ce"
datanode_4_1  | .
datanode_4_1  | 2020-04-17 18:01:22,291 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: addNew group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-3E8BAA29D9A7:java.util.concurrent.CompletableFuture@69530793[Not completed]
datanode_4_1  | 2020-04-17 18:01:22,305 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001: new RaftServerImpl for group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_4_1  | 2020-04-17 18:01:22,325 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4_1  | 2020-04-17 18:01:22,325 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | 2020-04-17 18:01:22,326 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_4_1  | 2020-04-17 18:01:22,326 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_4_1  | 2020-04-17 18:01:22,326 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4_1  | 2020-04-17 18:01:22,326 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_4_1  | 2020-04-17 18:01:22,333 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4_1  | 2020-04-17 18:01:22,333 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4_1  | 2020-04-17 18:01:22,333 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 does not exist. Creating ...
datanode_4_1  | 2020-04-17 18:01:22,344 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/in_use.lock acquired by nodename 6@25eca77e6515
datanode_4_1  | 2020-04-17 18:01:22,356 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7 has been successfully formatted.
datanode_4_1  | 2020-04-17 18:01:22,357 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-3E8BAA29D9A7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4_1  | 2020-04-17 18:01:22,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_4_1  | 2020-04-17 18:01:22,380 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4_1  | 2020-04-17 18:01:22,380 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4_1  | 2020-04-17 18:01:22,380 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:01:22,380 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:22,381 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4_1  | 2020-04-17 18:01:22,381 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7
datanode_4_1  | 2020-04-17 18:01:22,381 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_4_1  | 2020-04-17 18:01:22,381 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:01:22,381 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:22,382 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4_1  | 2020-04-17 18:01:22,382 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:01:22,382 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4_1  | 2020-04-17 18:01:22,382 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1_1  | 2020-04-17 18:03:03,912 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.LeaderElection: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2: begin an election at term 1 for -1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:03:04,000 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.LeaderElection: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2: Election PASSED; received 1 response(s) [10592d3b-d195-479b-9b4d-1b30ca37f5cc<-7fb5a271-9f73-44d7-9131-67b4b4fca07d#0:OK-t1] and 0 exception(s); 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C:t1, leader=null, voted=10592d3b-d195-479b-9b4d-1b30ca37f5cc, raftlog=10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:03:04,000 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown LeaderElection
datanode_1_1  | 2020-04-17 18:03:04,000 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1_1  | 2020-04-17 18:03:04,000 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-35283765472C with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:03:04,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for becomeLeader, leader elected after 5127ms
datanode_1_1  | 2020-04-17 18:03:04,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1_1  | 2020-04-17 18:03:04,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1_1  | 2020-04-17 18:03:04,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:03:04,001 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:03:04,002 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1_1  | 2020-04-17 18:03:04,006 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1_1  | 2020-04-17 18:03:04,007 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1_1  | 2020-04-17 18:03:04,007 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1_1  | 2020-04-17 18:03:04,009 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1_1  | 2020-04-17 18:03:04,009 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:03:04,010 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1_1  | 2020-04-17 18:03:04,017 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1_1  | 2020-04-17 18:03:04,021 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1_1  | 2020-04-17 18:03:04,021 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1_1  | 2020-04-17 18:03:04,032 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:03:04,036 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderState
datanode_1_1  | 2020-04-17 18:03:04,037 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1_1  | 2020-04-17 18:03:04,040 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/current/log_inprogress_0
datanode_1_1  | 2020-04-17 18:03:04,040 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-LeaderElection2] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: set configuration 0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_1_1  | 2020-04-17 18:03:14,111 [ChunkWriter-29-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_4_1  | 2020-04-17 18:01:22,382 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4_1  | 2020-04-17 18:01:22,385 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4_1  | 2020-04-17 18:01:22,390 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:01:22,391 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4_1  | 2020-04-17 18:01:22,392 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:01:22,392 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4_1  | 2020-04-17 18:01:22,395 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4_1  | 2020-04-17 18:01:22,399 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4_1  | 2020-04-17 18:01:22,403 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:01:22,403 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:01:22,405 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:01:22,405 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4_1  | 2020-04-17 18:01:22,407 [pool-69-thread-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:01:22,427 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:01:22,427 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:01:24,607 [grpc-default-executor-0] WARN impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: Failed groupAdd* GroupManagementRequest:client-1680BCD6EAA9->313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7, cid=0, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_4_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 313d3461-163d-4229-bfba-0697ea0de001: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_4_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_4_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_4_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_4_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 313d3461-163d-4229-bfba-0697ea0de001: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:01:25,259 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | .
datanode_4_1  | 2020-04-17 18:01:25,303 [grpc-default-executor-0] WARN impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: Failed groupAdd* GroupManagementRequest:client-60EF5656E85C->313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7, cid=3, seq=0, RW, null, Add:group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858]
datanode_4_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 313d3461-163d-4229-bfba-0697ea0de001: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_4_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_4_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_5_1  | 2020-04-17 18:01:26,266 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D
datanode_5_1  | 2020-04-17 18:01:26,270 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5_1  | 2020-04-17 18:01:26,270 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_5_1  | 2020-04-17 18:01:26,275 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5_1  | 2020-04-17 18:01:26,275 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5_1  | 2020-04-17 18:01:26,276 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5_1  | 2020-04-17 18:01:26,286 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start LeaderState
datanode_5_1  | 2020-04-17 18:01:26,299 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5_1  | 2020-04-17 18:01:26,332 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-LeaderElection1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D: set configuration 0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858], old=null at 0
datanode_5_1  | 2020-04-17 18:01:26,380 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-490C4A63FE1D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9e30214a-3af8-4f12-a594-490c4a63fe1d/current/log_inprogress_0
datanode_5_1  | 2020-04-17 18:01:26,571 [Thread-25] INFO impl.FollowerState: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-FollowerState: change to CANDIDATE, lastRpcTime:5133ms, electionTimeout:5063ms
datanode_5_1  | 2020-04-17 18:01:26,572 [Thread-25] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown FollowerState
datanode_5_1  | 2020-04-17 18:01:26,572 [Thread-25] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5_1  | 2020-04-17 18:01:26,572 [Thread-25] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start LeaderElection
datanode_5_1  | 2020-04-17 18:01:26,579 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.LeaderElection: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2: begin an election at term 1 for -1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_5_1  | 2020-04-17 18:01:26,682 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.LeaderElection: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2: Election PASSED; received 1 response(s) [7fb5a271-9f73-44d7-9131-67b4b4fca07d<-4e682fff-8411-4efe-954f-416a0833f687#0:OK-t1] and 0 exception(s); 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A:t1, leader=null, voted=7fb5a271-9f73-44d7-9131-67b4b4fca07d, raftlog=7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_5_1  | 2020-04-17 18:01:26,682 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown LeaderElection
datanode_5_1  | 2020-04-17 18:01:26,683 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5_1  | 2020-04-17 18:01:26,683 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D38C014C699A with new leaderId: 7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:01:26,683 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: change Leader from null to 7fb5a271-9f73-44d7-9131-67b4b4fca07d at term 1 for becomeLeader, leader elected after 5318ms
datanode_5_1  | 2020-04-17 18:01:26,684 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5_1  | 2020-04-17 18:01:26,685 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5_1  | 2020-04-17 18:01:26,685 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A
datanode_5_1  | 2020-04-17 18:01:26,686 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5_1  | 2020-04-17 18:01:26,687 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_5_1  | 2020-04-17 18:01:26,691 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5_1  | 2020-04-17 18:01:26,691 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5_1  | 2020-04-17 18:01:26,692 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5_1  | 2020-04-17 18:01:26,693 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5_1  | 2020-04-17 18:01:26,694 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:01:26,695 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5_1  | 2020-04-17 18:01:26,698 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5_1  | 2020-04-17 18:01:26,725 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5_1  | 2020-04-17 18:01:26,726 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5_1  | 2020-04-17 18:01:26,773 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=967213056 B) is less than the container size (=1073741824 B).
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1_1  | 	... 13 more
datanode_1_1  | 2020-04-17 18:03:14,158 [ChunkWriter-29-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | 2020-04-17 18:03:14,184 [ChunkWriter-29-0] ERROR ratis.ContainerStateMachine: group-35283765472C: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_1_1  | localID: 104015239176716296
datanode_1_1  | blockCommitSequenceId: 0
datanode_1_1  |  logIndex 1 chunkName 104015239176716296_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1_1  | 2020-04-17 18:03:14,192 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : ContainerID 3 creation failed
datanode_1_1  | 2020-04-17 18:03:14,243 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
datanode_1_1  | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_1_1  | 2020-04-17 18:03:29,875 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_1_1  | 2020-04-17 18:04:14,113 [java.util.concurrent.ThreadPoolExecutor$Worker@34ec503e[State = -1, empty queue]] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1_1  | 2020-04-17 18:04:14,113 [java.util.concurrent.ThreadPoolExecutor$Worker@34ec503e[State = -1, empty queue]] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->7fb5a271-9f73-44d7-9131-67b4b4fca07d-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=5,entriesCount=1,lastEntry=(t:1, i:1)
datanode_1_1  | 2020-04-17 18:04:14,141 [java.util.concurrent.ThreadPoolExecutor$Worker@34ec503e[State = -1, empty queue]] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1_1  | 2020-04-17 18:04:14,146 [java.util.concurrent.ThreadPoolExecutor$Worker@34ec503e[State = -1, empty queue]] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->7fb5a271-9f73-44d7-9131-67b4b4fca07d-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6,entriesCount=1,lastEntry=(t:1, i:2)
datanode_1_1  | 2020-04-17 18:04:45,245 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove    LEADER 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C:t1, leader=10592d3b-d195-479b-9b4d-1b30ca37f5cc, voted=10592d3b-d195-479b-9b4d-1b30ca37f5cc, raftlog=10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null RUNNING
datanode_1_1  | 2020-04-17 18:04:45,247 [Command processor thread] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: shutdown
datanode_1_1  | 2020-04-17 18:04:45,247 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:04:45,248 [Command processor thread] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown LeaderState
datanode_1_1  | 2020-04-17 18:04:45,248 [Command processor thread] INFO impl.PendingRequests: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-PendingRequests: sendNotLeaderResponses
datanode_1_1  | 2020-04-17 18:04:45,248 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$458/0x000000084057cc40@251f4296] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->7fb5a271-9f73-44d7-9131-67b4b4fca07d-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_6_1  | Enabled profiling in kernel
datanode_6_1  | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
datanode_6_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_6_1  | 2020-04-17 18:00:53,201 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_6_1  | /************************************************************
datanode_6_1  | STARTUP_MSG: Starting HddsDatanodeService
datanode_6_1  | STARTUP_MSG:   host = 600e3df956a0/10.5.0.9
datanode_6_1  | STARTUP_MSG:   args = []
datanode_6_1  | STARTUP_MSG:   version = 3.2.0
datanode_1_1  | 2020-04-17 18:04:45,248 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$458/0x000000084057cc40@78a34f4b] WARN server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_1_1  | 2020-04-17 18:04:45,255 [grpc-default-executor-1] INFO server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->313d3461-163d-4229-bfba-0697ea0de001-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1_1  | 2020-04-17 18:04:45,258 [grpc-default-executor-0] INFO server.GrpcLogAppender: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->7fb5a271-9f73-44d7-9131-67b4b4fca07d-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_1_1  | 2020-04-17 18:04:45,262 [grpc-default-executor-1] INFO impl.FollowerInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->313d3461-163d-4229-bfba-0697ea0de001: nextIndex: updateUnconditionally 3 -> 1
datanode_1_1  | 2020-04-17 18:04:45,262 [grpc-default-executor-0] INFO impl.FollowerInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C->7fb5a271-9f73-44d7-9131-67b4b4fca07d: nextIndex: updateUnconditionally 3 -> 1
datanode_1_1  | 2020-04-17 18:04:45,271 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:04:45,272 [Command processor thread] INFO impl.StateMachineUpdater: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater: set stopIndex = 0
datanode_1_1  | 2020-04-17 18:04:45,274 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1_1  | 2020-04-17 18:04:45,284 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_1_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | 2020-04-17 18:04:45,285 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1_1  | 2020-04-17 18:04:45,285 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_1_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_1_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | 2020-04-17 18:04:45,286 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:04:45,286 [Command processor thread] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C: closes. applyIndex: 0
datanode_1_1  | 2020-04-17 18:04:45,288 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_1_1  | 2020-04-17 18:04:45,289 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C-SegmentedRaftLogWorker close()
datanode_1_1  | 2020-04-17 18:04:45,289 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:04:45,289 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:04:45,290 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-35283765472C
datanode_1_1  | 2020-04-17 18:04:45,292 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  |  command on datanode #10592d3b-d195-479b-9b4d-1b30ca37f5cc.
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_6_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-0.6.0-SNAPSHOT.jar
datanode_6_1  | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
datanode_6_1  | STARTUP_MSG:   java = 11.0.6
datanode_6_1  | ************************************************************/
datanode_6_1  | 2020-04-17 18:00:53,580 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_6_1  | 2020-04-17 18:00:55,166 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_6_1  | 2020-04-17 18:00:55,785 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_6_1  | 2020-04-17 18:00:57,171 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_6_1  | 2020-04-17 18:00:57,171 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_6_1  | 2020-04-17 18:00:58,480 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:600e3df956a0 ip:10.5.0.9
datanode_6_1  | 2020-04-17 18:00:58,846 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_6_1  | 2020-04-17 18:00:58,851 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89311358976
datanode_6_1  | 2020-04-17 18:00:58,854 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_6_1  | 2020-04-17 18:00:58,906 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_6_1  | 2020-04-17 18:00:59,061 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_6_1  | 2020-04-17 18:01:04,328 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_6_1  | 2020-04-17 18:01:04,651 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)
datanode_6_1  | 2020-04-17 18:01:05,017 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9858 (custom)
datanode_6_1  | 2020-04-17 18:01:05,027 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_6_1  | 2020-04-17 18:01:05,028 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_6_1  | 2020-04-17 18:01:05,034 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_6_1  | 2020-04-17 18:01:05,035 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_6_1  | 2020-04-17 18:01:06,462 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_6_1  | 2020-04-17 18:01:07,309 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_6_1  | 2020-04-17 18:01:07,418 [main] INFO util.log: Logging initialized @19095ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_6_1  | 2020-04-17 18:01:08,079 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_6_1  | 2020-04-17 18:01:08,085 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_6_1  | 2020-04-17 18:01:08,155 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_6_1  | 2020-04-17 18:01:08,189 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_6_1  | 2020-04-17 18:01:08,193 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_6_1  | 2020-04-17 18:01:08,194 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_6_1  | 2020-04-17 18:01:08,324 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
datanode_6_1  | 2020-04-17 18:01:08,350 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_6_1  | 2020-04-17 18:01:08,354 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
datanode_6_1  | 2020-04-17 18:01:08,499 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_6_1  | 2020-04-17 18:01:08,501 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_6_1  | 2020-04-17 18:01:08,506 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_6_1  | 2020-04-17 18:01:08,670 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77681ce4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_6_1  | 2020-04-17 18:01:08,673 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@367f0121{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_6_1  | 2020-04-17 18:01:09,270 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@15c487a8{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-0_6_0-SNAPSHOT_jar-_-any-10737250011704255434.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_6_1  | 2020-04-17 18:01:09,320 [main] INFO server.AbstractConnector: Started ServerConnector@48904d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_6_1  | 2020-04-17 18:01:09,323 [main] INFO server.Server: Started @21000ms
datanode_6_1  | 2020-04-17 18:01:09,339 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_6_1  | 2020-04-17 18:01:09,340 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_6_1  | 2020-04-17 18:01:09,345 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_6_1  | 2020-04-17 18:01:09,480 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@609a84dc] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_6_1  | 2020-04-17 18:01:10,463 [Datanode State Machine Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_6_1  | 2020-04-17 18:01:12,766 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_6_1  | 2020-04-17 18:01:13,767 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_6_1  | 2020-04-17 18:01:14,767 [Datanode State Machine Thread - 0] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_6_1  | 2020-04-17 18:01:15,307 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_6_1  | 2020-04-17 18:01:15,308 [Datanode State Machine Thread - 0] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_6_1  | 2020-04-17 18:01:15,309 [Datanode State Machine Thread - 0] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 575e95f6-9340-44b3-ae3e-c508b9a3128d at port 9858
datanode_6_1  | 2020-04-17 18:01:15,479 [Datanode State Machine Thread - 0] INFO impl.RaftServerProxy: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start RPC server
datanode_4_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_4_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_4_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 313d3461-163d-4229-bfba-0697ea0de001: Failed to add group-3E8BAA29D9A7:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] since the group already exists in the map.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:01:26,963 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_4_1  | 2020-04-17 18:01:26,964 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:01:26,965 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:01:26,965 [Thread-26] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:01:27,191 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3E8BAA29D9A7 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_4_1  | 2020-04-17 18:01:27,192 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 1 for appendEntries, leader elected after 4834ms
datanode_4_1  | 2020-04-17 18:01:27,254 [Thread-24] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-FollowerState: change to CANDIDATE, lastRpcTime:5158ms, electionTimeout:5145ms
datanode_4_1  | 2020-04-17 18:01:27,256 [Thread-24] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:01:27,257 [Thread-24] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4_1  | 2020-04-17 18:01:27,259 [Thread-24] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start LeaderElection
datanode_4_1  | 2020-04-17 18:01:27,260 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_4_1  | 2020-04-17 18:01:27,273 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4_1  | 2020-04-17 18:01:27,289 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.LeaderElection: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1: begin an election at term 1 for -1: [313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:01:27,291 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown LeaderElection
datanode_4_1  | 2020-04-17 18:01:27,292 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4_1  | 2020-04-17 18:01:27,294 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-51237E4653CE with new leaderId: 313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:01:27,294 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: change Leader from null to 313d3461-163d-4229-bfba-0697ea0de001 at term 1 for becomeLeader, leader elected after 5978ms
datanode_4_1  | 2020-04-17 18:01:27,302 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4_1  | 2020-04-17 18:01:27,307 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4_1  | 2020-04-17 18:01:27,309 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE
datanode_4_1  | 2020-04-17 18:01:27,316 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:01:27,317 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_4_1  | 2020-04-17 18:01:27,334 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4_1  | 2020-04-17 18:01:27,335 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4_1  | 2020-04-17 18:01:27,337 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4_1  | 2020-04-17 18:01:27,350 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start LeaderState
datanode_4_1  | 2020-04-17 18:01:27,366 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4_1  | 2020-04-17 18:01:27,370 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-LeaderElection1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE: set configuration 0: [313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_4_1  | 2020-04-17 18:01:27,398 [313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-51237E4653CE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fd94547c-946e-4f87-9733-51237e4653ce/current/log_inprogress_0
datanode_4_1  | 2020-04-17 18:01:27,398 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/current/log_inprogress_0
datanode_4_1  | 2020-04-17 18:01:38,620 [ChunkWriter-59-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=967659520 B) is less than the container size (=1073741824 B).
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:01:38,674 [ChunkWriter-59-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:01:38,362 [ChunkWriter-24-0] ERROR ratis.ContainerStateMachine: group-3E8BAA29D9A7: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_3_1  | localID: 104015232882245633
datanode_3_1  | blockCommitSequenceId: 0
datanode_3_1  |  logIndex 1 chunkName 104015232882245633_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3_1  | 2020-04-17 18:01:38,380 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : ContainerID 2 creation failed
datanode_3_1  | 2020-04-17 18:01:38,515 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
datanode_3_1  | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_3_1  | 2020-04-17 18:01:56,013 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_3_1  | 2020-04-17 18:03:09,512 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove  FOLLOWER 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7:t1, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_3_1  | 2020-04-17 18:03:09,515 [Command processor thread] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: shutdown
datanode_3_1  | 2020-04-17 18:03:09,515 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:03:09,516 [Command processor thread] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:03:09,516 [Command processor thread] INFO impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater: set stopIndex = 0
datanode_3_1  | 2020-04-17 18:03:09,516 [Thread-36] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:03:09,522 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3_1  | 2020-04-17 18:03:09,523 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_3_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:03:09,524 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3_1  | 2020-04-17 18:03:09,524 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_3_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:03:09,525 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_3_1  | 2020-04-17 18:03:09,526 [Command processor thread] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7: closes. applyIndex: 0
datanode_3_1  | 2020-04-17 18:03:09,527 [4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3_1  | 2020-04-17 18:03:09,532 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7-SegmentedRaftLogWorker close()
datanode_3_1  | 2020-04-17 18:03:09,533 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:03:09,533 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_6_1  | 2020-04-17 18:01:15,761 [Datanode State Machine Thread - 0] INFO server.GrpcService: 575e95f6-9340-44b3-ae3e-c508b9a3128d: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_6_1  | 2020-04-17 18:01:20,555 [Command processor thread] INFO impl.RaftServerProxy: 575e95f6-9340-44b3-ae3e-c508b9a3128d: addNew group-A0053359583B:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858] returns group-A0053359583B:java.util.concurrent.CompletableFuture@7dcd9b02[Not completed]
datanode_6_1  | 2020-04-17 18:01:20,656 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d: new RaftServerImpl for group-A0053359583B:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858] with ContainerStateMachine:uninitialized
datanode_6_1  | 2020-04-17 18:01:20,663 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_6_1  | 2020-04-17 18:01:20,667 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_6_1  | 2020-04-17 18:01:20,669 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_6_1  | 2020-04-17 18:01:20,670 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_6_1  | 2020-04-17 18:01:20,675 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_6_1  | 2020-04-17 18:01:20,695 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: ConfigurationManager, init=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_6_1  | 2020-04-17 18:01:20,707 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_6_1  | 2020-04-17 18:01:20,723 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_6_1  | 2020-04-17 18:01:20,739 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8501b544-67c4-4d39-a949-a0053359583b does not exist. Creating ...
datanode_6_1  | 2020-04-17 18:01:20,760 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8501b544-67c4-4d39-a949-a0053359583b/in_use.lock acquired by nodename 6@600e3df956a0
datanode_6_1  | 2020-04-17 18:01:20,768 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8501b544-67c4-4d39-a949-a0053359583b has been successfully formatted.
datanode_6_1  | 2020-04-17 18:01:20,779 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-A0053359583B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_6_1  | 2020-04-17 18:01:20,780 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_6_1  | 2020-04-17 18:01:20,784 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_6_1  | 2020-04-17 18:01:20,802 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_6_1  | 2020-04-17 18:01:20,802 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_6_1  | 2020-04-17 18:01:20,803 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:20,828 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.575e95f6-9340-44b3-ae3e-c508b9a3128d
datanode_6_1  | 2020-04-17 18:01:20,911 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_6_1  | 2020-04-17 18:01:20,955 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/8501b544-67c4-4d39-a949-a0053359583b
datanode_6_1  | 2020-04-17 18:01:20,957 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_6_1  | 2020-04-17 18:01:20,958 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_6_1  | 2020-04-17 18:01:20,961 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:20,962 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_6_1  | 2020-04-17 18:01:20,967 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:20,968 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_6_1  | 2020-04-17 18:01:20,978 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_6_1  | 2020-04-17 18:01:20,979 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_6_1  | 2020-04-17 18:01:20,982 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_6_1  | 2020-04-17 18:01:21,168 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_6_1  | 2020-04-17 18:01:21,190 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_6_1  | 2020-04-17 18:01:21,254 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_6_1  | 2020-04-17 18:01:21,254 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_6_1  | 2020-04-17 18:01:21,264 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_6_1  | 2020-04-17 18:01:21,265 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_6_1  | 2020-04-17 18:01:21,455 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B
datanode_6_1  | 2020-04-17 18:01:21,476 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B
datanode_6_1  | 2020-04-17 18:01:21,478 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: start as a follower, conf=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858], old=null
datanode_6_1  | 2020-04-17 18:01:21,478 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_6_1  | 2020-04-17 18:01:21,513 [pool-69-thread-1] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start FollowerState
datanode_6_1  | 2020-04-17 18:01:21,541 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-A0053359583B,id=575e95f6-9340-44b3-ae3e-c508b9a3128d
datanode_6_1  | 2020-04-17 18:01:21,549 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B
datanode_6_1  | 2020-04-17 18:01:21,633 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: "8501b544-67c4-4d39-a949-a0053359583b"
datanode_6_1  | .
datanode_2_1  | 2020-04-17 18:01:27,115 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2_1  | 2020-04-17 18:01:27,115 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2_1  | 2020-04-17 18:01:27,115 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2_1  | 2020-04-17 18:01:27,115 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:01:27,118 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderState
datanode_2_1  | 2020-04-17 18:01:27,123 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2_1  | 2020-04-17 18:01:27,147 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-LeaderElection2] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_2_1  | 2020-04-17 18:01:27,217 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/059e2fb4-f95b-4489-a61e-3e8baa29d9a7/current/log_inprogress_0
datanode_2_1  | 2020-04-17 18:01:27,236 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-BDE057308D8F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e9514b62-8305-4387-8371-bde057308d8f/current/log_inprogress_0
datanode_2_1  | 2020-04-17 18:01:38,388 [ChunkWriter-51-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=967680000 B) is less than the container size (=1073741824 B).
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2_1  | 	... 13 more
datanode_2_1  | 2020-04-17 18:01:38,468 [ChunkWriter-51-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0 , Message: ContainerID 2 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 2 creation failed
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:01:38,477 [ChunkWriter-51-0] ERROR ratis.ContainerStateMachine: group-3E8BAA29D9A7: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_2_1  | localID: 104015232882245633
datanode_2_1  | blockCommitSequenceId: 0
datanode_2_1  |  logIndex 1 chunkName 104015232882245633_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_2_1  | 2020-04-17 18:01:38,578 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : ContainerID 2 creation failed
datanode_2_1  | 2020-04-17 18:01:38,676 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
datanode_2_1  | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_2_1  | 2020-04-17 18:01:52,725 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_6_1  | 2020-04-17 18:01:21,640 [Command processor thread] INFO impl.RaftServerProxy: 575e95f6-9340-44b3-ae3e-c508b9a3128d: addNew group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] returns group-D38C014C699A:java.util.concurrent.CompletableFuture@5ce86862[Not completed]
datanode_6_1  | 2020-04-17 18:01:21,692 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d: new RaftServerImpl for group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] with ContainerStateMachine:uninitialized
datanode_6_1  | 2020-04-17 18:01:21,695 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_6_1  | 2020-04-17 18:01:21,696 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_6_1  | 2020-04-17 18:01:21,714 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_6_1  | 2020-04-17 18:01:21,715 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_6_1  | 2020-04-17 18:01:21,716 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_6_1  | 2020-04-17 18:01:21,716 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: ConfigurationManager, init=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null, confs=<EMPTY_MAP>
datanode_6_1  | 2020-04-17 18:01:21,717 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_6_1  | 2020-04-17 18:01:21,717 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_6_1  | 2020-04-17 18:01:21,720 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a does not exist. Creating ...
datanode_6_1  | 2020-04-17 18:01:21,721 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/in_use.lock acquired by nodename 6@600e3df956a0
datanode_6_1  | 2020-04-17 18:01:21,725 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a has been successfully formatted.
datanode_6_1  | 2020-04-17 18:01:21,734 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-D38C014C699A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_6_1  | 2020-04-17 18:01:21,759 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_6_1  | 2020-04-17 18:01:21,763 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_6_1  | 2020-04-17 18:01:21,763 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_6_1  | 2020-04-17 18:01:21,763 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_6_1  | 2020-04-17 18:01:21,763 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:21,764 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_6_1  | 2020-04-17 18:01:21,767 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a
datanode_6_1  | 2020-04-17 18:01:21,768 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_6_1  | 2020-04-17 18:01:21,768 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_6_1  | 2020-04-17 18:01:21,770 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:21,770 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_6_1  | 2020-04-17 18:01:21,781 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_6_1  | 2020-04-17 18:01:21,781 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_6_1  | 2020-04-17 18:01:21,781 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_6_1  | 2020-04-17 18:01:21,782 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_6_1  | 2020-04-17 18:01:21,782 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_6_1  | 2020-04-17 18:01:21,787 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_6_1  | 2020-04-17 18:01:21,787 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_6_1  | 2020-04-17 18:01:21,811 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_6_1  | 2020-04-17 18:01:21,811 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_6_1  | 2020-04-17 18:01:21,812 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_6_1  | 2020-04-17 18:01:21,812 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_6_1  | 2020-04-17 18:01:21,814 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A
datanode_6_1  | 2020-04-17 18:01:21,815 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A
datanode_6_1  | 2020-04-17 18:01:21,818 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: start as a follower, conf=-1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null
datanode_6_1  | 2020-04-17 18:01:21,820 [pool-69-thread-1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_6_1  | 2020-04-17 18:01:21,820 [pool-69-thread-1] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start FollowerState
datanode_6_1  | 2020-04-17 18:01:21,821 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D38C014C699A,id=575e95f6-9340-44b3-ae3e-c508b9a3128d
datanode_6_1  | 2020-04-17 18:01:21,821 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A
datanode_6_1  | 2020-04-17 18:01:24,670 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "3a881537-d96e-4768-950c-d38c014c699a"
datanode_6_1  | .
datanode_6_1  | 2020-04-17 18:01:24,836 [grpc-default-executor-0] WARN impl.RaftServerProxy: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed groupAdd* GroupManagementRequest:client-08D03B6D5B99->575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A, cid=1, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:04:45,293 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:04:45,294 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:04:45,294 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:04:45,294 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:04:45,294 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:05:15,244 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: remove group-35283765472C:null
datanode_1_1  | 2020-04-17 18:05:15,244 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_1_1  | 
datanode_1_1  | java.io.IOException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: Group group-35283765472C not found.
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_1_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_6_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_6_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_6_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_6_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_6_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_6_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_6_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_6_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_6_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_6_1  | 	... 13 more
datanode_6_1  | 2020-04-17 18:01:24,843 [grpc-default-executor-1] WARN impl.RaftServerProxy: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed groupAdd* GroupManagementRequest:client-0FDA77AA3BBE->575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A, cid=1, seq=0, RW, null, Add:group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858]
datanode_6_1  | java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyExistsException: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyNow(CompletableFuture.java:670)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:658)
datanode_6_1  | 	at java.base/java.util.concurrent.CompletableFuture.thenApplyAsync(CompletableFuture.java:2104)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:379)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:363)
datanode_6_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.lambda$groupManagement$0(GrpcAdminProtocolService.java:42)
datanode_6_1  | 	at org.apache.ratis.grpc.GrpcUtil.asyncCall(GrpcUtil.java:160)
datanode_6_1  | 	at org.apache.ratis.grpc.server.GrpcAdminProtocolService.groupManagement(GrpcAdminProtocolService.java:42)
datanode_6_1  | 	at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$MethodHandlers.invoke(AdminProtocolServiceGrpc.java:358)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:172)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
datanode_6_1  | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
datanode_6_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_6_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_6_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_6_1  | Caused by: org.apache.ratis.protocol.AlreadyExistsException: 575e95f6-9340-44b3-ae3e-c508b9a3128d: Failed to add group-D38C014C699A:[575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858] since the group already exists in the map.
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.addNew(RaftServerProxy.java:83)
datanode_6_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupAddAsync(RaftServerProxy.java:378)
datanode_6_1  | 	... 13 more
datanode_6_1  | 2020-04-17 18:01:26,573 [Thread-23] INFO impl.FollowerState: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-FollowerState: change to CANDIDATE, lastRpcTime:5060ms, electionTimeout:5042ms
datanode_6_1  | 2020-04-17 18:01:26,575 [Thread-23] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: shutdown FollowerState
datanode_6_1  | 2020-04-17 18:01:26,575 [Thread-23] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_6_1  | 2020-04-17 18:01:26,578 [Thread-23] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start LeaderElection
datanode_6_1  | 2020-04-17 18:01:26,590 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.LeaderElection: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1: begin an election at term 1 for -1: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858], old=null
datanode_6_1  | 2020-04-17 18:01:26,593 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: shutdown LeaderElection
datanode_6_1  | 2020-04-17 18:01:26,595 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_6_1  | 2020-04-17 18:01:26,595 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-A0053359583B with new leaderId: 575e95f6-9340-44b3-ae3e-c508b9a3128d
datanode_3_1  | 2020-04-17 18:03:09,534 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-3E8BAA29D9A7
datanode_3_1  | 2020-04-17 18:03:09,537 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  |  command on datanode #4e682fff-8411-4efe-954f-416a0833f687.
datanode_3_1  | 2020-04-17 18:03:09,537 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:09,538 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:03:09,538 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:09,539 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:03:09,539 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:09,539 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:03:09,541 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:09,541 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:03:09,542 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:09,542 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_6_1  | 2020-04-17 18:01:26,596 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: change Leader from null to 575e95f6-9340-44b3-ae3e-c508b9a3128d at term 1 for becomeLeader, leader elected after 5815ms
datanode_6_1  | 2020-04-17 18:01:26,607 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_6_1  | 2020-04-17 18:01:26,608 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_6_1  | 2020-04-17 18:01:26,622 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B
datanode_6_1  | 2020-04-17 18:01:26,626 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_6_1  | 2020-04-17 18:01:26,652 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_6_1  | 2020-04-17 18:01:26,652 [grpc-default-executor-0] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_6_1  | 2020-04-17 18:01:26,653 [grpc-default-executor-0] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: shutdown FollowerState
datanode_6_1  | 2020-04-17 18:01:26,655 [Thread-25] INFO impl.FollowerState: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_6_1  | 2020-04-17 18:01:26,674 [grpc-default-executor-0] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start FollowerState
datanode_6_1  | 2020-04-17 18:01:26,678 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_6_1  | 2020-04-17 18:01:26,678 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_6_1  | 2020-04-17 18:01:26,680 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_6_1  | 2020-04-17 18:01:26,728 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.RoleInfo: 575e95f6-9340-44b3-ae3e-c508b9a3128d: start LeaderState
datanode_6_1  | 2020-04-17 18:01:26,818 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_6_1  | 2020-04-17 18:01:26,882 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-LeaderElection1] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B: set configuration 0: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858], old=null at 0
datanode_6_1  | 2020-04-17 18:01:26,909 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D38C014C699A with new leaderId: 7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_6_1  | 2020-04-17 18:01:26,910 [grpc-default-executor-0] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: change Leader from null to 7fb5a271-9f73-44d7-9131-67b4b4fca07d at term 1 for appendEntries, leader elected after 5171ms
datanode_6_1  | 2020-04-17 18:01:26,948 [grpc-default-executor-0] INFO impl.RaftServerImpl: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A: set configuration 0: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null at 0
datanode_6_1  | 2020-04-17 18:01:26,955 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_6_1  | 2020-04-17 18:01:27,073 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-A0053359583B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8501b544-67c4-4d39-a949-a0053359583b/current/log_inprogress_0
datanode_6_1  | 2020-04-17 18:01:27,073 [575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 575e95f6-9340-44b3-ae3e-c508b9a3128d@group-D38C014C699A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/current/log_inprogress_0
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:01:38,700 [ChunkWriter-59-0] ERROR ratis.ContainerStateMachine: group-3E8BAA29D9A7: writeChunk writeStateMachineData failed: blockIdcontainerID: 2
datanode_4_1  | localID: 104015232882245633
datanode_4_1  | blockCommitSequenceId: 0
datanode_4_1  |  logIndex 1 chunkName 104015232882245633_chunk_1 Error message: ContainerID 2 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_4_1  | 2020-04-17 18:01:38,705 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : ContainerID 2 creation failed
datanode_4_1  | 2020-04-17 18:01:38,746 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
datanode_4_1  | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_4_1  | 2020-04-17 18:01:53,361 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #2 does not exist in datanode. Container close failed.
datanode_4_1  | 2020-04-17 18:02:59,217 [grpc-default-executor-0] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: addNew group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-35283765472C:java.util.concurrent.CompletableFuture@2586866f[Not completed]
datanode_4_1  | 2020-04-17 18:02:59,218 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001: new RaftServerImpl for group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_4_1  | 2020-04-17 18:02:59,220 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4_1  | 2020-04-17 18:02:59,220 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | 2020-04-17 18:02:59,220 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_4_1  | 2020-04-17 18:02:59,220 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_4_1  | 2020-04-17 18:02:59,221 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4_1  | 2020-04-17 18:02:59,221 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: ConfigurationManager, init=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_4_1  | 2020-04-17 18:02:59,223 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5_1  | 2020-04-17 18:01:26,775 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:01:26,776 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5_1  | 2020-04-17 18:01:26,776 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5_1  | 2020-04-17 18:01:26,777 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5_1  | 2020-04-17 18:01:26,783 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5_1  | 2020-04-17 18:01:26,793 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start LeaderState
datanode_5_1  | 2020-04-17 18:01:26,794 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5_1  | 2020-04-17 18:01:26,807 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a881537-d96e-4768-950c-d38c014c699a/current/log_inprogress_0
datanode_5_1  | 2020-04-17 18:01:26,822 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A-LeaderElection2] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-D38C014C699A: set configuration 0: [575e95f6-9340-44b3-ae3e-c508b9a3128d:10.5.0.9:9858, 7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858], old=null at 0
datanode_5_1  | 2020-04-17 18:02:59,351 [grpc-default-executor-0] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: addNew group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-35283765472C:java.util.concurrent.CompletableFuture@210c3aa1[Not completed]
datanode_5_1  | 2020-04-17 18:02:59,352 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: new RaftServerImpl for group-35283765472C:[7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: ConfigurationManager, init=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_5_1  | 2020-04-17 18:02:59,353 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5_1  | 2020-04-17 18:02:59,354 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5_1  | 2020-04-17 18:02:59,354 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c does not exist. Creating ...
datanode_5_1  | 2020-04-17 18:02:59,360 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/in_use.lock acquired by nodename 6@ea97b2d03cd4
datanode_5_1  | 2020-04-17 18:02:59,362 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c has been successfully formatted.
datanode_5_1  | 2020-04-17 18:02:59,362 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-35283765472C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5_1  | 2020-04-17 18:02:59,362 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_5_1  | 2020-04-17 18:02:59,363 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5_1  | 2020-04-17 18:02:59,363 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5_1  | 2020-04-17 18:02:59,363 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5_1  | 2020-04-17 18:02:59,363 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5_1  | 2020-04-17 18:02:59,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5_1  | 2020-04-17 18:02:59,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5_1  | 2020-04-17 18:02:59,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5_1  | 2020-04-17 18:02:59,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5_1  | 2020-04-17 18:02:59,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5_1  | 2020-04-17 18:02:59,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:02:59,366 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5_1  | 2020-04-17 18:02:59,370 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5_1  | 2020-04-17 18:02:59,370 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2_1  | 2020-04-17 18:02:38,344 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6,entriesCount=1,lastEntry=(t:1, i:1)
datanode_2_1  | 2020-04-17 18:02:38,351 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=7,entriesCount=1,lastEntry=(t:1, i:2)
datanode_2_1  | 2020-04-17 18:02:38,358 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=6,entriesCount=1,lastEntry=(t:1, i:1)
datanode_2_1  | 2020-04-17 18:02:38,359 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=7,entriesCount=1,lastEntry=(t:1, i:2)
datanode_2_1  | 2020-04-17 18:03:09,678 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove    LEADER 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7:t1, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_2_1  | 2020-04-17 18:03:09,683 [Command processor thread] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: shutdown
datanode_2_1  | 2020-04-17 18:03:09,683 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:03:09,684 [Command processor thread] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderState
datanode_2_1  | 2020-04-17 18:03:09,684 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$443/0x0000000840592040@20e1109c] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2_1  | 2020-04-17 18:03:09,685 [Command processor thread] INFO impl.PendingRequests: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-PendingRequests: sendNotLeaderResponses
datanode_2_1  | 2020-04-17 18:03:09,685 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$443/0x0000000840592040@4acd2bc7] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2_1  | 2020-04-17 18:03:09,692 [grpc-default-executor-1] INFO server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->313d3461-163d-4229-bfba-0697ea0de001-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_2_1  | 2020-04-17 18:03:09,695 [grpc-default-executor-2] INFO server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->4e682fff-8411-4efe-954f-416a0833f687-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_2_1  | 2020-04-17 18:03:09,705 [grpc-default-executor-1] INFO impl.FollowerInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->313d3461-163d-4229-bfba-0697ea0de001: nextIndex: updateUnconditionally 3 -> 1
datanode_2_1  | 2020-04-17 18:03:09,711 [grpc-default-executor-2] INFO impl.FollowerInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7->4e682fff-8411-4efe-954f-416a0833f687: nextIndex: updateUnconditionally 3 -> 1
datanode_2_1  | 2020-04-17 18:03:09,721 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:03:09,724 [Command processor thread] INFO impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater: set stopIndex = 0
datanode_2_1  | 2020-04-17 18:03:09,731 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2_1  | 2020-04-17 18:03:09,732 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_2_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:03:09,732 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2_1  | 2020-04-17 18:03:09,732 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_2_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:03:09,733 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:03:09,733 [Command processor thread] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7: closes. applyIndex: 0
datanode_2_1  | 2020-04-17 18:03:09,735 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2_1  | 2020-04-17 18:03:09,737 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7-SegmentedRaftLogWorker close()
datanode_2_1  | 2020-04-17 18:03:09,738 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_1_1  | 	... 4 more
datanode_1_1  | 2020-04-17 18:05:15,245 [Command processor thread] INFO impl.RaftServerProxy: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: addNew group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-432946335976:java.util.concurrent.CompletableFuture@3728e6c[Not completed]
datanode_1_1  | 2020-04-17 18:05:15,248 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: new RaftServerImpl for group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_1_1  | 2020-04-17 18:05:15,248 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1_1  | 2020-04-17 18:05:15,249 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1_1  | 2020-04-17 18:05:15,249 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_1_1  | 2020-04-17 18:05:15,249 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_1_1  | 2020-04-17 18:05:15,250 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:05:15,250 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: ConfigurationManager, init=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_1_1  | 2020-04-17 18:05:15,250 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1_1  | 2020-04-17 18:05:15,251 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1_1  | 2020-04-17 18:05:15,251 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 does not exist. Creating ...
datanode_1_1  | 2020-04-17 18:05:15,254 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/in_use.lock acquired by nodename 6@bfed8f6b6a8a
datanode_1_1  | 2020-04-17 18:05:15,255 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 has been successfully formatted.
datanode_1_1  | 2020-04-17 18:05:15,256 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-432946335976: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1_1  | 2020-04-17 18:05:15,256 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_1_1  | 2020-04-17 18:05:15,256 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1_1  | 2020-04-17 18:05:15,256 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1_1  | 2020-04-17 18:05:15,257 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:05:15,257 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:05:15,257 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:05:15,259 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1_1  | 2020-04-17 18:05:15,259 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976
datanode_1_1  | 2020-04-17 18:05:15,260 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1_1  | 2020-04-17 18:05:15,260 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:05:15,260 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:05:15,261 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1_1  | 2020-04-17 18:05:15,261 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1_1  | 2020-04-17 18:05:15,261 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1_1  | 2020-04-17 18:05:15,261 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1_1  | 2020-04-17 18:05:15,261 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1_1  | 2020-04-17 18:05:15,262 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1_1  | 2020-04-17 18:05:15,263 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:05:15,264 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1_1  | 2020-04-17 18:05:15,264 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1_1  | 2020-04-17 18:05:15,264 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1_1  | 2020-04-17 18:05:15,264 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1_1  | 2020-04-17 18:05:15,265 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1_1  | 2020-04-17 18:05:15,265 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976
datanode_1_1  | 2020-04-17 18:05:15,265 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976
datanode_1_1  | 2020-04-17 18:05:15,265 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: start as a follower, conf=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:05:15,266 [pool-69-thread-1] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1_1  | 2020-04-17 18:05:15,266 [pool-69-thread-1] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start FollowerState
datanode_1_1  | 2020-04-17 18:05:15,266 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-432946335976,id=10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:05:15,266 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976
datanode_4_1  | 2020-04-17 18:02:59,229 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4_1  | 2020-04-17 18:02:59,229 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c does not exist. Creating ...
datanode_4_1  | 2020-04-17 18:02:59,235 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/in_use.lock acquired by nodename 6@25eca77e6515
datanode_4_1  | 2020-04-17 18:02:59,239 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c has been successfully formatted.
datanode_4_1  | 2020-04-17 18:02:59,240 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-35283765472C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4_1  | 2020-04-17 18:02:59,240 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_4_1  | 2020-04-17 18:02:59,240 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4_1  | 2020-04-17 18:02:59,241 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4_1  | 2020-04-17 18:02:59,241 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:02:59,241 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:03:09,738 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:03:09,739 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-3E8BAA29D9A7
datanode_2_1  | 2020-04-17 18:03:09,742 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  |  command on datanode #9297bad1-1bc3-407b-ab8e-258f1dd490f7.
datanode_2_1  | 2020-04-17 18:03:09,742 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-3E8BAA29D9A7:null
datanode_2_1  | 2020-04-17 18:03:09,743 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:03:09,743 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-3E8BAA29D9A7:null
datanode_2_1  | 2020-04-17 18:03:09,743 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:03:09,744 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-3E8BAA29D9A7:null
datanode_2_1  | 2020-04-17 18:03:09,744 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:03:09,744 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-3E8BAA29D9A7:null
datanode_2_1  | 2020-04-17 18:03:09,745 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:03:09,750 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-3E8BAA29D9A7:null
datanode_2_1  | 2020-04-17 18:03:09,750 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 2020-04-17 18:02:59,241 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4_1  | 2020-04-17 18:02:59,242 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:02:59,243 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4_1  | 2020-04-17 18:02:59,243 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4_1  | 2020-04-17 18:02:59,243 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4_1  | 2020-04-17 18:02:59,243 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4_1  | 2020-04-17 18:02:59,244 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:02:59,244 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4_1  | 2020-04-17 18:02:59,253 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:02:59,266 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4_1  | 2020-04-17 18:02:59,266 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4_1  | 2020-04-17 18:02:59,267 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4_1  | 2020-04-17 18:02:59,267 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:02:59,267 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:02:59,268 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: start as a follower, conf=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_4_1  | 2020-04-17 18:02:59,268 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4_1  | 2020-04-17 18:02:59,268 [pool-69-thread-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:02:59,269 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:02:59,269 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:03:03,986 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_4_1  | 2020-04-17 18:03:03,986 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:03:03,987 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:03:03,987 [Thread-79] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:03:04,070 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-35283765472C with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_4_1  | 2020-04-17 18:03:04,070 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for appendEntries, leader elected after 4829ms
datanode_4_1  | 2020-04-17 18:03:04,090 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: set configuration 0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_4_1  | 2020-04-17 18:03:04,090 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4_1  | 2020-04-17 18:03:04,094 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/current/log_inprogress_0
datanode_1_1  | 2020-04-17 18:05:15,401 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "dd6c75a4-93d3-45f4-84a9-432946335976"
datanode_1_1  | .
datanode_1_1  | 2020-04-17 18:05:20,276 [Thread-51] INFO impl.FollowerState: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-FollowerState: change to CANDIDATE, lastRpcTime:5010ms, electionTimeout:5010ms
datanode_1_1  | 2020-04-17 18:05:20,277 [Thread-51] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown FollowerState
datanode_1_1  | 2020-04-17 18:05:20,277 [Thread-51] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1_1  | 2020-04-17 18:05:20,277 [Thread-51] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderElection
datanode_1_1  | 2020-04-17 18:05:20,283 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.LeaderElection: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3: begin an election at term 1 for -1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:05:20,323 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.LeaderElection: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3: Election PASSED; received 1 response(s) [10592d3b-d195-479b-9b4d-1b30ca37f5cc<-4e682fff-8411-4efe-954f-416a0833f687#0:OK-t1] and 0 exception(s); 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976:t1, leader=null, voted=10592d3b-d195-479b-9b4d-1b30ca37f5cc, raftlog=10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_1_1  | 2020-04-17 18:05:20,323 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: shutdown LeaderElection
datanode_1_1  | 2020-04-17 18:05:20,324 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1_1  | 2020-04-17 18:05:20,324 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-432946335976 with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_1_1  | 2020-04-17 18:05:20,324 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for becomeLeader, leader elected after 5067ms
datanode_1_1  | 2020-04-17 18:05:20,325 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1_1  | 2020-04-17 18:05:20,325 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1_1  | 2020-04-17 18:05:20,325 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976
datanode_1_1  | 2020-04-17 18:05:20,326 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1_1  | 2020-04-17 18:05:20,326 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1_1  | 2020-04-17 18:05:20,327 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1_1  | 2020-04-17 18:05:20,328 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1_1  | 2020-04-17 18:05:20,328 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1_1  | 2020-04-17 18:05:20,328 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1_1  | 2020-04-17 18:05:20,328 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:05:20,328 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1_1  | 2020-04-17 18:05:20,329 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1_1  | 2020-04-17 18:05:20,329 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1_1  | 2020-04-17 18:05:20,329 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:05:20,330 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1_1  | 2020-04-17 18:05:20,330 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1_1  | 2020-04-17 18:05:20,330 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1_1  | 2020-04-17 18:05:20,330 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1_1  | 2020-04-17 18:05:20,330 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1_1  | 2020-04-17 18:05:20,331 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1_1  | 2020-04-17 18:05:20,331 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.RoleInfo: 10592d3b-d195-479b-9b4d-1b30ca37f5cc: start LeaderState
datanode_1_1  | 2020-04-17 18:05:20,332 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1_1  | 2020-04-17 18:05:20,337 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/current/log_inprogress_0
datanode_1_1  | 2020-04-17 18:05:20,344 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-LeaderElection3] INFO impl.RaftServerImpl: 10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976: set configuration 0: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:03:09,693 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 4e682fff-8411-4efe-954f-416a0833f687: Completed APPEND_ENTRIES, lastRequest: 9297bad1-1bc3-407b-ab8e-258f1dd490f7->4e682fff-8411-4efe-954f-416a0833f687#7-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=6
datanode_3_1  | 2020-04-17 18:03:39,510 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-3E8BAA29D9A7:null
datanode_3_1  | 2020-04-17 18:03:39,510 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-3E8BAA29D9A7 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:04:39,510 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: addNew group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-197A8EB51962:java.util.concurrent.CompletableFuture@bd2f478[Not completed]
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687: new RaftServerImpl for group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3_1  | 2020-04-17 18:04:39,512 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3_1  | 2020-04-17 18:04:39,513 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_3_1  | 2020-04-17 18:04:39,513 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:04:39,514 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3_1  | 2020-04-17 18:04:39,514 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 does not exist. Creating ...
datanode_3_1  | 2020-04-17 18:04:39,517 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/in_use.lock acquired by nodename 6@a8c66e3e0d0a
datanode_3_1  | 2020-04-17 18:04:39,519 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 has been successfully formatted.
datanode_3_1  | 2020-04-17 18:04:39,519 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-197A8EB51962: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3_1  | 2020-04-17 18:04:39,519 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3_1  | 2020-04-17 18:04:39,520 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:04:39,549 [grpc-default-executor-3] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: addNew group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-197A8EB51962:java.util.concurrent.CompletableFuture@59f83dda[Not completed]
datanode_2_1  | 2020-04-17 18:04:39,551 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: new RaftServerImpl for group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_2_1  | 2020-04-17 18:04:39,555 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2_1  | 2020-04-17 18:04:39,556 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2_1  | 2020-04-17 18:04:39,556 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 does not exist. Creating ...
datanode_2_1  | 2020-04-17 18:04:39,557 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/in_use.lock acquired by nodename 6@06da621581ba
datanode_2_1  | 2020-04-17 18:04:39,559 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 has been successfully formatted.
datanode_2_1  | 2020-04-17 18:04:39,560 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-197A8EB51962: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2_1  | 2020-04-17 18:04:39,561 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_2_1  | 2020-04-17 18:04:39,561 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2_1  | 2020-04-17 18:04:39,562 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2_1  | 2020-04-17 18:04:39,562 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:04:39,562 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:04:39,562 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:04:39,562 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2_1  | 2020-04-17 18:04:39,563 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962
datanode_2_1  | 2020-04-17 18:04:39,563 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2_1  | 2020-04-17 18:04:39,563 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:04:39,563 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:04:39,563 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2_1  | 2020-04-17 18:04:39,564 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2_1  | 2020-04-17 18:04:39,564 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2_1  | 2020-04-17 18:04:39,564 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2_1  | 2020-04-17 18:04:39,564 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2_1  | 2020-04-17 18:04:39,564 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2_1  | 2020-04-17 18:04:39,566 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:04:39,567 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2_1  | 2020-04-17 18:04:39,567 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2_1  | 2020-04-17 18:04:39,568 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2_1  | 2020-04-17 18:04:39,568 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2_1  | 2020-04-17 18:04:39,568 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2_1  | 2020-04-17 18:04:39,568 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:04:39,568 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
om_1          | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
om_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1          | 2020-04-17 18:00:50,300 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1          | /************************************************************
om_1          | STARTUP_MSG: Starting OzoneManager
om_1          | STARTUP_MSG:   host = 4387223f42c5/10.5.0.70
om_1          | STARTUP_MSG:   args = [--init]
om_1          | STARTUP_MSG:   version = 3.2.0
datanode_4_1  | 2020-04-17 18:03:09,690 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 313d3461-163d-4229-bfba-0697ea0de001: Completed APPEND_ENTRIES, lastRequest: 9297bad1-1bc3-407b-ab8e-258f1dd490f7->313d3461-163d-4229-bfba-0697ea0de001#7-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=6
datanode_4_1  | 2020-04-17 18:03:09,747 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove  FOLLOWER 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7:t1, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_4_1  | 2020-04-17 18:03:09,749 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: shutdown
datanode_4_1  | 2020-04-17 18:03:09,749 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3E8BAA29D9A7,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:03:09,749 [Command processor thread] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:03:09,750 [Thread-29] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:03:09,751 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 2020-04-17 18:03:09,751 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:03:09,751 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 2020-04-17 18:03:09,751 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-3E8BAA29D9A7 as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:03:09,752 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:03:09,752 [Command processor thread] INFO impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-StateMachineUpdater: set stopIndex = 0
datanode_4_1  | 2020-04-17 18:03:09,753 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7: closes. applyIndex: 0
datanode_4_1  | 2020-04-17 18:03:09,754 [313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_4_1  | 2020-04-17 18:03:09,759 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7-SegmentedRaftLogWorker close()
datanode_4_1  | 2020-04-17 18:03:09,760 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:03:09,760 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:03:09,760 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-3E8BAA29D9A7
datanode_4_1  | 2020-04-17 18:03:09,768 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  |  command on datanode #313d3461-163d-4229-bfba-0697ea0de001.
datanode_4_1  | 2020-04-17 18:03:09,769 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:09,769 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:09,769 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:09,770 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_2_1  | 2020-04-17 18:04:39,569 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:04:39,569 [pool-69-thread-1] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2_1  | 2020-04-17 18:04:39,569 [pool-69-thread-1] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start FollowerState
datanode_2_1  | 2020-04-17 18:04:39,569 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:04:39,569 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:04:44,623 [Thread-51] INFO impl.FollowerState: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-FollowerState: change to CANDIDATE, lastRpcTime:5053ms, electionTimeout:5052ms
datanode_2_1  | 2020-04-17 18:04:44,623 [Thread-51] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown FollowerState
datanode_2_1  | 2020-04-17 18:04:44,624 [Thread-51] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2_1  | 2020-04-17 18:04:44,624 [Thread-51] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderElection
datanode_2_1  | 2020-04-17 18:04:44,627 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection3] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection3: begin an election at term 1 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:04:44,695 [grpc-default-executor-3] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: changes role from CANDIDATE to FOLLOWER at term 2 for recognizeCandidate:4e682fff-8411-4efe-954f-416a0833f687
datanode_2_1  | 2020-04-17 18:04:44,695 [grpc-default-executor-3] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderElection
datanode_2_1  | 2020-04-17 18:04:44,696 [grpc-default-executor-3] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start FollowerState
datanode_2_1  | 2020-04-17 18:04:44,723 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection3] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection3: Election REJECTED; received 2 response(s) [9297bad1-1bc3-407b-ab8e-258f1dd490f7<-4e682fff-8411-4efe-954f-416a0833f687#0:FAIL-t1, 9297bad1-1bc3-407b-ab8e-258f1dd490f7<-313d3461-163d-4229-bfba-0697ea0de001#0:FAIL-t1] and 0 exception(s); 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962:t2, leader=null, voted=null, raftlog=9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:04:49,734 [Thread-56] INFO impl.FollowerState: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-FollowerState: change to CANDIDATE, lastRpcTime:5037ms, electionTimeout:5037ms
datanode_2_1  | 2020-04-17 18:04:49,734 [Thread-56] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown FollowerState
datanode_2_1  | 2020-04-17 18:04:49,734 [Thread-56] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2_1  | 2020-04-17 18:04:49,734 [Thread-56] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderElection
datanode_2_1  | 2020-04-17 18:04:49,737 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4: begin an election at term 3 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:04:49,754 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.LeaderElection: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4: Election PASSED; received 1 response(s) [9297bad1-1bc3-407b-ab8e-258f1dd490f7<-313d3461-163d-4229-bfba-0697ea0de001#0:OK-t3] and 0 exception(s); 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962:t3, leader=null, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_2_1  | 2020-04-17 18:04:49,754 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderElection
datanode_2_1  | 2020-04-17 18:04:49,755 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_2_1  | 2020-04-17 18:04:49,758 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-197A8EB51962 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:04:49,760 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 3 for becomeLeader, leader elected after 10197ms
datanode_2_1  | 2020-04-17 18:04:49,760 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2_1  | 2020-04-17 18:04:49,760 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2_1  | 2020-04-17 18:04:49,763 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:04:49,764 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2_1  | 2020-04-17 18:04:49,765 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2_1  | 2020-04-17 18:04:49,766 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2_1  | 2020-04-17 18:04:49,766 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2_1  | 2020-04-17 18:04:49,766 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2_1  | 2020-04-17 18:04:49,778 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:09,770 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:09,770 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:09,771 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:09,771 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:09,774 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:09,775 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:14,117 [ChunkWriter-53-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2_1  | 2020-04-17 18:04:49,779 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:04:49,779 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2_1  | 2020-04-17 18:04:49,779 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2_1  | 2020-04-17 18:04:49,779 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2_1  | 2020-04-17 18:04:49,780 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:04:49,780 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2_1  | 2020-04-17 18:04:49,782 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2_1  | 2020-04-17 18:04:49,782 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2_1  | 2020-04-17 18:04:49,782 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2_1  | 2020-04-17 18:04:49,782 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2_1  | 2020-04-17 18:04:49,783 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2_1  | 2020-04-17 18:04:49,783 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: start LeaderState
datanode_2_1  | 2020-04-17 18:04:49,784 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2_1  | 2020-04-17 18:04:49,788 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/current/log_inprogress_0
datanode_2_1  | 2020-04-17 18:04:49,788 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-LeaderElection4] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_2_1  | 2020-04-17 18:04:52,405 [ChunkWriter-48-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=966537216 B) is less than the container size (=1073741824 B).
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_2_1  | 	... 13 more
datanode_2_1  | 2020-04-17 18:04:52,412 [ChunkWriter-48-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_2_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_2_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:04:52,423 [ChunkWriter-48-0] ERROR ratis.ContainerStateMachine: group-197A8EB51962: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_2_1  | localID: 104015245631094805
datanode_2_1  | blockCommitSequenceId: 0
datanode_2_1  |  logIndex 1 chunkName 104015245631094805_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1_1  | 2020-04-17 18:06:27,250 [ChunkWriter-12-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=966262784 B) is less than the container size (=1073741824 B).
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_1_1  | 	... 13 more
datanode_1_1  | 2020-04-17 18:06:27,262 [ChunkWriter-12-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_1_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_1_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_1_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1_1  | 2020-04-17 18:06:27,272 [ChunkWriter-12-0] ERROR ratis.ContainerStateMachine: group-432946335976: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_1_1  | localID: 104015251845611544
datanode_1_1  | blockCommitSequenceId: 0
datanode_1_1  |  logIndex 1 chunkName 104015251845611544_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_1_1  | 2020-04-17 18:06:27,272 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : ContainerID 5 creation failed
datanode_1_1  | 2020-04-17 18:06:27,274 [10592d3b-d195-479b-9b4d-1b30ca37f5cc@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
datanode_1_1  | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3_1  | 2020-04-17 18:04:39,521 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3_1  | 2020-04-17 18:04:39,527 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:04:39,527 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3_1  | 2020-04-17 18:04:39,531 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:04:39,531 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3_1  | 2020-04-17 18:04:39,531 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3_1  | 2020-04-17 18:04:39,532 [pool-69-thread-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:04:39,534 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:04:39,534 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:04:39,626 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | .
datanode_3_1  | 2020-04-17 18:04:44,663 [Thread-194] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-FollowerState: change to CANDIDATE, lastRpcTime:5130ms, electionTimeout:5128ms
datanode_3_1  | 2020-04-17 18:04:44,663 [Thread-194] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:04:44,664 [Thread-194] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3_1  | 2020-04-17 18:04:44,664 [Thread-194] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start LeaderElection
datanode_3_1  | 2020-04-17 18:04:44,667 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: changes role from CANDIDATE to FOLLOWER at term 1 for recognizeCandidate:9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_3_1  | 2020-04-17 18:04:44,667 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown LeaderElection
datanode_3_1  | 2020-04-17 18:04:44,667 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:04:44,676 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-LeaderElection2] INFO impl.LeaderElection: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-LeaderElection2: begin an election at term 2 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_3_1  | 2020-04-17 18:04:44,680 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-LeaderElection2] INFO impl.LeaderElection: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-LeaderElection2: Election REJECTED; received 0 response(s) [] and 0 exception(s); 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962:t2, leader=null, voted=4e682fff-8411-4efe-954f-416a0833f687, raftlog=4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_3_1  | 2020-04-17 18:04:49,745 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: changes role from  FOLLOWER to FOLLOWER at term 3 for recognizeCandidate:9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_3_1  | 2020-04-17 18:04:49,745 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:04:49,745 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:04:49,745 [Thread-198] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:04:49,795 [grpc-default-executor-2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-197A8EB51962 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_3_1  | 2020-04-17 18:04:49,795 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 3 for appendEntries, leader elected after 10275ms
datanode_3_1  | 2020-04-17 18:04:49,796 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_3_1  | 2020-04-17 18:04:49,796 [grpc-default-executor-2] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3_1  | 2020-04-17 18:04:49,799 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/current/log_inprogress_0
datanode_3_1  | 2020-04-17 18:04:52,408 [ChunkWriter-27-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_2_1  | 2020-04-17 18:04:52,427 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : ContainerID 4 creation failed
datanode_2_1  | 2020-04-17 18:04:52,431 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
datanode_2_1  | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_2_1  | 2020-04-17 18:05:10,559 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_2_1  | 2020-04-17 18:05:10,560 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_2_1  | 2020-04-17 18:05:52,406 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3,entriesCount=1,lastEntry=(t:3, i:1)
datanode_2_1  | 2020-04-17 18:05:52,406 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=3,entriesCount=1,lastEntry=(t:3, i:1)
datanode_2_1  | 2020-04-17 18:05:52,430 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4,entriesCount=1,lastEntry=(t:3, i:2)
datanode_2_1  | 2020-04-17 18:05:52,431 [java.util.concurrent.ThreadPoolExecutor$Worker@421ae624[State = -1, empty queue]] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender:  appendEntries Timeout, request=AppendEntriesRequest:cid=4,entriesCount=1,lastEntry=(t:3, i:2)
datanode_2_1  | 2020-04-17 18:06:23,439 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove    LEADER 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962:t3, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_2_1  | 2020-04-17 18:06:23,439 [Command processor thread] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: shutdown
datanode_2_1  | 2020-04-17 18:06:23,440 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:06:23,440 [Command processor thread] INFO impl.RoleInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: shutdown LeaderState
datanode_2_1  | 2020-04-17 18:06:23,440 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$443/0x0000000840592040@777174dd] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->4e682fff-8411-4efe-954f-416a0833f687-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2_1  | 2020-04-17 18:06:23,440 [org.apache.ratis.server.impl.LogAppender$AppenderDaemon$$Lambda$443/0x0000000840592040@46cc85d7] WARN server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->313d3461-163d-4229-bfba-0697ea0de001-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
datanode_2_1  | 2020-04-17 18:06:23,440 [Command processor thread] INFO impl.PendingRequests: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-PendingRequests: sendNotLeaderResponses
datanode_2_1  | 2020-04-17 18:06:23,442 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_appender.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:06:23,442 [Command processor thread] INFO impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater: set stopIndex = 0
datanode_2_1  | 2020-04-17 18:06:23,446 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_2_1  | 2020-04-17 18:06:23,446 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_2_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:06:23,446 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_2_1  | 2020-04-17 18:06:23,446 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_2_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_2_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | 2020-04-17 18:06:23,446 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:06:23,448 [grpc-default-executor-3] INFO server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->313d3461-163d-4229-bfba-0697ea0de001-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=0 B) is less than the container size (=1073741824 B).
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:04:52,409 [ChunkWriter-27-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:04:52,410 [ChunkWriter-27-0] ERROR ratis.ContainerStateMachine: group-197A8EB51962: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_3_1  | localID: 104015245631094805
datanode_3_1  | blockCommitSequenceId: 0
datanode_3_1  |  logIndex 1 chunkName 104015245631094805_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3_1  | 2020-04-17 18:04:52,410 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : ContainerID 4 creation failed
datanode_3_1  | 2020-04-17 18:04:52,442 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
datanode_3_1  | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_3_1  | 2020-04-17 18:05:10,519 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_3_1  | 2020-04-17 18:05:15,341 [grpc-default-executor-2] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: addNew group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-432946335976:java.util.concurrent.CompletableFuture@5e3ca8fb[Not completed]
datanode_3_1  | 2020-04-17 18:05:15,343 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687: new RaftServerImpl for group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3_1  | 2020-04-17 18:05:15,345 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: ConfigurationManager, init=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_3_1  | 2020-04-17 18:05:15,346 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3_1  | 2020-04-17 18:05:15,346 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3_1  | 2020-04-17 18:05:15,346 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 does not exist. Creating ...
datanode_3_1  | 2020-04-17 18:05:15,351 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/in_use.lock acquired by nodename 6@a8c66e3e0d0a
datanode_3_1  | 2020-04-17 18:05:15,355 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 has been successfully formatted.
datanode_3_1  | 2020-04-17 18:05:15,356 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-432946335976: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
om_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om_1          | STARTUP_MSG:   java = 11.0.6
om_1          | ************************************************************/
om_1          | 2020-04-17 18:00:50,342 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1          | 2020-04-17 18:00:56,544 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1          | 2020-04-17 18:00:56,895 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/10.5.0.70:9862
om_1          | 2020-04-17 18:00:56,903 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1          | 2020-04-17 18:00:56,943 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1          | 2020-04-17 18:00:59,704 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:00,704 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:01,705 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:02,706 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:03,707 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:04,708 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:05,709 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:06,710 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:07,711 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:08,712 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | 2020-04-17 18:01:08,714 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1          | 2020-04-17 18:01:14,715 [main] INFO ipc.Client: Retrying connect to server: scm/10.5.0.71:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1          | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-a2d7cef2-3870-468c-b963-63f389074954
om_1          | 2020-04-17 18:01:15,178 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1          | /************************************************************
om_1          | SHUTDOWN_MSG: Shutting down OzoneManager at 4387223f42c5/10.5.0.70
om_1          | ************************************************************/
om_1          | Enabled profiling in kernel
om_1          | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
om_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1          | 2020-04-17 18:01:17,481 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1          | /************************************************************
om_1          | STARTUP_MSG: Starting OzoneManager
om_1          | STARTUP_MSG:   host = 4387223f42c5/10.5.0.70
om_1          | STARTUP_MSG:   args = []
om_1          | STARTUP_MSG:   version = 3.2.0
datanode_3_1  | 2020-04-17 18:05:15,356 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_3_1  | 2020-04-17 18:05:15,356 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3_1  | 2020-04-17 18:05:15,357 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3_1  | 2020-04-17 18:05:15,358 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3_1  | 2020-04-17 18:05:15,358 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3_1  | 2020-04-17 18:05:15,358 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:05:15,359 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3_1  | 2020-04-17 18:05:15,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3_1  | 2020-04-17 18:05:15,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3_1  | 2020-04-17 18:05:15,364 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3_1  | 2020-04-17 18:05:15,365 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3_1  | 2020-04-17 18:05:15,365 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-432946335976
datanode_3_1  | 2020-04-17 18:05:15,367 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-432946335976
datanode_3_1  | 2020-04-17 18:05:15,367 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: start as a follower, conf=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_3_1  | 2020-04-17 18:05:15,367 [pool-69-thread-1] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3_1  | 2020-04-17 18:05:15,367 [pool-69-thread-1] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:05:15,368 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-432946335976,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:05:15,368 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-432946335976
datanode_3_1  | 2020-04-17 18:05:20,306 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_3_1  | 2020-04-17 18:05:20,308 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_3_1  | 2020-04-17 18:05:20,308 [grpc-default-executor-2] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: start FollowerState
datanode_3_1  | 2020-04-17 18:05:20,308 [Thread-276] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:05:20,336 [grpc-default-executor-2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-432946335976 with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_3_1  | 2020-04-17 18:05:20,336 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for appendEntries, leader elected after 4979ms
datanode_3_1  | 2020-04-17 18:05:20,346 [grpc-default-executor-2] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976: set configuration 0: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_3_1  | 2020-04-17 18:05:20,346 [grpc-default-executor-2] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3_1  | 2020-04-17 18:05:20,349 [4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/current/log_inprogress_0
datanode_3_1  | 2020-04-17 18:05:23,442 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_3_1  | 2020-04-17 18:06:16,357 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove  FOLLOWER 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962:t3, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_3_1  | 2020-04-17 18:06:16,357 [Command processor thread] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: shutdown
datanode_3_1  | 2020-04-17 18:06:16,357 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:06:16,357 [Command processor thread] INFO impl.RoleInfo: 4e682fff-8411-4efe-954f-416a0833f687: shutdown FollowerState
datanode_5_1  | 2020-04-17 18:02:59,370 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5_1  | 2020-04-17 18:02:59,371 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5_1  | 2020-04-17 18:02:59,371 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:02:59,371 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:02:59,372 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: start as a follower, conf=-1: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_5_1  | 2020-04-17 18:02:59,372 [pool-69-thread-1] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5_1  | 2020-04-17 18:02:59,373 [pool-69-thread-1] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start FollowerState
datanode_5_1  | 2020-04-17 18:02:59,379 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:02:59,379 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:03:03,970 [grpc-default-executor-0] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_5_1  | 2020-04-17 18:03:03,970 [grpc-default-executor-0] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown FollowerState
datanode_5_1  | 2020-04-17 18:03:03,971 [grpc-default-executor-0] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: start FollowerState
datanode_5_1  | 2020-04-17 18:03:03,971 [Thread-48] INFO impl.FollowerState: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_5_1  | 2020-04-17 18:03:04,084 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-35283765472C with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_5_1  | 2020-04-17 18:03:04,084 [grpc-default-executor-0] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for appendEntries, leader elected after 4721ms
datanode_5_1  | 2020-04-17 18:03:04,089 [grpc-default-executor-0] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: set configuration 0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_5_1  | 2020-04-17 18:03:04,092 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5_1  | 2020-04-17 18:03:04,098 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/643f591a-05e9-4372-bc18-35283765472c/current/log_inprogress_0
datanode_5_1  | 2020-04-17 18:03:14,107 [ChunkWriter-14-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_5_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_5_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=0 B) is less than the container size (=1073741824 B).
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_5_1  | 	... 13 more
datanode_5_1  | 2020-04-17 18:03:14,108 [ChunkWriter-14-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_5_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_5_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | 2020-04-17 18:03:14,144 [ChunkWriter-14-0] ERROR ratis.ContainerStateMachine: group-35283765472C: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_5_1  | localID: 104015239176716296
datanode_3_1  | 2020-04-17 18:06:16,358 [Command processor thread] INFO impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater: set stopIndex = 0
datanode_3_1  | 2020-04-17 18:06:16,358 [Thread-245] INFO impl.FollowerState: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_3_1  | 2020-04-17 18:06:16,358 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_3_1  | 2020-04-17 18:06:16,359 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_3_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:06:16,359 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_3_1  | 2020-04-17 18:06:16,360 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_3_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_3_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:06:16,360 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:06:16,360 [Command processor thread] INFO impl.RaftServerImpl: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962: closes. applyIndex: 0
datanode_3_1  | 2020-04-17 18:06:16,361 [4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_3_1  | 2020-04-17 18:06:16,362 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962-SegmentedRaftLogWorker close()
datanode_3_1  | 2020-04-17 18:06:16,362 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.4e682fff-8411-4efe-954f-416a0833f687
datanode_3_1  | 2020-04-17 18:06:16,362 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:06:16,362 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.4e682fff-8411-4efe-954f-416a0833f687@group-197A8EB51962
datanode_3_1  | 2020-04-17 18:06:16,363 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  |  command on datanode #4e682fff-8411-4efe-954f-416a0833f687.
datanode_3_1  | 2020-04-17 18:06:16,363 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-197A8EB51962:null
datanode_3_1  | 2020-04-17 18:06:16,363 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:06:16,364 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-197A8EB51962:null
datanode_3_1  | 2020-04-17 18:06:16,364 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_5_1  | blockCommitSequenceId: 0
datanode_5_1  |  logIndex 1 chunkName 104015239176716296_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_5_1  | 2020-04-17 18:03:14,155 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : ContainerID 3 creation failed
datanode_5_1  | 2020-04-17 18:03:14,217 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
datanode_5_1  | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_5_1  | 2020-04-17 18:03:30,364 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_5_1  | 2020-04-17 18:04:45,218 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove  FOLLOWER 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C:t1, leader=10592d3b-d195-479b-9b4d-1b30ca37f5cc, voted=10592d3b-d195-479b-9b4d-1b30ca37f5cc, raftlog=7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null RUNNING
datanode_5_1  | 2020-04-17 18:04:45,220 [Command processor thread] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: shutdown
datanode_5_1  | 2020-04-17 18:04:45,220 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:04:45,221 [Command processor thread] INFO impl.RoleInfo: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: shutdown FollowerState
datanode_5_1  | 2020-04-17 18:04:45,221 [Thread-50] INFO impl.FollowerState: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_5_1  | 2020-04-17 18:04:45,221 [Command processor thread] INFO impl.StateMachineUpdater: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater: set stopIndex = 0
datanode_5_1  | 2020-04-17 18:04:45,222 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_5_1  | 2020-04-17 18:04:45,222 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_5_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | 2020-04-17 18:04:45,223 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_5_1  | 2020-04-17 18:04:45,223 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_5_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_5_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | 2020-04-17 18:04:45,223 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:04:45,224 [Command processor thread] INFO impl.RaftServerImpl: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C: closes. applyIndex: 0
datanode_5_1  | 2020-04-17 18:04:45,225 [7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_5_1  | 2020-04-17 18:04:45,227 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C-SegmentedRaftLogWorker close()
datanode_5_1  | 2020-04-17 18:04:45,228 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.7fb5a271-9f73-44d7-9131-67b4b4fca07d
datanode_5_1  | 2020-04-17 18:04:45,228 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:04:45,228 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.7fb5a271-9f73-44d7-9131-67b4b4fca07d@group-35283765472C
datanode_5_1  | 2020-04-17 18:04:45,231 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  |  command on datanode #7fb5a271-9f73-44d7-9131-67b4b4fca07d.
datanode_5_1  | 2020-04-17 18:04:45,231 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove group-35283765472C:null
datanode_5_1  | 2020-04-17 18:04:45,231 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  | 
datanode_5_1  | java.io.IOException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 2020-04-17 18:06:23,451 [grpc-default-executor-4] INFO server.GrpcLogAppender: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->4e682fff-8411-4efe-954f-416a0833f687-AppendLogResponseHandler: follower responses appendEntries COMPLETED
datanode_2_1  | 2020-04-17 18:06:23,457 [Command processor thread] INFO impl.RaftServerImpl: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962: closes. applyIndex: 0
datanode_2_1  | 2020-04-17 18:06:23,458 [9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_2_1  | 2020-04-17 18:06:23,469 [grpc-default-executor-3] INFO impl.FollowerInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->313d3461-163d-4229-bfba-0697ea0de001: nextIndex: updateUnconditionally 3 -> 1
datanode_2_1  | 2020-04-17 18:06:23,469 [grpc-default-executor-4] INFO impl.FollowerInfo: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962->4e682fff-8411-4efe-954f-416a0833f687: nextIndex: updateUnconditionally 3 -> 1
datanode_2_1  | 2020-04-17 18:06:23,469 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962-SegmentedRaftLogWorker close()
datanode_2_1  | 2020-04-17 18:06:23,470 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_2_1  | 2020-04-17 18:06:23,470 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:06:23,470 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.9297bad1-1bc3-407b-ab8e-258f1dd490f7@group-197A8EB51962
datanode_2_1  | 2020-04-17 18:06:23,471 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  |  command on datanode #9297bad1-1bc3-407b-ab8e-258f1dd490f7.
datanode_2_1  | 2020-04-17 18:06:23,471 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-197A8EB51962:null
datanode_2_1  | 2020-04-17 18:06:23,471 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:06:23,471 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-197A8EB51962:null
datanode_2_1  | 2020-04-17 18:06:23,472 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:06:23,472 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-197A8EB51962:null
datanode_2_1  | 2020-04-17 18:06:23,472 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:06:23,472 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-197A8EB51962:null
datanode_2_1  | 2020-04-17 18:06:23,473 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_5_1  | 	... 4 more
datanode_5_1  | 2020-04-17 18:04:45,232 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove group-35283765472C:null
datanode_5_1  | 2020-04-17 18:04:45,232 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  | 
datanode_5_1  | java.io.IOException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_5_1  | 	... 4 more
datanode_5_1  | 2020-04-17 18:04:45,232 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove group-35283765472C:null
datanode_5_1  | 2020-04-17 18:04:45,232 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  | 
datanode_5_1  | java.io.IOException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_5_1  | 	... 4 more
datanode_5_1  | 2020-04-17 18:04:45,233 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove group-35283765472C:null
datanode_5_1  | 2020-04-17 18:04:45,233 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  | 
datanode_5_1  | java.io.IOException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_5_1  | 	... 4 more
datanode_5_1  | 2020-04-17 18:04:45,233 [Command processor thread] INFO impl.RaftServerProxy: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: remove group-35283765472C:null
datanode_5_1  | 2020-04-17 18:04:45,233 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_5_1  | 
datanode_5_1  | java.io.IOException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_5_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_5_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Group group-35283765472C not found.
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_5_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_5_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_5_1  | 	... 4 more
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=967208960 B) is less than the container size (=1073741824 B).
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:03:14,118 [ChunkWriter-53-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0 , Message: ContainerID 3 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 3 creation failed
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:03:14,118 [ChunkWriter-53-0] ERROR ratis.ContainerStateMachine: group-35283765472C: writeChunk writeStateMachineData failed: blockIdcontainerID: 3
datanode_4_1  | localID: 104015239176716296
datanode_4_1  | blockCommitSequenceId: 0
datanode_4_1  |  logIndex 1 chunkName 104015239176716296_chunk_1 Error message: ContainerID 3 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_4_1  | 2020-04-17 18:03:14,118 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : ContainerID 3 creation failed
datanode_4_1  | 2020-04-17 18:03:14,142 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=643f591a-05e9-4372-bc18-35283765472c.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
datanode_4_1  | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_4_1  | 2020-04-17 18:03:30,240 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:30,240 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:30,241 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-3E8BAA29D9A7:null
datanode_4_1  | 2020-04-17 18:03:30,241 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-3E8BAA29D9A7 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:03:30,241 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #3 does not exist in datanode. Container close failed.
datanode_4_1  | 2020-04-17 18:04:39,591 [grpc-default-executor-0] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: addNew group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] returns group-197A8EB51962:java.util.concurrent.CompletableFuture@205229f0[Not completed]
datanode_5_1  | 2020-04-17 18:04:45,254 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 7fb5a271-9f73-44d7-9131-67b4b4fca07d: Completed APPEND_ENTRIES, lastRequest: 10592d3b-d195-479b-9b4d-1b30ca37f5cc->7fb5a271-9f73-44d7-9131-67b4b4fca07d#6-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=27
scm_1         | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
scm_1         | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1         | 2020-04-17 18:00:50,391 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1         | /************************************************************
scm_1         | STARTUP_MSG: Starting StorageContainerManager
scm_1         | STARTUP_MSG:   host = 187b11aaab3a/10.5.0.71
scm_1         | STARTUP_MSG:   args = [--init]
scm_1         | STARTUP_MSG:   version = 3.2.0
om_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar
om_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
om_1          | STARTUP_MSG:   java = 11.0.6
om_1          | ************************************************************/
om_1          | 2020-04-17 18:01:17,489 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1          | 2020-04-17 18:01:18,541 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1          | 2020-04-17 18:01:18,597 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/10.5.0.70:9862
om_1          | 2020-04-17 18:01:18,597 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1          | 2020-04-17 18:01:18,607 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1          | 2020-04-17 18:01:18,785 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1          | 2020-04-17 18:01:20,184 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1          | 2020-04-17 18:01:21,018 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1          | 2020-04-17 18:01:21,099 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1          | 2020-04-17 18:01:21,921 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1          | 2020-04-17 18:01:22,243 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1          | 2020-04-17 18:01:22,243 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1          | 2020-04-17 18:01:22,474 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/10.5.0.70:9862
om_1          | 2020-04-17 18:01:22,740 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1          | 2020-04-17 18:01:22,758 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1          | 2020-04-17 18:01:23,568 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1          | 2020-04-17 18:01:23,679 [main] INFO util.log: Logging initialized @8103ms to org.eclipse.jetty.util.log.Slf4jLog
om_1          | 2020-04-17 18:01:24,252 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1          | 2020-04-17 18:01:24,259 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1          | 2020-04-17 18:01:24,311 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1          | 2020-04-17 18:01:24,314 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1          | 2020-04-17 18:01:24,315 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
om_1          | 2020-04-17 18:01:24,315 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3_1  | 2020-04-17 18:06:16,364 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-197A8EB51962:null
datanode_3_1  | 2020-04-17 18:06:16,364 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:06:16,366 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-197A8EB51962:null
datanode_3_1  | 2020-04-17 18:06:16,367 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:06:16,367 [Command processor thread] INFO impl.RaftServerProxy: 4e682fff-8411-4efe-954f-416a0833f687: remove group-197A8EB51962:null
datanode_3_1  | 2020-04-17 18:06:16,367 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_3_1  | 
datanode_3_1  | java.io.IOException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 4e682fff-8411-4efe-954f-416a0833f687: Group group-197A8EB51962 not found.
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_3_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_3_1  | 	... 4 more
datanode_3_1  | 2020-04-17 18:06:23,447 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: 4e682fff-8411-4efe-954f-416a0833f687: Completed APPEND_ENTRIES, lastRequest: 9297bad1-1bc3-407b-ab8e-258f1dd490f7->4e682fff-8411-4efe-954f-416a0833f687#4-t3, previous=(t:3, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:3, i:2), STATEMACHINELOGENTRY, client-86DCCB287922, cid=67
datanode_3_1  | 2020-04-17 18:06:27,251 [ChunkWriter-33-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=0 B) is less than the container size (=1073741824 B).
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
om_1          | 2020-04-17 18:01:24,431 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
om_1          | 2020-04-17 18:01:24,437 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1          | 2020-04-17 18:01:24,439 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
om_1          | 2020-04-17 18:01:24,607 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1          | 2020-04-17 18:01:24,624 [main] INFO server.session: No SessionScavenger set, using defaults
om_1          | 2020-04-17 18:01:24,629 [main] INFO server.session: node0 Scavenging every 660000ms
om_1          | 2020-04-17 18:01:24,707 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6504a875{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1          | 2020-04-17 18:01:24,716 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1bb740f2{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1          | 2020-04-17 18:01:25,567 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1697f2b3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-0_6_0-SNAPSHOT_jar-_-any-3804937946997468338.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-0.6.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1          | 2020-04-17 18:01:25,586 [main] INFO server.AbstractConnector: Started ServerConnector@17092fff{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1          | 2020-04-17 18:01:25,586 [main] INFO server.Server: Started @10010ms
om_1          | 2020-04-17 18:01:25,597 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1          | 2020-04-17 18:01:25,598 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1          | 2020-04-17 18:01:25,603 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1          | 2020-04-17 18:01:35,098 [IPC Server handler 75 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-0-32734 for user:hadoop
om_1          | 2020-04-17 18:01:35,118 [IPC Server handler 94 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-1-72222 for user:hadoop
om_1          | 2020-04-17 18:01:35,126 [IPC Server handler 97 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-2-26166 for user:hadoop
om_1          | 2020-04-17 18:01:35,132 [IPC Server handler 93 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-3-49140 for user:hadoop
om_1          | 2020-04-17 18:01:35,138 [IPC Server handler 88 on 9862] INFO volume.OMVolumeCreateRequest: created volume:vol-4-32013 for user:hadoop
datanode_4_1  | 2020-04-17 18:04:39,593 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001: new RaftServerImpl for group-197A8EB51962:[9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858] with ContainerStateMachine:uninitialized
datanode_4_1  | 2020-04-17 18:04:39,595 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4_1  | 2020-04-17 18:04:39,595 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | 2020-04-17 18:04:39,596 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_4_1  | 2020-04-17 18:04:39,596 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_4_1  | 2020-04-17 18:04:39,596 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4_1  | 2020-04-17 18:04:39,596 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: ConfigurationManager, init=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null, confs=<EMPTY_MAP>
datanode_4_1  | 2020-04-17 18:04:39,596 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4_1  | 2020-04-17 18:04:39,597 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4_1  | 2020-04-17 18:04:39,597 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 does not exist. Creating ...
datanode_4_1  | 2020-04-17 18:04:39,599 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/in_use.lock acquired by nodename 6@25eca77e6515
datanode_4_1  | 2020-04-17 18:04:39,601 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962 has been successfully formatted.
datanode_4_1  | 2020-04-17 18:04:39,602 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-197A8EB51962: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4_1  | 2020-04-17 18:04:39,602 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_4_1  | 2020-04-17 18:04:39,602 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4_1  | 2020-04-17 18:04:39,602 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4_1  | 2020-04-17 18:04:39,602 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_4_1  | 2020-04-17 18:04:39,603 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4_1  | 2020-04-17 18:04:39,604 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4_1  | 2020-04-17 18:04:39,605 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4_1  | 2020-04-17 18:04:39,606 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:04:39,607 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4_1  | 2020-04-17 18:04:39,608 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:04:39,609 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4_1  | 2020-04-17 18:04:39,609 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4_1  | 2020-04-17 18:04:39,611 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4_1  | 2020-04-17 18:04:39,611 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:04:39,612 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:04:39,613 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: start as a follower, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:04:39,613 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4_1  | 2020-04-17 18:04:39,616 [pool-69-thread-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:04:39,618 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:04:39,618 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:04:44,632 [Thread-135] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-FollowerState: change to CANDIDATE, lastRpcTime:5018ms, electionTimeout:5014ms
datanode_4_1  | 2020-04-17 18:04:44,632 [Thread-135] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:04:44,633 [Thread-135] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4_1  | 2020-04-17 18:04:44,633 [Thread-135] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start LeaderElection
datanode_3_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_3_1  | 	... 13 more
datanode_3_1  | 2020-04-17 18:06:27,252 [ChunkWriter-33-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_3_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_3_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_3_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3_1  | 2020-04-17 18:06:27,252 [ChunkWriter-33-0] ERROR ratis.ContainerStateMachine: group-432946335976: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_3_1  | localID: 104015251845611544
datanode_3_1  | blockCommitSequenceId: 0
datanode_3_1  |  logIndex 1 chunkName 104015251845611544_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_3_1  | 2020-04-17 18:06:27,252 [4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : ContainerID 5 creation failed
datanode_3_1  | 2020-04-17 18:06:27,278 [4e682fff-8411-4efe-954f-416a0833f687@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
datanode_3_1  | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
datanode_4_1  | 2020-04-17 18:04:44,639 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-LeaderElection2] INFO impl.LeaderElection: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-LeaderElection2: begin an election at term 1 for -1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:04:44,747 [grpc-default-executor-0] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: changes role from CANDIDATE to FOLLOWER at term 2 for recognizeCandidate:4e682fff-8411-4efe-954f-416a0833f687
datanode_4_1  | 2020-04-17 18:04:44,748 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown LeaderElection
datanode_4_1  | 2020-04-17 18:04:44,748 [grpc-default-executor-0] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:04:44,788 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-LeaderElection2] INFO impl.LeaderElection: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-LeaderElection2: Election DISCOVERED_A_NEW_TERM; received 1 response(s) [313d3461-163d-4229-bfba-0697ea0de001<-9297bad1-1bc3-407b-ab8e-258f1dd490f7#0:FAIL-t2] and 0 exception(s); 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962:t2, leader=null, voted=null, raftlog=313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null
datanode_4_1  | 2020-04-17 18:04:45,143 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove  FOLLOWER 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C:t1, leader=10592d3b-d195-479b-9b4d-1b30ca37f5cc, voted=10592d3b-d195-479b-9b4d-1b30ca37f5cc, raftlog=313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [7fb5a271-9f73-44d7-9131-67b4b4fca07d:10.5.0.8:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null RUNNING
datanode_4_1  | 2020-04-17 18:04:45,143 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: shutdown
datanode_4_1  | 2020-04-17 18:04:45,143 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-35283765472C,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:04:45,143 [Command processor thread] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:04:45,143 [Command processor thread] INFO impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater: set stopIndex = 0
datanode_4_1  | 2020-04-17 18:04:45,144 [Thread-83] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:04:45,144 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 2020-04-17 18:04:45,144 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:04:45,144 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 2020-04-17 18:04:45,145 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-35283765472C as the stateMachine is unhealthy. The last applied index is at (t:1, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:04:45,145 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:04:45,145 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C: closes. applyIndex: 0
datanode_4_1  | 2020-04-17 18:04:45,145 [313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_4_1  | 2020-04-17 18:04:45,146 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C-SegmentedRaftLogWorker close()
datanode_4_1  | 2020-04-17 18:04:45,146 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:04:45,146 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:04:45,146 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-35283765472C
datanode_4_1  | 2020-04-17 18:04:45,147 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  |  command on datanode #313d3461-163d-4229-bfba-0697ea0de001.
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
scm_1         | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1         | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1         | STARTUP_MSG:   java = 11.0.6
scm_1         | ************************************************************/
scm_1         | 2020-04-17 18:00:50,410 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1         | 2020-04-17 18:00:50,711 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1         | 2020-04-17 18:00:50,834 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-a2d7cef2-3870-468c-b963-63f389074954
scm_1         | 2020-04-17 18:00:50,889 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1         | /************************************************************
scm_1         | SHUTDOWN_MSG: Shutting down StorageContainerManager at 187b11aaab3a/10.5.0.71
scm_1         | ************************************************************/
scm_1         | Enabled profiling in kernel
scm_1         | No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS
scm_1         | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1         | 2020-04-17 18:01:03,274 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1         | /************************************************************
scm_1         | STARTUP_MSG: Starting StorageContainerManager
scm_1         | STARTUP_MSG:   host = 187b11aaab3a/10.5.0.71
scm_1         | STARTUP_MSG:   args = []
scm_1         | STARTUP_MSG:   version = 3.2.0
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_2_1  | 2020-04-17 18:06:23,473 [Command processor thread] INFO impl.RaftServerProxy: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: remove group-197A8EB51962:null
datanode_2_1  | 2020-04-17 18:06:23,473 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_2_1  | 
datanode_2_1  | java.io.IOException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_2_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 9297bad1-1bc3-407b-ab8e-258f1dd490f7: Group group-197A8EB51962 not found.
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_2_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_2_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_2_1  | 	... 4 more
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:04:45,148 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:04:45,149 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:04:45,149 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:04:45,149 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
scm_1         | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-proto-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.3.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.31.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/libthrift-0.12.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.6.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/avro-1.7.7.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-docs-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jaeger-client-0.34.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.31.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-0.34.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.5.jar:/opt/hadoop/share/ozone/lib/ratis-common-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/paranamer-2.3.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/okio-1.13.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-0.34.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-0.34.0.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-3.9.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.31.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-0.6.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.6.0-SNAPSHOT-tests.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/lib/ratis-netty-0.6.0-7c5b30d-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar
scm_1         | STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-15T17:34Z
scm_1         | STARTUP_MSG:   java = 11.0.6
scm_1         | ************************************************************/
scm_1         | 2020-04-17 18:01:03,301 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1         | 2020-04-17 18:01:04,025 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1         | 2020-04-17 18:01:04,163 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1         | 2020-04-17 18:01:06,537 [main] INFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@2bec854f
scm_1         | 2020-04-17 18:01:06,568 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1         | 2020-04-17 18:01:08,081 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1         | 2020-04-17 18:01:09,639 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1         | 2020-04-17 18:01:09,661 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1         | 2020-04-17 18:01:10,005 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1         | 2020-04-17 18:01:10,019 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1         | 2020-04-17 18:01:10,774 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1         | 2020-04-17 18:01:10,825 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1         | 2020-04-17 18:01:11,165 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 0 nodes. Healthy nodes 0
scm_1         | 2020-04-17 18:01:13,919 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1         | 2020-04-17 18:01:13,951 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1         | 2020-04-17 18:01:14,067 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1         | 2020-04-17 18:01:14,089 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1         | 2020-04-17 18:01:14,104 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1         | 2020-04-17 18:01:14,105 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1         | 2020-04-17 18:01:14,130 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1         | 2020-04-17 18:01:14,150 [main] INFO util.log: Logging initialized @22428ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1         | 2020-04-17 18:01:14,286 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1         | 2020-04-17 18:01:14,299 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1         | 2020-04-17 18:01:14,305 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1         | 2020-04-17 18:01:14,307 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1         | 2020-04-17 18:01:14,307 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1         | 2020-04-17 18:01:14,307 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.hdds.server.http.StaticUserWebFilter$StaticUserFilter) to context static
scm_1         | 2020-04-17 18:01:14,331 [main] WARN http.BaseHttpServer: /prof java profiling servlet is activated. Not safe for production!
scm_1         | 2020-04-17 18:01:14,337 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1         | 2020-04-17 18:01:14,422 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1         | 2020-04-17 18:01:14,481 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1         | 2020-04-17 18:01:14,481 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1         | 2020-04-17 18:01:14,750 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1         | 2020-04-17 18:01:14,751 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1         | 2020-04-17 18:01:14,789 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1         | 2020-04-17 18:01:14,814 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1         | 2020-04-17 18:01:14,815 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1         | 2020-04-17 18:01:14,818 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1         | 2020-04-17 18:01:14,821 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1         | 2020-04-17 18:01:14,947 [main] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1         | 2020-04-17 18:01:14,947 [main] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1         | 2020-04-17 18:01:14,950 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1         | 2020-04-17 18:01:14,951 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1         | 2020-04-17 18:01:15,011 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1         | 2020-04-17 18:01:15,012 [main] INFO server.Server: jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.6+10-LTS
scm_1         | 2020-04-17 18:01:15,082 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1         | 2020-04-17 18:01:15,082 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1         | 2020-04-17 18:01:15,115 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1         | 2020-04-17 18:01:15,127 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@76d05cc9{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1         | 2020-04-17 18:01:15,139 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d37f1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1         | 2020-04-17 18:01:15,243 [IPC Server handler 0 on 9861] WARN ipc.Server: IPC Server handler 0 on 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.5.0.7:43500: output error
scm_1         | 2020-04-17 18:01:15,244 [IPC Server handler 4 on 9861] WARN ipc.Server: IPC Server handler 4 on 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.5.0.4:41850: output error
scm_1         | 2020-04-17 18:01:15,282 [IPC Server handler 0 on 9861] INFO ipc.Server: IPC Server handler 0 on 9861 caught an exception
scm_1         | java.nio.channels.AsynchronousCloseException
scm_1         | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1         | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3250)
scm_1         | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:137)
scm_1         | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1473)
scm_1         | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1543)
scm_1         | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2593)
scm_1         | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1615)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:940)
scm_1         | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:774)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:885)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1         | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1         | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1         | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1         | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1         | 2020-04-17 18:01:15,287 [IPC Server handler 4 on 9861] INFO ipc.Server: IPC Server handler 4 on 9861 caught an exception
scm_1         | java.nio.channels.AsynchronousCloseException
scm_1         | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1         | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3250)
scm_1         | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:137)
scm_1         | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1473)
scm_1         | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1543)
scm_1         | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2593)
scm_1         | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1615)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:940)
scm_1         | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:774)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:885)
scm_1         | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
scm_1         | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1         | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1         | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1         | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
scm_1         | 2020-04-17 18:01:16,280 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7ecec90d{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-0_6_0-SNAPSHOT_jar-_-any-2948124409581450094.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-0.6.0-SNAPSHOT.jar!/webapps/scm}
scm_1         | 2020-04-17 18:01:16,306 [main] INFO server.AbstractConnector: Started ServerConnector@4393593c{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1         | 2020-04-17 18:01:16,306 [main] INFO server.Server: Started @24584ms
scm_1         | 2020-04-17 18:01:16,307 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1         | 2020-04-17 18:01:16,307 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1         | 2020-04-17 18:01:16,312 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1         | 2020-04-17 18:01:16,326 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@56dfab87] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1         | 2020-04-17 18:01:17,241 [IPC Server handler 0 on 9861] INFO net.NetworkTopology: Added a new node: /rack2/7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:01:17,241 [IPC Server handler 0 on 9861] INFO node.SCMNodeManager: Registered Data node : 7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}
scm_1         | 2020-04-17 18:01:17,263 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,267 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 4 required.
scm_1         | 2020-04-17 18:01:17,289 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9e30214a-3af8-4f12-a594-490c4a63fe1d to datanode:7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:01:17,299 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 9e30214a-3af8-4f12-a594-490c4a63fe1d, Nodes: 7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.288264Z]
scm_1         | 2020-04-17 18:01:17,301 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 1 nodes. Healthy nodes 1
scm_1         | 2020-04-17 18:01:17,458 [IPC Server handler 6 on 9861] INFO net.NetworkTopology: Added a new node: /rack1/4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:01:17,459 [IPC Server handler 6 on 9861] INFO node.SCMNodeManager: Registered Data node : 4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}
scm_1         | 2020-04-17 18:01:17,459 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 4 required.
scm_1         | 2020-04-17 18:01:17,459 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,462 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1deb4c57-bfe1-4217-b2d1-e0e2714d3e07 to datanode:4e682fff-8411-4efe-954f-416a0833f687
datanode_4_1  | 2020-04-17 18:04:45,251 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 313d3461-163d-4229-bfba-0697ea0de001: Completed APPEND_ENTRIES, lastRequest: 10592d3b-d195-479b-9b4d-1b30ca37f5cc->313d3461-163d-4229-bfba-0697ea0de001#6-t1, previous=(t:1, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:1, i:2), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=27
datanode_4_1  | 2020-04-17 18:04:49,743 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: changes role from  FOLLOWER to FOLLOWER at term 3 for recognizeCandidate:9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_4_1  | 2020-04-17 18:04:49,743 [grpc-default-executor-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:04:49,743 [Thread-142] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:04:49,744 [grpc-default-executor-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:04:49,827 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-197A8EB51962 with new leaderId: 9297bad1-1bc3-407b-ab8e-258f1dd490f7
datanode_4_1  | 2020-04-17 18:04:49,827 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: change Leader from null to 9297bad1-1bc3-407b-ab8e-258f1dd490f7 at term 3 for appendEntries, leader elected after 10225ms
datanode_4_1  | 2020-04-17 18:04:49,831 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: set configuration 0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null at 0
datanode_4_1  | 2020-04-17 18:04:49,831 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4_1  | 2020-04-17 18:04:49,839 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/2e99f3a8-87a2-45fe-b8d0-197a8eb51962/current/log_inprogress_0
datanode_4_1  | 2020-04-17 18:04:52,414 [ChunkWriter-4-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=966524928 B) is less than the container size (=1073741824 B).
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:04:52,415 [ChunkWriter-4-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: 8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0 , Message: ContainerID 4 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 4 creation failed
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:04:52,416 [ChunkWriter-4-0] ERROR ratis.ContainerStateMachine: group-197A8EB51962: writeChunk writeStateMachineData failed: blockIdcontainerID: 4
datanode_4_1  | localID: 104015245631094805
datanode_4_1  | blockCommitSequenceId: 0
datanode_4_1  |  logIndex 1 chunkName 104015245631094805_chunk_1 Error message: ContainerID 4 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_4_1  | 2020-04-17 18:04:52,416 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : ContainerID 4 creation failed
datanode_4_1  | 2020-04-17 18:04:52,434 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962.Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
datanode_4_1  | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:01:17,464 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1deb4c57-bfe1-4217-b2d1-e0e2714d3e07, Nodes: 4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.462740Z]
scm_1         | 2020-04-17 18:01:17,465 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 2 nodes. Healthy nodes 2
scm_1         | 2020-04-17 18:01:17,478 [IPC Server handler 8 on 9861] INFO net.NetworkTopology: Added a new node: /rack1/9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:01:17,479 [IPC Server handler 8 on 9861] INFO node.SCMNodeManager: Registered Data node : 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}
scm_1         | 2020-04-17 18:01:17,479 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e9514b62-8305-4387-8371-bde057308d8f to datanode:9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:01:17,480 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: e9514b62-8305-4387-8371-bde057308d8f, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.479137Z]
scm_1         | 2020-04-17 18:01:17,480 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3
scm_1         | 2020-04-17 18:01:17,481 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,480 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 4 required.
scm_1         | 2020-04-17 18:01:17,546 [IPC Server handler 9 on 9861] INFO net.NetworkTopology: Added a new node: /rack2/575e95f6-9340-44b3-ae3e-c508b9a3128d
scm_1         | 2020-04-17 18:01:17,546 [IPC Server handler 9 on 9861] INFO node.SCMNodeManager: Registered Data node : 575e95f6-9340-44b3-ae3e-c508b9a3128d{ip: 10.5.0.9, host: ozone-topology_datanode_6_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}
scm_1         | 2020-04-17 18:01:17,546 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 4 DataNodes registered, 4 required.
scm_1         | 2020-04-17 18:01:17,548 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8501b544-67c4-4d39-a949-a0053359583b to datanode:575e95f6-9340-44b3-ae3e-c508b9a3128d
scm_1         | 2020-04-17 18:01:17,547 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,550 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8501b544-67c4-4d39-a949-a0053359583b, Nodes: 575e95f6-9340-44b3-ae3e-c508b9a3128d{ip: 10.5.0.9, host: ozone-topology_datanode_6_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.548116Z]
scm_1         | 2020-04-17 18:01:17,551 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 4 nodes. Healthy nodes 4
scm_1         | 2020-04-17 18:01:17,551 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,552 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1         | 2020-04-17 18:01:17,553 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 4 nodes. Healthy nodes 4
scm_1         | 2020-04-17 18:01:17,569 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3a881537-d96e-4768-950c-d38c014c699a to datanode:7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:01:17,569 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3a881537-d96e-4768-950c-d38c014c699a to datanode:4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:01:17,569 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3a881537-d96e-4768-950c-d38c014c699a to datanode:575e95f6-9340-44b3-ae3e-c508b9a3128d
scm_1         | 2020-04-17 18:01:17,574 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 3a881537-d96e-4768-950c-d38c014c699a, Nodes: 7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}575e95f6-9340-44b3-ae3e-c508b9a3128d{ip: 10.5.0.9, host: ozone-topology_datanode_6_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.568984Z]
scm_1         | 2020-04-17 18:01:17,576 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:01:17,978 [IPC Server handler 32 on 9861] INFO net.NetworkTopology: Added a new node: /rack2/313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:01:17,979 [IPC Server handler 32 on 9861] INFO node.SCMNodeManager: Registered Data node : 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}
scm_1         | 2020-04-17 18:01:17,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,980 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fd94547c-946e-4f87-9733-51237e4653ce to datanode:313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:01:17,980 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:17,981 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: fd94547c-946e-4f87-9733-51237e4653ce, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:17.980134Z]
scm_1         | 2020-04-17 18:01:17,982 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 5 nodes. Healthy nodes 5
datanode_4_1  | 2020-04-17 18:05:10,603 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-35283765472C:null
datanode_4_1  | 2020-04-17 18:05:10,603 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "643f591a-05e9-4372-bc18-35283765472c"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-35283765472C not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:05:15,283 [grpc-default-executor-1] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: addNew group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] returns group-432946335976:java.util.concurrent.CompletableFuture@2c52ad1[Not completed]
datanode_4_1  | 2020-04-17 18:05:15,285 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001: new RaftServerImpl for group-432946335976:[4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858] with ContainerStateMachine:uninitialized
datanode_4_1  | 2020-04-17 18:05:15,285 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4_1  | 2020-04-17 18:05:15,285 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpcslowness.timeout = 300s (custom)
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300 (default)
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: ConfigurationManager, init=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null, confs=<EMPTY_MAP>
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4_1  | 2020-04-17 18:05:15,286 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4_1  | 2020-04-17 18:05:15,287 [pool-69-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 does not exist. Creating ...
datanode_4_1  | 2020-04-17 18:05:15,288 [pool-69-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/in_use.lock acquired by nodename 6@25eca77e6515
datanode_4_1  | 2020-04-17 18:05:15,289 [pool-69-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976 has been successfully formatted.
datanode_4_1  | 2020-04-17 18:05:15,290 [pool-69-thread-1] INFO ratis.ContainerStateMachine: group-432946335976: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4_1  | 2020-04-17 18:05:15,290 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
datanode_4_1  | 2020-04-17 18:05:15,302 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4_1  | 2020-04-17 18:05:15,303 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4_1  | 2020-04-17 18:05:15,304 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4_1  | 2020-04-17 18:05:15,304 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:05:15,304 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: new 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4_1  | 2020-04-17 18:05:15,305 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4_1  | 2020-04-17 18:05:15,306 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4_1  | 2020-04-17 18:05:15,306 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4_1  | 2020-04-17 18:05:15,306 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4_1  | 2020-04-17 18:05:15,307 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:05:15,307 [pool-69-thread-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4_1  | 2020-04-17 18:05:15,312 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4_1  | 2020-04-17 18:05:15,312 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4_1  | 2020-04-17 18:05:15,312 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4_1  | 2020-04-17 18:05:15,312 [pool-69-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4_1  | 2020-04-17 18:05:15,313 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-432946335976
datanode_4_1  | 2020-04-17 18:05:15,313 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-432946335976
datanode_4_1  | 2020-04-17 18:05:15,313 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: start as a follower, conf=-1: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null
datanode_4_1  | 2020-04-17 18:05:15,313 [pool-69-thread-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4_1  | 2020-04-17 18:05:15,314 [pool-69-thread-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:05:15,321 [pool-69-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-432946335976,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:05:15,321 [pool-69-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-432946335976
datanode_4_1  | 2020-04-17 18:05:20,311 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_4_1  | 2020-04-17 18:05:20,311 [grpc-default-executor-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:05:20,311 [grpc-default-executor-1] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: start FollowerState
datanode_4_1  | 2020-04-17 18:05:20,312 [Thread-161] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:05:20,353 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-432946335976 with new leaderId: 10592d3b-d195-479b-9b4d-1b30ca37f5cc
datanode_4_1  | 2020-04-17 18:05:20,353 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: change Leader from null to 10592d3b-d195-479b-9b4d-1b30ca37f5cc at term 1 for appendEntries, leader elected after 5063ms
datanode_4_1  | 2020-04-17 18:05:20,354 [grpc-default-executor-1] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976: set configuration 0: [4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858, 10592d3b-d195-479b-9b4d-1b30ca37f5cc:10.5.0.4:9858], old=null at 0
datanode_4_1  | 2020-04-17 18:05:20,355 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4_1  | 2020-04-17 18:05:20,360 [313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dd6c75a4-93d3-45f4-84a9-432946335976/current/log_inprogress_0
datanode_4_1  | 2020-04-17 18:05:23,438 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_4_1  | 2020-04-17 18:05:23,438 [Command processor thread] ERROR commandhandler.CloseContainerCommandHandler: Container #4 does not exist in datanode. Container close failed.
datanode_4_1  | 2020-04-17 18:06:16,292 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove  FOLLOWER 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962:t3, leader=9297bad1-1bc3-407b-ab8e-258f1dd490f7, voted=9297bad1-1bc3-407b-ab8e-258f1dd490f7, raftlog=313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLog:OPENED:c0,f0,i2, conf=0: [9297bad1-1bc3-407b-ab8e-258f1dd490f7:10.5.0.5:9858, 4e682fff-8411-4efe-954f-416a0833f687:10.5.0.6:9858, 313d3461-163d-4229-bfba-0697ea0de001:10.5.0.7:9858], old=null RUNNING
datanode_4_1  | 2020-04-17 18:06:16,292 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: shutdown
datanode_4_1  | 2020-04-17 18:06:16,292 [Command processor thread] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-197A8EB51962,id=313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:06:16,293 [Command processor thread] INFO impl.RoleInfo: 313d3461-163d-4229-bfba-0697ea0de001: shutdown FollowerState
datanode_4_1  | 2020-04-17 18:06:16,293 [Command processor thread] INFO impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater: set stopIndex = 0
datanode_4_1  | 2020-04-17 18:06:16,293 [Thread-143] INFO impl.FollowerState: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_4_1  | 2020-04-17 18:06:16,293 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_4_1  | 2020-04-17 18:06:16,294 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:169)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:06:16,294 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater] ERROR ratis.ContainerStateMachine: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_4_1  | 2020-04-17 18:06:16,294 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater] ERROR impl.StateMachineUpdater: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater: Failed to take snapshot
datanode_4_1  | org.apache.ratis.protocol.StateMachineException: Failed to take snapshot  for group-197A8EB51962 as the stateMachine is unhealthy. The last applied index is at (t:3, i:0)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.takeSnapshot(ContainerStateMachine.java:288)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.takeSnapshot(StateMachineUpdater.java:258)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.checkAndTakeSnapshot(StateMachineUpdater.java:250)
datanode_4_1  | 	at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:172)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:06:16,295 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-StateMachineUpdater] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.state_machine.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:06:16,295 [Command processor thread] INFO impl.RaftServerImpl: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962: closes. applyIndex: 0
datanode_4_1  | 2020-04-17 18:06:16,297 [313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
datanode_4_1  | 2020-04-17 18:06:16,298 [Command processor thread] INFO segmented.SegmentedRaftLogWorker: 313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962-SegmentedRaftLogWorker close()
datanode_4_1  | 2020-04-17 18:06:16,298 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.log_worker.313d3461-163d-4229-bfba-0697ea0de001
datanode_4_1  | 2020-04-17 18:06:16,299 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.leader_election.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:06:16,299 [Command processor thread] INFO metrics.RatisMetrics: Unregistering Metrics Registry : ratis.server.313d3461-163d-4229-bfba-0697ea0de001@group-197A8EB51962
datanode_4_1  | 2020-04-17 18:06:16,300 [Command processor thread] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  |  command on datanode #313d3461-163d-4229-bfba-0697ea0de001.
datanode_4_1  | 2020-04-17 18:06:16,301 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-197A8EB51962:null
datanode_4_1  | 2020-04-17 18:06:16,301 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:06:16,301 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-197A8EB51962:null
datanode_4_1  | 2020-04-17 18:06:16,302 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:06:16,302 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-197A8EB51962:null
datanode_4_1  | 2020-04-17 18:06:16,302 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:06:16,303 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-197A8EB51962:null
datanode_4_1  | 2020-04-17 18:06:16,303 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
scm_1         | 2020-04-17 18:01:17,982 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 2
scm_1         | 2020-04-17 18:01:18,005 [IPC Server handler 89 on 9861] INFO net.NetworkTopology: Added a new node: /rack1/10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:01:18,006 [IPC Server handler 89 on 9861] INFO node.SCMNodeManager: Registered Data node : 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}
scm_1         | 2020-04-17 18:01:18,006 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:18,007 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:18,008 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2382b71f-a16d-4a1d-a152-c6e015833419 to datanode:10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:01:18,009 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2382b71f-a16d-4a1d-a152-c6e015833419, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:18.008573Z]
scm_1         | 2020-04-17 18:01:18,009 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:01:18,010 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 to datanode:9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:01:18,010 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 to datanode:313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:01:18,010 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 to datanode:4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:01:18,011 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:18,011 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:01:20,932 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9e30214a-3af8-4f12-a594-490c4a63fe1d, Nodes: 7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:7fb5a271-9f73-44d7-9131-67b4b4fca07d, CreationTimestamp2020-04-17T18:01:17.288264Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:21,104 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:21,104 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:21,451 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 8501b544-67c4-4d39-a949-a0053359583b, Nodes: 575e95f6-9340-44b3-ae3e-c508b9a3128d{ip: 10.5.0.9, host: ozone-topology_datanode_6_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:575e95f6-9340-44b3-ae3e-c508b9a3128d, CreationTimestamp2020-04-17T18:01:17.548116Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:21,452 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:21,452 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:21,484 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e9514b62-8305-4387-8371-bde057308d8f, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:17.479137Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:21,484 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:21,484 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:21,696 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1deb4c57-bfe1-4217-b2d1-e0e2714d3e07, Nodes: 4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:4e682fff-8411-4efe-954f-416a0833f687, CreationTimestamp2020-04-17T18:01:17.462740Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:21,696 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:21,696 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:22,057 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: fd94547c-946e-4f87-9733-51237e4653ce, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:313d3461-163d-4229-bfba-0697ea0de001, CreationTimestamp2020-04-17T18:01:17.980134Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:22,057 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:22,057 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:22,550 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2382b71f-a16d-4a1d-a152-c6e015833419, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:01:18.008573Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:22,551 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:22,551 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:26,685 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 3a881537-d96e-4768-950c-d38c014c699a, Nodes: 7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}575e95f6-9340-44b3-ae3e-c508b9a3128d{ip: 10.5.0.9, host: ozone-topology_datanode_6_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:7fb5a271-9f73-44d7-9131-67b4b4fca07d, CreationTimestamp2020-04-17T18:01:17.568984Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:26,686 [EventQueue-OpenPipelineForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1         | 2020-04-17 18:01:26,687 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1         | 2020-04-17 18:01:26,688 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1         | 2020-04-17 18:01:26,688 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1         | 2020-04-17 18:01:26,691 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1         | 2020-04-17 18:01:27,090 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z] moved to OPEN state
scm_1         | 2020-04-17 18:01:38,385 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : ContainerID 2 creation failed
scm_1         | 2020-04-17 18:01:38,385 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:38,385 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z] moved to CLOSED state
scm_1         | 2020-04-17 18:01:38,387 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2
scm_1         | 2020-04-17 18:01:38,522 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:01:38,524 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:38,598 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7. Reason : ContainerID 2 creation failed
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:06:16,303 [Command processor thread] INFO impl.RaftServerProxy: 313d3461-163d-4229-bfba-0697ea0de001: remove group-197A8EB51962:null
datanode_4_1  | 2020-04-17 18:06:16,303 [Command processor thread] ERROR commandhandler.ClosePipelineCommandHandler: Can't close pipeline #id: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962"
datanode_4_1  | 
datanode_4_1  | java.io.IOException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:659)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.ClosePipelineCommandHandler.handle(ClosePipelineCommandHandler.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:99)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$1(DatanodeStateMachine.java:472)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.ratis.protocol.GroupMismatchException: 313d3461-163d-4229-bfba-0697ea0de001: Group group-197A8EB51962 not found.
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:404)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:367)
datanode_4_1  | 	at org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:350)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:657)
datanode_4_1  | 	... 4 more
datanode_4_1  | 2020-04-17 18:06:23,443 [grpc-default-executor-1] INFO server.GrpcServerProtocolService: 313d3461-163d-4229-bfba-0697ea0de001: Completed APPEND_ENTRIES, lastRequest: 9297bad1-1bc3-407b-ab8e-258f1dd490f7->313d3461-163d-4229-bfba-0697ea0de001#4-t3, previous=(t:3, i:1), leaderCommit=0, initializing? false, entries: size=1, first=(t:3, i:2), STATEMACHINELOGENTRY, client-86DCCB287922, cid=67
datanode_4_1  | 2020-04-17 18:06:27,253 [ChunkWriter-10-0] INFO keyvalue.KeyValueHandler: Operation: CreateContainer , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: Container creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container creation failed, due to disk out of space
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:165)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:247)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.dispatchRequest(KeyValueHandler.java:164)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:152)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:414)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:250)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | Caused by: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=966258688 B) is less than the container size (=1073741824 B).
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.volume.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:77)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:124)
datanode_4_1  | 	... 13 more
datanode_4_1  | 2020-04-17 18:06:27,254 [ChunkWriter-10-0] INFO impl.HddsDispatcher: Operation: WriteChunk , Trace ID: b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0 , Message: ContainerID 5 creation failed , Result: DISK_OUT_OF_SPACE , StorageContainerException Occurred.
datanode_4_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 5 creation failed
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:254)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:162)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:402)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:412)
datanode_4_1  | 	at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$2(ContainerStateMachine.java:447)
datanode_4_1  | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4_1  | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_4_1  | 2020-04-17 18:06:27,254 [ChunkWriter-10-0] ERROR ratis.ContainerStateMachine: group-432946335976: writeChunk writeStateMachineData failed: blockIdcontainerID: 5
datanode_4_1  | localID: 104015251845611544
datanode_4_1  | blockCommitSequenceId: 0
datanode_4_1  |  logIndex 1 chunkName 104015251845611544_chunk_1 Error message: ContainerID 5 creation failed Container Result: DISK_OUT_OF_SPACE
datanode_4_1  | 2020-04-17 18:06:27,254 [313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : ContainerID 5 creation failed
datanode_4_1  | 2020-04-17 18:06:27,265 [313d3461-163d-4229-bfba-0697ea0de001@group-432946335976-SegmentedRaftLogWorker] ERROR ratis.XceiverServerRatis: pipeline Action CLOSE on pipeline PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976.Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
datanode_4_1  | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:01:38,599 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:38,686 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:01:38,686 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:38,718 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : ContainerID 2 creation failed
scm_1         | 2020-04-17 18:01:38,718 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:01:38,750 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-A6ABD0C3804E, cid=5
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "c7c4584b166e09cc:b7a6f2d8e70de2b0:c7c4584b166e09cc:0" containerID: 2 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "059e2fb4-f95b-4489-a61e-3e8baa29d9a7" writeChunk { blockID { containerID: 2 localID: 104015232882245633 blockCommitSequenceId: 0 } chunkData { chunkName: "104015232882245633_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:01:38,750 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | 2020-04-17 18:02:44,397 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,397 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,398 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,398 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z] removed from db
scm_1         | 2020-04-17 18:02:44,399 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:02:44,400 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c to datanode:313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,400 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c to datanode:10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:02:44,400 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c to datanode:7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:02:44,400 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:02:44,400 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:02:44,525 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,525 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,525 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,525 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:02:44,599 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,599 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,600 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,600 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:02:44,687 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,687 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,687 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,687 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:02:44,719 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,719 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,719 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,719 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:02:44,750 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:02:44,750 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:02:44,751 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:02:44,751 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 059e2fb4-f95b-4489-a61e-3e8baa29d9a7, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:01:18.010289Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=059e2fb4-f95b-4489-a61e-3e8baa29d9a7 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:03:04,010 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z] moved to OPEN state
scm_1         | 2020-04-17 18:03:11,195 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:03:11,196 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:03:14,130 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : ContainerID 3 creation failed
scm_1         | 2020-04-17 18:03:14,131 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:03:14,131 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z] moved to CLOSED state
scm_1         | 2020-04-17 18:03:14,131 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #3
scm_1         | 2020-04-17 18:03:14,145 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:03:14,145 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:03:14,156 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d. Reason : ContainerID 3 creation failed
scm_1         | 2020-04-17 18:03:14,157 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:03:14,193 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc. Reason : ContainerID 3 creation failed
scm_1         | 2020-04-17 18:03:14,193 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:03:14,217 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:03:14,218 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:03:14,248 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=643f591a-05e9-4372-bc18-35283765472c from datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-F7864D086DEC, cid=26
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "f7934ee6d71cd1a1:b2671a63be5f524d:f7934ee6d71cd1a1:0" containerID: 3 datanodeUuid: "313d3461-163d-4229-bfba-0697ea0de001" pipelineID: "643f591a-05e9-4372-bc18-35283765472c" writeChunk { blockID { containerID: 3 localID: 104015239176716296 blockCommitSequenceId: 0 } chunkData { chunkName: "104015239176716296_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:03:14,248 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | 2020-04-17 18:04:20,131 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,132 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,132 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,133 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z] removed from db
scm_1         | 2020-04-17 18:04:20,134 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:04:20,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 to datanode:9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:04:20,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 to datanode:313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,134 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 to datanode:4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:04:20,135 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:20,135 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:04:20,146 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,146 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,146 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,146 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=643f591a-05e9-4372-bc18-35283765472c not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:04:20,157 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,157 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,157 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,158 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=643f591a-05e9-4372-bc18-35283765472c not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:04:20,193 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,193 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,194 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,194 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=643f591a-05e9-4372-bc18-35283765472c not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:04:20,219 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,219 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,219 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,219 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=643f591a-05e9-4372-bc18-35283765472c not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:04:20,248 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:04:20,249 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:04:20,249 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=643f591a-05e9-4372-bc18-35283765472c close command to datanode 7fb5a271-9f73-44d7-9131-67b4b4fca07d
scm_1         | 2020-04-17 18:04:20,249 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 643f591a-05e9-4372-bc18-35283765472c, Nodes: 313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}7fb5a271-9f73-44d7-9131-67b4b4fca07d{ip: 10.5.0.8, host: ozone-topology_datanode_5_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:02:44.400142Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=643f591a-05e9-4372-bc18-35283765472c not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:04:49,776 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z] moved to OPEN state
scm_1         | 2020-04-17 18:04:52,432 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7. Reason : ContainerID 4 creation failed
scm_1         | 2020-04-17 18:04:52,449 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:52,450 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z] moved to CLOSED state
scm_1         | 2020-04-17 18:04:52,451 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : ContainerID 4 creation failed
scm_1         | 2020-04-17 18:04:52,452 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:52,453 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : ContainerID 4 creation failed
scm_1         | 2020-04-17 18:04:52,452 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4
scm_1         | 2020-04-17 18:04:52,454 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #4
scm_1         | 2020-04-17 18:04:52,457 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:52,457 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:04:52,457 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:52,458 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:04:52,458 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:04:52,461 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 from datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7. Reason : Log already failed at index 1 for task WriteLog:1: (t:3, i:1), STATEMACHINELOGENTRY, client-86DCCB287922, cid=66
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "8bc25a83974bdae3:8f6ff0597a2684d6:8bc25a83974bdae3:0" containerID: 4 datanodeUuid: "9297bad1-1bc3-407b-ab8e-258f1dd490f7" pipelineID: "2e99f3a8-87a2-45fe-b8d0-197a8eb51962" writeChunk { blockID { containerID: 4 localID: 104015245631094805 blockCommitSequenceId: 0 } chunkData { chunkName: "104015245631094805_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:04:52,461 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | 2020-04-17 18:05:11,198 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:05:11,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 to datanode:10592d3b-d195-479b-9b4d-1b30ca37f5cc
scm_1         | 2020-04-17 18:05:11,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 to datanode:313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:11,201 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 to datanode:4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:11,202 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:05:11,202 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:05:20,338 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z] moved to OPEN state
scm_1         | 2020-04-17 18:05:58,452 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,452 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,452 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,453 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z] removed from db
scm_1         | 2020-04-17 18:05:58,453 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,453 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,453 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,454 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:05:58,454 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 6 nodes. Healthy nodes 6
scm_1         | 2020-04-17 18:05:58,455 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 1
scm_1         | 2020-04-17 18:05:58,457 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,457 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,458 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,458 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:05:58,458 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,458 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,459 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,460 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:05:58,460 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,460 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,460 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,460 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:05:58,462 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 9297bad1-1bc3-407b-ab8e-258f1dd490f7
scm_1         | 2020-04-17 18:05:58,462 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 313d3461-163d-4229-bfba-0697ea0de001
scm_1         | 2020-04-17 18:05:58,462 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 close command to datanode 4e682fff-8411-4efe-954f-416a0833f687
scm_1         | 2020-04-17 18:05:58,463 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Destroy pipeline failed for pipeline:Pipeline[ Id: 2e99f3a8-87a2-45fe-b8d0-197a8eb51962, Nodes: 9297bad1-1bc3-407b-ab8e-258f1dd490f7{ip: 10.5.0.5, host: ozone-topology_datanode_2_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:9297bad1-1bc3-407b-ab8e-258f1dd490f7, CreationTimestamp2020-04-17T18:04:20.134535Z]
scm_1         | org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=2e99f3a8-87a2-45fe-b8d0-197a8eb51962 not found
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removePipeline(PipelineStateMap.java:323)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removePipeline(PipelineStateManager.java:104)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(SCMPipelineManager.java:595)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(SCMPipelineManager.java:579)
scm_1         | 	at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.lambda$finalizeAndDestroyPipeline$0(SCMPipelineManager.java:422)
scm_1         | 	at org.apache.hadoop.hdds.utils.Scheduler.lambda$schedule$1(Scheduler.java:70)
scm_1         | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm_1         | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm_1         | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm_1         | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm_1         | 	at java.base/java.lang.Thread.run(Thread.java:834)
scm_1         | 2020-04-17 18:06:26,716 [Thread-338] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1         | 2020-04-17 18:06:26,731 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 7 milliseconds for processing 4 containers.
scm_1         | 2020-04-17 18:06:27,276 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc. Reason : ContainerID 5 creation failed
scm_1         | 2020-04-17 18:06:27,279 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:06:27,281 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z] moved to CLOSED state
scm_1         | 2020-04-17 18:06:27,281 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : ContainerID 5 creation failed
scm_1         | 2020-04-17 18:06:27,283 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:06:27,281 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #5
scm_1         | 2020-04-17 18:06:27,285 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #5
scm_1         | 2020-04-17 18:06:27,285 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 313d3461-163d-4229-bfba-0697ea0de001. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:06:27,286 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:06:27,287 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 10592d3b-d195-479b-9b4d-1b30ca37f5cc. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:06:27,287 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:06:27,298 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : ContainerID 5 creation failed
scm_1         | 2020-04-17 18:06:27,298 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
scm_1         | 2020-04-17 18:06:27,300 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.PipelineActionHandler: Received pipeline action CLOSE for PipelineID=dd6c75a4-93d3-45f4-84a9-432946335976 from datanode 4e682fff-8411-4efe-954f-416a0833f687. Reason : Log already failed at index 1 for task WriteLog:1: (t:1, i:1), STATEMACHINELOGENTRY, client-D60E185ECF64, cid=76
scm_1         | 	 State Machine: cmdType: WriteChunk traceID: "b9b0d5beb2c7ac0a:e5eeffe88275b01c:b9b0d5beb2c7ac0a:0" containerID: 5 datanodeUuid: "10592d3b-d195-479b-9b4d-1b30ca37f5cc" pipelineID: "dd6c75a4-93d3-45f4-84a9-432946335976" writeChunk { blockID { containerID: 5 localID: 104015251845611544 blockCommitSequenceId: 0 } chunkData { chunkName: "104015251845611544_chunk_1" offset: 0 len: 10240 checksumData { type: CRC32 bytesPerChecksum: 1048576 checksums: "\032\302rr" } } }, container path=nonexistent
scm_1         | 2020-04-17 18:06:27,300 [EventQueue-PipelineActionsForPipelineActionHandler] INFO pipeline.SCMPipelineManager: Destroying pipeline:Pipeline[ Id: dd6c75a4-93d3-45f4-84a9-432946335976, Nodes: 10592d3b-d195-479b-9b4d-1b30ca37f5cc{ip: 10.5.0.4, host: ozone-topology_datanode_1_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}313d3461-163d-4229-bfba-0697ea0de001{ip: 10.5.0.7, host: ozone-topology_datanode_4_1.ozone-topology_net, networkLocation: /rack2, certSerialId: null}4e682fff-8411-4efe-954f-416a0833f687{ip: 10.5.0.6, host: ozone-topology_datanode_3_1.ozone-topology_net, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:CLOSED, leaderId:10592d3b-d195-479b-9b4d-1b30ca37f5cc, CreationTimestamp2020-04-17T18:05:11.200980Z]
