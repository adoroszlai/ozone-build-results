Using Docker Compose v2
Executing test ozonesecure-ha/test-debug-tools.sh
Using Docker Compose v2
Port 88 is not available on kdc yet
Port 88 is available on kdc
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is available on scm1.org
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 233
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 232
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 231
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 230
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 229
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 228
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 227
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 226
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 225
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 224
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 223
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 222
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 221
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 220
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 219
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 218
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 217
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 216
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 215
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 214
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 213
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 212
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 211
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 210
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 209
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 208
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 207
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 206
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 205
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 204
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 203
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 202
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 201
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 200
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 199
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 198
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 197
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 196
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 195
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 194
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 193
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 192
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 191
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 190
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 189
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 188
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 187
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 186
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 185
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 184
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 183
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 182
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 181
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 180
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 179
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 178
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 177
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 176
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 175
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 174
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 173
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 172
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 171
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 170
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 169
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 168
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 167
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 166
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 165
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 164
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 163
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 162
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 161
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 159
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 158
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 157
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 156
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 155
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 154
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 153
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 152
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 151
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 150
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 149
SCM is out of safe mode.
Found OM leader for service omservice: om3 : LEADER (om3)
Replaced OM order with om3,om2,om1 in ozonesecure-ha-datanode1-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-datanode2-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-datanode3-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-datanode4-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-datanode5-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-httpfs-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-om1-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-om2-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-om3-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-recon-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-s3g-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-scm1.org-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-scm2.org-1
Replaced OM order with om3,om2,om1 in ozonesecure-ha-scm3.org-1
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-001.xml
==============================================================================
Auditparser :: Smoketest ozone cluster startup                                
==============================================================================
Testing audit parser                                                  | PASS |
------------------------------------------------------------------------------
Auditparser :: Smoketest ozone cluster startup                        | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-002.xml
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-003.xml
Taking a backup of container.db
==============================================================================
Ozone-Debug-Tests :: Test ozone debug CLI                                     
==============================================================================
Test ozone debug replicas verify checksums, block-existence and co... | PASS |
------------------------------------------------------------------------------
Test ozone debug replicas verify with RATIS ONE filter                | PASS |
------------------------------------------------------------------------------
Test ozone debug replicas verify with RATIS THREE filter              | PASS |
------------------------------------------------------------------------------
Test ozone debug replicas verify with EC rs-3-2-1024k filter          | PASS |
------------------------------------------------------------------------------
Test ozone debug version                                              | PASS |
------------------------------------------------------------------------------
Ozone-Debug-Tests :: Test ozone debug CLI                             | PASS |
5 tests, 5 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-004.xml
==============================================================================
Corrupt-Block-Checksum :: Test checksums on a corrupt block replica           
==============================================================================
Test checksums with a corrupt block replica                           | PASS |
------------------------------------------------------------------------------
Corrupt-Block-Checksum :: Test checksums on a corrupt block replica   | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-005.xml
Overwriting container.db with the backup db
ozonesecure-ha-datanode1-1
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 3
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 6
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 8
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 11
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 14
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 17
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 19
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 22
Waiting for ozonesecure-ha-datanode1-1 to be STALE
SECONDS: 25
ozonesecure-ha-datanode1-1 is STALE
==============================================================================
Stale-Datanode-Checksum :: Test checksums in case of a stale datanode         
==============================================================================
Test checksums with a stale datanode                                  | PASS |
------------------------------------------------------------------------------
Stale-Datanode-Checksum :: Test checksums in case of a stale datanode | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-006.xml
ozonesecure-ha-datanode1-1
Waiting for ozonesecure-ha-datanode1-1 to be HEALTHY
SECONDS: 5
Waiting for ozonesecure-ha-datanode1-1 to be HEALTHY
SECONDS: 11
Waiting for ozonesecure-ha-datanode1-1 to be HEALTHY
SECONDS: 14
Waiting for ozonesecure-ha-datanode1-1 to be HEALTHY
SECONDS: 17
ozonesecure-ha-datanode1-1 is HEALTHY
==============================================================================
Block-Existence-Check :: Test existence of a block on a datanode              
==============================================================================
Test block existence with a block missing on a replica                | PASS |
------------------------------------------------------------------------------
Block-Existence-Check :: Test existence of a block on a datanode      | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-007.xml
==============================================================================
Container-State-Verifier :: Test container state on a UNHEALTHY, DELETED an...
==============================================================================
Verify Container State With Unhealthy Container Replica               | PASS |
------------------------------------------------------------------------------
Verify Container State With Deleted Container Replica                 | PASS |
------------------------------------------------------------------------------
Verify Container State With Invalid Container Replica                 | PASS |
------------------------------------------------------------------------------
Container-State-Verifier :: Test container state on a UNHEALTHY, D... | PASS |
3 tests, 3 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-008.xml
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-009.xml
==============================================================================
Ozone-Debug-Tests-Ec3-2 :: Test ozone Debug CLI for EC(3,2) replicated keys   
==============================================================================
Test ozone debug replicas chunk-info                                  | PASS |
------------------------------------------------------------------------------
Ozone-Debug-Tests-Ec3-2 :: Test ozone Debug CLI for EC(3,2) replic... | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-010.xml
To use Ozone please mount ozone folder to /opt/hadoop
Output:  /rebot-output/ozonesecure-ha-debug-tools.xml
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/rebot-WXnfad/ozonesecure-ha-debug-tools.xml' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha-debug-tools.xml'
removed 'ozonesecure-ha/result/robot-001.xml'
removed 'ozonesecure-ha/result/robot-002.xml'
removed 'ozonesecure-ha/result/robot-003.xml'
removed 'ozonesecure-ha/result/robot-004.xml'
removed 'ozonesecure-ha/result/robot-005.xml'
removed 'ozonesecure-ha/result/robot-006.xml'
removed 'ozonesecure-ha/result/robot-007.xml'
removed 'ozonesecure-ha/result/robot-008.xml'
removed 'ozonesecure-ha/result/robot-009.xml'
removed 'ozonesecure-ha/result/robot-010.xml'
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha'
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools'
renamed 'ozonesecure-ha/result/dn-audit-204452ee546a.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/dn-audit-204452ee546a.log'
renamed 'ozonesecure-ha/result/dn-audit-34a144a3664b.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/dn-audit-34a144a3664b.log'
renamed 'ozonesecure-ha/result/dn-audit-8b0ef4b44987.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/dn-audit-8b0ef4b44987.log'
renamed 'ozonesecure-ha/result/dn-audit-a364d55ecf1c.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/dn-audit-a364d55ecf1c.log'
renamed 'ozonesecure-ha/result/dn-audit-cc7b8b5d9cb6.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/dn-audit-cc7b8b5d9cb6.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-datanode1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-datanode2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-datanode3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode4-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-datanode4-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode5-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-datanode5-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-httpfs-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-httpfs-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kdc-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-kdc-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kms-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-kms-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-om1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-om2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-om3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om4-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-om4-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-recon-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-recon-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-s3g-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-s3g-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm1.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-scm1.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm2.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-scm2.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm3.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/docker-ozonesecure-ha-scm3.org-1.log'
renamed 'ozonesecure-ha/result/kms-audit.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/kms-audit.log'
renamed 'ozonesecure-ha/result/om-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-audit-om1.log'
renamed 'ozonesecure-ha/result/om-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-audit-om2.log'
renamed 'ozonesecure-ha/result/om-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-audit-om3.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-sys-audit-om1.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-sys-audit-om2.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/om-sys-audit-om3.log'
renamed 'ozonesecure-ha/result/s3g-audit-s3g.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/s3g-audit-s3g.log'
renamed 'ozonesecure-ha/result/scm-audit-scm1.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/scm-audit-scm1.org.log'
renamed 'ozonesecure-ha/result/scm-audit-scm2.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/scm-audit-scm2.org.log'
renamed 'ozonesecure-ha/result/scm-audit-scm3.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/debug-tools/scm-audit-scm3.org.log'
Executing test ozonesecure-ha/test-repair-tools.sh
Using Docker Compose v2
Port 88 is available on kdc
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is not available on scm1.org yet
Port 9860 is available on scm1.org
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 233
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 232
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 231
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 230
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 229
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 228
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 227
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 226
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 225
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 224
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 223
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 222
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 221
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 220
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 219
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 218
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 217
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 216
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 215
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 214
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 213
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 212
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 211
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 210
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 209
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 208
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 207
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 206
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 205
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 204
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 203
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 202
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 201
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 200
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 199
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 198
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 197
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 196
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 195
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 194
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 193
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 192
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 191
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 190
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 189
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 188
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 187
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 186
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 185
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 184
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 183
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 182
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 181
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 180
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 179
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 178
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 177
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 176
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 175
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 174
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 173
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 172
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 171
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 170
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 169
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 168
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 167
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 166
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 165
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 164
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 163
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 162
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 161
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 160
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 159
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 158
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 157
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 156
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 155
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 153
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 152
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 151
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 150
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 149
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 148
SCM is in safe mode. Will retry in 1 sec. Remaining time (sec): 147
SCM is out of safe mode.
Found OM leader for service omservice: om1 : LEADER (om1)
Replaced OM order with om1,om3,om2 in ozonesecure-ha-datanode1-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-datanode2-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-datanode3-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-datanode4-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-datanode5-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-httpfs-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-om1-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-om2-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-om3-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-recon-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-s3g-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-scm1.org-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-scm2.org-1
Replaced OM order with om1,om3,om2 in ozonesecure-ha-scm3.org-1
Testing ratis transaction repair on all OMs
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-001.xml
==============================================================================
Ratis-Transaction-Repair :: Test recovering from OM crash due to transactio...
==============================================================================
Verify OM crash at bucket create                                      | PASS |
------------------------------------------------------------------------------
Ratis-Transaction-Repair :: Test recovering from OM crash due to t... | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-002.xml
Waiting for container 'ozonesecure-ha-om1-1' to stop...
Container 'ozonesecure-ha-om1-1' has stopped.
Ratis log segment file path: /opt/hadoop/compose/ozonesecure-ha/data/om1/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
ATTENTION: Running as user hadoop. Make sure this is the same user used to run the Ozone process. Are you sure you want to continue (y/N)? Run as user: hadoop
No running OM service detected. Proceeding with repair.
BackupDir "/opt/hadoop/compose/ozonesecure-ha/data/om1/backup1" does not exist. Creating the directory path.
Taking back up of Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om1/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to location: /opt/hadoop/compose/ozonesecure-ha/data/om1/backup1/log_inprogress_0
File backed-up successfully!
Created temporary output file: /opt/hadoop/compose/ozonesecure-ha/data/om1/backup1/srt-output18354201869845278919.tmp
Processing Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om1/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 size:4194304
Replacing {cmdType: CreateBucket  traceID: ""  clientId: "client-2FA713CA42A0"  userInfo {    userName: "testuser/scm@EXAMPLE.COM"    remoteAddress: "172.25.0.116"    hostName: "scm1.org"  }  version: 3  layoutVersion {    version: 9  }  createBucketRequest {    bucketInfo {      volumeName: "test-txn-vol"      bucketName: "bucket-crash-1"      isVersionEnabled: false      storageType: DISK      creationTime: 1768474523920      objectID: 0      updateID: 0      modificationTime: 1768474523920      usedBytes: 0      quotaInBytes: -1      quotaInNamespace: -1      usedNamespace: 0      owner: "testuser"      snapshotUsedBytes: 0      snapshotUsedNamespace: 0    }  }  } with EchoRPC command at index 2
Finished processing all the entries (3 logs) from the segment file.
Moved temporary output file to correct raft log location : /opt/hadoop/compose/ozonesecure-ha/data/om1/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
Repair command executed for om1.
ozonesecure-ha-om1-1
Container 'ozonesecure-ha-om1-1' started again.
Bucket table for om1:
bucket 'bucket-crash-1' is not present in the bucketTable of om1 as expected.
Waiting for container 'ozonesecure-ha-om2-1' to stop...
Container 'ozonesecure-ha-om2-1' has stopped.
Ratis log segment file path: /opt/hadoop/compose/ozonesecure-ha/data/om2/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
ATTENTION: Running as user hadoop. Make sure this is the same user used to run the Ozone process. Are you sure you want to continue (y/N)? Run as user: hadoop
No running OM service detected. Proceeding with repair.
BackupDir "/opt/hadoop/compose/ozonesecure-ha/data/om2/backup1" does not exist. Creating the directory path.
Taking back up of Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om2/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to location: /opt/hadoop/compose/ozonesecure-ha/data/om2/backup1/log_inprogress_0
File backed-up successfully!
Created temporary output file: /opt/hadoop/compose/ozonesecure-ha/data/om2/backup1/srt-output14805614144831584465.tmp
Processing Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om2/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 size:4194304
Replacing {cmdType: CreateBucket  traceID: ""  clientId: "client-2FA713CA42A0"  userInfo {    userName: "testuser/scm@EXAMPLE.COM"    remoteAddress: "172.25.0.116"    hostName: "scm1.org"  }  version: 3  layoutVersion {    version: 9  }  createBucketRequest {    bucketInfo {      volumeName: "test-txn-vol"      bucketName: "bucket-crash-1"      isVersionEnabled: false      storageType: DISK      creationTime: 1768474523920      objectID: 0      updateID: 0      modificationTime: 1768474523920      usedBytes: 0      quotaInBytes: -1      quotaInNamespace: -1      usedNamespace: 0      owner: "testuser"      snapshotUsedBytes: 0      snapshotUsedNamespace: 0    }  }  } with EchoRPC command at index 2
Finished processing all the entries (3 logs) from the segment file.
Moved temporary output file to correct raft log location : /opt/hadoop/compose/ozonesecure-ha/data/om2/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
Repair command executed for om2.
ozonesecure-ha-om2-1
Container 'ozonesecure-ha-om2-1' started again.
Bucket table for om2:
bucket 'bucket-crash-1' is not present in the bucketTable of om2 as expected.
Waiting for container 'ozonesecure-ha-om3-1' to stop...
Container 'ozonesecure-ha-om3-1' has stopped.
Ratis log segment file path: /opt/hadoop/compose/ozonesecure-ha/data/om3/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
ATTENTION: Running as user hadoop. Make sure this is the same user used to run the Ozone process. Are you sure you want to continue (y/N)? Run as user: hadoop
No running OM service detected. Proceeding with repair.
BackupDir "/opt/hadoop/compose/ozonesecure-ha/data/om3/backup1" does not exist. Creating the directory path.
Taking back up of Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om3/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to location: /opt/hadoop/compose/ozonesecure-ha/data/om3/backup1/log_inprogress_0
File backed-up successfully!
Created temporary output file: /opt/hadoop/compose/ozonesecure-ha/data/om3/backup1/srt-output3218832481812016477.tmp
Processing Raft Log file: /opt/hadoop/compose/ozonesecure-ha/data/om3/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 size:4194304
Replacing {cmdType: CreateBucket  traceID: ""  clientId: "client-2FA713CA42A0"  userInfo {    userName: "testuser/scm@EXAMPLE.COM"    remoteAddress: "172.25.0.116"    hostName: "scm1.org"  }  version: 3  layoutVersion {    version: 9  }  createBucketRequest {    bucketInfo {      volumeName: "test-txn-vol"      bucketName: "bucket-crash-1"      isVersionEnabled: false      storageType: DISK      creationTime: 1768474523920      objectID: 0      updateID: 0      modificationTime: 1768474523920      usedBytes: 0      quotaInBytes: -1      quotaInNamespace: -1      usedNamespace: 0      owner: "testuser"      snapshotUsedBytes: 0      snapshotUsedNamespace: 0    }  }  } with EchoRPC command at index 2
Finished processing all the entries (3 logs) from the segment file.
Moved temporary output file to correct raft log location : /opt/hadoop/compose/ozonesecure-ha/data/om3/metadata/om.ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
Repair command executed for om3.
ozonesecure-ha-om3-1
Container 'ozonesecure-ha-om3-1' started again.
Bucket table for om3:
bucket 'bucket-crash-1' is not present in the bucketTable of om3 as expected.
Found OM leader for service omservice: om2 : LEADER (om2)
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode1-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode2-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode3-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode4-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-datanode5-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-httpfs-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om1-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om2-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-om3-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-recon-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-s3g-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm1.org-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm2.org-1
Replaced OM order with om2,om3,om1 in ozonesecure-ha-scm3.org-1
Testing ratis transaction repair completed successfully.
==============================================================================
Kinit :: Kinit test user                                                      
==============================================================================
Kinit                                                                 | PASS |
------------------------------------------------------------------------------
Kinit :: Kinit test user                                              | PASS |
1 test, 1 passed, 0 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-003.xml
Creating test keys to verify om compaction
Test keys created
Restarting OM after key creation to flush and generate sst files
ozonesecure-ha-om1-1
==============================================================================
Om-Compact :: Test for OM DB Compaction Repair Tool                           
==============================================================================
Testing OM DB Size Reduction After Compaction                         | FAIL |
OM DB size should be reduced after compaction. Before: 4773437, After: 5322282
------------------------------------------------------------------------------
Om-Compact :: Test for OM DB Compaction Repair Tool                   | FAIL |
1 test, 0 passed, 1 failed
==============================================================================
Output:  /tmp/smoketest/ozonesecure-ha/result/robot-004.xml
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-datanode1-1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-datanode2-1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-datanode3-1_HddsDatanodeService.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-datanode4-1_HddsDatanodeService.stack
jstack 6 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-datanode5-1_HddsDatanodeService.stack
jstack 8 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-om1-1_OzoneManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-om2-1_OzoneManagerStarter.stack
jstack 6 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-om3-1_OzoneManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-recon-1_ReconServer.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-s3g-1_Gateway.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-scm1.org-1_StorageContainerManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-scm2.org-1_StorageContainerManagerStarter.stack
jstack 7 > /home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/ozonesecure-ha/result/ozonesecure-ha-scm3.org-1_StorageContainerManagerStarter.stack
ERROR: Test execution of ozonesecure-ha/test-repair-tools.sh is FAILED!!!!
To use Ozone please mount ozone folder to /opt/hadoop
Output:  /rebot-output/ozonesecure-ha-repair-tools.xml
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/rebot-fyVa90/ozonesecure-ha-repair-tools.xml' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha-repair-tools.xml'
removed 'ozonesecure-ha/result/robot-001.xml'
removed 'ozonesecure-ha/result/robot-002.xml'
removed 'ozonesecure-ha/result/robot-003.xml'
removed 'ozonesecure-ha/result/robot-004.xml'
mkdir: created directory '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools'
renamed 'ozonesecure-ha/result/dn-audit-3f860e95ad03.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/dn-audit-3f860e95ad03.log'
renamed 'ozonesecure-ha/result/dn-audit-6d2077bb57ed.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/dn-audit-6d2077bb57ed.log'
renamed 'ozonesecure-ha/result/dn-audit-7af2771cbd1e.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/dn-audit-7af2771cbd1e.log'
renamed 'ozonesecure-ha/result/dn-audit-8dbe66b124c2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/dn-audit-8dbe66b124c2.log'
renamed 'ozonesecure-ha/result/dn-audit-b3a4f3ccf6b3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/dn-audit-b3a4f3ccf6b3.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-datanode1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-datanode2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-datanode3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode4-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-datanode4-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-datanode5-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-datanode5-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-httpfs-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-httpfs-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kdc-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-kdc-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-kms-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-kms-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om1-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-om1-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om2-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-om2-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om3-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-om3-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-om4-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-om4-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-recon-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-recon-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-s3g-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-s3g-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm1.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-scm1.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm2.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-scm2.org-1.log'
renamed 'ozonesecure-ha/result/docker-ozonesecure-ha-scm3.org-1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/docker-ozonesecure-ha-scm3.org-1.log'
renamed 'ozonesecure-ha/result/kms-audit.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/kms-audit.log'
renamed 'ozonesecure-ha/result/om-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-audit-om1.log'
renamed 'ozonesecure-ha/result/om-audit-om2-2026-01-15-1.log.gz' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-audit-om2-2026-01-15-1.log.gz'
renamed 'ozonesecure-ha/result/om-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-audit-om2.log'
renamed 'ozonesecure-ha/result/om-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-audit-om3.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om1.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-sys-audit-om1.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om2.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-sys-audit-om2.log'
renamed 'ozonesecure-ha/result/om-sys-audit-om3.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/om-sys-audit-om3.log'
renamed 'ozonesecure-ha/result/ozonesecure-ha-datanode1-1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-datanode1-1_HddsDatanodeService.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-datanode2-1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-datanode2-1_HddsDatanodeService.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-datanode3-1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-datanode3-1_HddsDatanodeService.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-datanode4-1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-datanode4-1_HddsDatanodeService.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-datanode5-1_HddsDatanodeService.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-datanode5-1_HddsDatanodeService.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-om1-1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-om1-1_OzoneManagerStarter.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-om2-1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-om2-1_OzoneManagerStarter.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-om3-1_OzoneManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-om3-1_OzoneManagerStarter.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-recon-1_ReconServer.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-recon-1_ReconServer.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-s3g-1_Gateway.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-s3g-1_Gateway.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-scm1.org-1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-scm1.org-1_StorageContainerManagerStarter.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-scm2.org-1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-scm2.org-1_StorageContainerManagerStarter.stack'
renamed 'ozonesecure-ha/result/ozonesecure-ha-scm3.org-1_StorageContainerManagerStarter.stack' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/ozonesecure-ha-scm3.org-1_StorageContainerManagerStarter.stack'
renamed 'ozonesecure-ha/result/s3g-audit-s3g.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/s3g-audit-s3g.log'
renamed 'ozonesecure-ha/result/scm-audit-scm1.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/scm-audit-scm1.org.log'
renamed 'ozonesecure-ha/result/scm-audit-scm2.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/scm-audit-scm2.org.log'
renamed 'ozonesecure-ha/result/scm-audit-scm3.org.log' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/ozonesecure-ha/repair-tools/scm-audit-scm3.org.log'
To use Ozone please mount ozone folder to /opt/hadoop
Log:     /rebot-output/log.html
Report:  /rebot-output/report.html
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/rebot-jnRBPb/log.html' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/log.html'
renamed '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/rebot-jnRBPb/report.html' -> '/home/runner/work/ozone/ozone/hadoop-ozone/dist/target/ozone-2.2.0-SNAPSHOT/compose/result/report.html'
removed directory '/tmp/robot-data-dpFhEe'
